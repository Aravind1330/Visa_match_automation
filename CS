
def match_one_to_one(T5, Invoice_table):
    matching_invoices = []
    match_rule = None

    for Invoice in Invoice_table:
        for rule_index, rule in enumerate(rules, start=1):
            if rule(T5, Invoice=Invoice) and float(T5['fin_source_amt']) == float(Invoice['inv_match_source_amt']):
                matching_invoices.append((Invoice, rule_index))
                match_rule = rule_index
                break

        if matching_invoices:
            return matching_invoices, match_rule

    return [], None


def match_bundle(T5, Invoice_table):
    matching_invoices = []
    total_invoice_source_amt = 0
    match_rule = None

    for Invoice in Invoice_table:
        for rule_index, rule in enumerate(rules, start=1):
            if rule(T5, Invoice=Invoice):
                matching_invoices.append((Invoice, rule_index))
                total_invoice_source_amt += float(Invoice['inv_match_source_amt'])
                break

        if total_invoice_source_amt == float(T5['fin_source_amt']):
            return matching_invoices, match_rule

    return [], None


output_rows_one_to_one = []
unmatched_output_rows_one_to_one = []
output_rows_bundle = []
unmatched_output_rows_bundle = []
matched_invoices = []

for T5 in T5_table:
    matching_invoices, match_rule = match_one_to_one(T5, Invoice_table)
    match_type = "one-to-one"

    if matching_invoices:
        for invoice, rule_index in matching_invoices:
            description = rule_descriptions.get(rule_index)
            output_row = {column: T5[column] if column in T5 else invoice[column] for column in output_columns}
            output_row['Match Rule'] = rule_numbers.get(match_rule)
            output_row['Description'] = description
            output_row['Match Type'] = match_type
            output_rows_one_to_one.append(output_row)
            matched_invoices.append(invoice)

        # Remove the matched invoice from Invoice_table
        Invoice_table = [inv for inv in Invoice_table if inv not in matched_invoices]

        # Remove the matched transaction from T5_table
        T5_table = [T5_row for T5_row in T5_table if T5_row['fin_record_key'] != T5['fin_record_key']]
        unmatched_fin_record_keys.remove(T5['fin_record_key'])

        unmatched_indices = unmatched_output_df[
            (unmatched_output_df['fin_record_key'] == T5['fin_record_key']) &
            (unmatched_output_df['fin_source_amt'] == T5['fin_source_amt']) &
            (unmatched_output_df['fin_debit_credit_ind'] == T5['fin_debit_credit_ind'])
        ].index

        unmatched_output_df.drop(unmatched_indices, inplace=True)

    else:
        unmatched_output_row = {column: T5[column] if column in T5 else None for column in unmatched_output_columns}
        unmatched_output_rows_one_to_one.append(unmatched_output_row)
        unmatched_fin_record_keys.add(T5['fin_record_key'])

for T5 in T5_table:
    matching_invoices, match_rule = match_bundle(T5, Invoice_table)
    match_type = "bundle match"

    if matching_invoices:
        for invoice, rule_index in matching_invoices:
            description = rule_descriptions.get(rule_index)
            output_row = {column: T5[column] if column in T5 else invoice[column] for column in output_columns}
            output_row['Match Rule'] = rule_numbers.get(match_rule)
            output_row['Description'] = description
            output_row['Match Type'] = match_type
            output_rows_bundle.append(output_row)
            matched_invoices.append(invoice)

        # Remove the matched invoices from Invoice_table
        Invoice_table = [inv for inv in Invoice_table if inv not in matched_invoices]

        # Remove the matched transaction from T5_table
        T5_table = [T5_row for T5_row in T5_table if T5_row['fin_record_key'] != T5['fin_record_key']]
        unmatched_fin_record_keys.discard(T5['fin_record_key'])

        unmatched_indices = unmatched_output_df[
            (unmatched_output_df['fin_record_key'] == T5['fin_record_key']) &
            (unmatched_output_df['fin_source_amt'] == T5['fin_source_amt'])
        ].index

        unmatched_output_df.drop(unmatched_indices, inplace=True)

    else:
        unmatched_output_rows_bundle.append(T5)












import re

def is_valid_rule(T5, Invoice):
    authno_T5 = T5['authno']
    authno_Invoice = Invoice['authno']

    if authno_T5.isalpha() and authno_Invoice.isalpha():
        return False  # Both values are alphabetic, so not a valid match
    else:
        return authno_T5 == authno_Invoice

def match_one_to_one(T5, Invoice_table):
    matching_invoices = []
    match_rule = None
    invoices_to_remove = []

    for Invoice in Invoice_table:
        for rule_index, rule in enumerate(rules, start=1):
            if (
                rule(T5, Invoice=Invoice)
                and float(T5['fin_source_amt']) == float(Invoice['inv_match_source_amt'])
                and is_valid_rule(T5, Invoice)  # Check if the rule is valid
            ):
                matching_invoices.append((Invoice, rule_index))
                match_rule = rule_index
                invoices_to_remove.append(Invoice)
                break

    for invoice in invoices_to_remove:
        Invoice_table.remove(invoice)

    if matching_invoices:
        return matching_invoices, match_rule
    else:
        return [], None


def match_bundle(T5, Invoice_table):
    matching_invoices = []
    total_invoice_source_amt = 0
    match_rule = None
    invoices_to_remove = []

    for Invoice in Invoice_table:
        for rule_index, rule in enumerate(rules, start=1):
            if (
                rule(T5, Invoice=Invoice)
                and is_valid_rule(T5, Invoice)  # Check if the rule is valid
            ):
                matching_invoices.append((Invoice, rule_index))
                total_invoice_source_amt += float(Invoice['inv_match_source_amt'])
                invoices_to_remove.append(Invoice)

        if total_invoice_source_amt == float(T5['fin_source_amt']):
            break

    for invoice in invoices_to_remove:
        Invoice_table.remove(invoice)

    if matching_invoices:
        return matching_invoices, match_rule
    else:
        return [], None






def match_one_to_one(T5, Invoice_table):
    matching_invoices = []
    match_rule = None

    for i, Invoice in enumerate(Invoice_table):
        if Invoice in matching_invoices:
            continue  # Skip already matched invoices

        for rule_index, rule in enumerate(rules, start=1):
            if rule(T5, Invoice=Invoice) and float(T5['fin_source_amt']) == float(Invoice['inv_match_source_amt']):
                matching_invoices.append((Invoice, rule_index))
                match_rule = rule_index
                break

        if matching_invoices:
            break

    for invoice, _ in matching_invoices:
        Invoice_table.remove(invoice)

    if matching_invoices:
        return matching_invoices, match_rule
    else:
        return [], None





def match_bundle(T5, Invoice_table):
    matching_invoices = []
    unmatched_invoices = Invoice_table[:]
    total_invoice_source_amt = 0
    match_rule = None

    for rule_index, rule in enumerate(rules, start=1):
        matched_invoices = []

        for Invoice in unmatched_invoices:
            if rule(T5, Invoice=Invoice):
                invoice_amt = float(Invoice['inv_match_source_amt'])
                remaining_amt = float(T5['fin_source_amt']) - total_invoice_source_amt

                if invoice_amt <= remaining_amt:
                    matched_invoices.append((Invoice, rule_index))

        if matched_invoices:
            best_match = max(matched_invoices, key=lambda x: float(x[0]['inv_match_source_amt']))
            matching_invoices.append(best_match)
            total_invoice_source_amt += float(best_match[0]['inv_match_source_amt'])
            unmatched_invoices.remove(best_match[0])
            match_rule = rule_index

        if total_invoice_source_amt == float(T5['fin_source_amt']):
            break

    if total_invoice_source_amt == float(T5['fin_source_amt']):
        return matching_invoices, match_rule
    else:
        return [], unmatched_invoices







def match_bundle(T5, Invoice_table):
    matching_invoices = []
    unmatched_invoices = Invoice_table[:]
    total_invoice_source_amt = 0
    match_rule = None

    for rule_index, rule in enumerate(rules, start=1):
        for i in range(len(unmatched_invoices)):
            Invoice = unmatched_invoices[i]
            if Invoice in matching_invoices:
                continue  # Skip already matched invoices

            if rule(T5, Invoice=Invoice):
                invoice_amt = float(Invoice['inv_match_source_amt'])
                remaining_amt = float(T5['fin_source_amt']) - total_invoice_source_amt

                if invoice_amt <= remaining_amt:
                    matching_invoices.append((Invoice, rule_index))
                    total_invoice_source_amt += invoice_amt

        if total_invoice_source_amt == float(T5['fin_source_amt']):
            match_rule = rule_index
            break

    if total_invoice_source_amt == float(T5['fin_source_amt']):
        return matching_invoices, match_rule
    else:
        return [], unmatched_invoices






import pandas as pd

def match_one_to_one(T5, Invoice_table):
    matching_invoices = []
    match_rule = None

    for i, Invoice in enumerate(Invoice_table):
        for rule_index, rule in enumerate(rules, start=1):
            if rule_index == 25 and (
                (T5['fin_orig_supplier_nm'] is None or pd.isnull(T5['fin_orig_supplier_nm'])) and
                (Invoice['inv_ticket_num'] is None or pd.isnull(Invoice['inv_ticket_num']))
            ):
                continue

            if rule(T5, Invoice=Invoice) and float(T5['fin_source_amt']) == float(Invoice['inv_match_source_amt']):
                matching_invoices.append((Invoice, rule_index))
                match_rule = rule_index
                break

        if matching_invoices:
            break

    return matching_invoices, match_rule









import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Assuming you have two DataFrames representing the tables
finrecord_df = pd.DataFrame(data={'finrecord_key': [1, 2, 3], 'column1': [...], 'column2': [...]})
invoice_df = pd.DataFrame(data={'invoice_id': [101, 102, 103], 'column1': [...], 'column2': [...]})

# Assuming you have two arrays representing the desired columns for each table
table1_columns = finrecord_df[['column1', 'column2', ...]].values
table2_columns = invoice_df[['column1', 'column2', ...]].values

similarity_matrix = cosine_similarity(table1_columns, table2_columns)

column_thresholds = {
    'column1': 0.8,
    'column2': 0.9,
    ...
}

matching_pairs = []
for i, row in enumerate(similarity_matrix):
    for j, score in enumerate(row):
        if all(score >= threshold for threshold in column_thresholds.values()):
            matching_pairs.append((i, j))

unmatched_invoice_ids = set(range(len(table2_columns)))

for pair in matching_pairs:
    finrecord_key = finrecord_df['finrecord_key'].iloc[pair[0]]  # finrecord_key from Table 1
    invoice_id = invoice_df['invoice_id'].iloc[pair[1]]  # invoice_id from Table 2
    unmatched_invoice_ids.remove(pair[1])
    print(f"Matching pair: finrecord_key={finrecord_key}, invoice_id={invoice_id}")

for unmatched_id in unmatched_invoice_ids:
    invoice_id = invoice_df['invoice_id'].iloc[unmatched_id]  # invoice_id from Table 2
    print(f"Unmatched invoice_id: {invoice_id}")







import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import CountVectorizer

# Assuming you have two DataFrames representing the tables
finrecord_df = pd.DataFrame(data={'finrecord_key': [1, 2, 3], 'column1': ['text1', 'text2', 'text3'], 'column2': ['A', 'B', 'C']})
invoice_df = pd.DataFrame(data={'invoice_id': [101, 102, 103], 'column1': ['text2', 'text4', 'text1'], 'column2': ['B', 'D', 'A']})

# Text preprocessing
text_cols = ['column1']
for col in text_cols:
    finrecord_df[col] = finrecord_df[col].str.lower().replace('[^a-zA-Z0-9]', '', regex=True)
    invoice_df[col] = invoice_df[col].str.lower().replace('[^a-zA-Z0-9]', '', regex=True)

# One-hot encoding for alphanumeric columns
alphanumeric_cols = ['column2']
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')

# Fit and transform the encoder on the combined dataset of both tables
combined_df = pd.concat([finrecord_df[alphanumeric_cols], invoice_df[alphanumeric_cols]])
encoded_features = encoder.fit_transform(combined_df)

# Split the encoded features back into separate arrays for each table
num_finrecord_rows = len(finrecord_df)
encoded_finrecord = encoded_features[:num_finrecord_rows]
encoded_invoice = encoded_features[num_finrecord_rows:]

# Assuming you have two arrays representing the desired columns for each table
table1_columns = np.concatenate((finrecord_df[text_cols].values, encoded_finrecord), axis=1)
table2_columns = np.concatenate((invoice_df[text_cols].values, encoded_invoice), axis=1)

# Calculate cosine similarity
similarity_matrix = cosine_similarity(table1_columns, table2_columns)

column_thresholds = {
    'column1': 0.8,
    'column2': 0.9,
    ...
}

matching_pairs = []
for i, row in enumerate(similarity_matrix):
    for j, score in enumerate(row):
        if all(score >= threshold for threshold in column_thresholds.values()):
            matching_pairs.append((i, j))

unmatched_invoice_ids = set(range(len(table2_columns)))

for pair in matching_pairs:
    finrecord_key = finrecord_df['finrecord_key'].iloc[pair[0]]  # finrecord_key from Table 1
    invoice_id = invoice_df['invoice_id'].iloc[pair[1]]  # invoice_id from Table 2
    unmatched_invoice_ids.remove(pair[1])
    print(f"Matching pair: finrecord_key={finrecord_key}, invoice_id={invoice_id}")

for unmatched_id in unmatched_invoice_ids:
    invoice_id = invoice_df['invoice_id'].iloc[unmatched_id]  # invoice_id from Table 2
    print(f"Unmatched invoice_id: {invoice_id}")














import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Load your transaction and invoice data into pandas DataFrames
transaction_data = pd.read_csv('transaction_data.csv')
invoice_data = pd.read_csv('invoice_data.csv')

# Select relevant columns for matching
transaction_columns = ['supplier_name', 'source_amount', 'other_column']
transaction_features = transaction_data[transaction_columns].astype(str)
invoice_features = invoice_data['ticket_number'].astype(str).str[:5]

# Create a set to store matched invoice indices
matched_indices = set()

# Create CountVectorizer to convert text data to vectors
vectorizer = CountVectorizer()

# Fit and transform the transaction names
transaction_vectors = vectorizer.fit_transform(transaction_features['supplier_name'])

# Transform the invoice names
invoice_vectors = vectorizer.transform(invoice_features)

# Calculate the cosine similarity matrix
similarity_matrix = cosine_similarity(transaction_vectors, invoice_vectors)

# Add threshold values for each column
thresholds = {
    'supplier_name': 0.8,
    'source_amount': 0.9,
    'other_column': 0.7
}

# Find the best matches
threshold = 0.8  # Adjust the threshold based on your needs
matches = []
for transaction_idx, transaction_name in enumerate(transaction_features['supplier_name']):
    best_match_score = -1
    best_match_idx = -1
    for invoice_idx, _ in enumerate(invoice_features):
        if invoice_idx not in matched_indices:  # Check if the invoice has already been matched
            similarity_score = similarity_matrix[transaction_idx, invoice_idx]
            if similarity_score > best_match_score:
                best_match_score = similarity_score
                best_match_idx = invoice_idx
    if best_match_score >= threshold:
        transaction_id = transaction_data.iloc[transaction_idx]['fin_record_key']
        invoice_id = invoice_data.iloc[best_match_idx]['inv_unmatch_inv_id']
        matches.append((transaction_id, invoice_id, best_match_idx, best_match_score))
        matched_indices.add(best_match_idx)

# Display and save the matches
output_file = 'matches.csv'
with open(output_file, 'w') as file:
    file.write("Transaction ID, Invoice ID, Best Match Index, Score\n")
    for match in matches:
        file.write(f"{match[0]}, {match[1]}, {match[2]}, {match[3]}\n")
        print(f"Match found: Transaction ID {match[0]} -> Invoice ID {match[1]}, "
              f"Best Match Index: {match[2]}, Score: {match[3]}")

print(f"Matches saved to {output_file}")











matches = []
for transaction_idx, transaction_name in enumerate(transaction_features['supplier_name']):
    best_match_score = -1
    best_match_idx = -1
    for invoice_idx, _ in enumerate(invoice_features):
        if invoice_idx not in matched_indices:  # Check if the invoice has already been matched
            similarity_score = similarity_matrix[transaction_idx, invoice_idx]
            if similarity_score > best_match_score:
                # Check if the best match score meets the threshold for each column
                if (similarity_score >= thresholds['supplier_name'] and
                        float(transaction_features.iloc[transaction_idx]['source_amount']) >= thresholds['source_amount'] and
                        float(transaction_features.iloc[transaction_idx]['other_column']) >= thresholds['other_column']):
                    best_match_score = similarity_score
                    best_match_idx = invoice_idx
    if best_match_idx != -1:
        transaction_id = transaction_data.iloc[transaction_idx]['fin_record_key']
        invoice_id = invoice_data.iloc[best_match_idx]['inv_unmatch_inv_id']
        matches.append((transaction_id, invoice_id, best_match_idx, best_match_score))
        matched_indices.add(best_match_idx)








import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Load your transaction and invoice data into pandas DataFrames
transaction_data = pd.read_csv('transaction_data.csv')
invoice_data = pd.read_csv('invoice_data.csv')

# Select relevant columns for matching
transaction_columns = ['supplier_name', 'source_amount', 'other_column']
invoice_columns = ['invoice_number', 'customer_name', 'invoice_amount']
transaction_features = transaction_data[transaction_columns].astype(str)
invoice_features = invoice_data[invoice_columns].astype(str)

# Create a set to store matched invoice indices
matched_indices = set()

# Create CountVectorizer to convert text data to vectors
vectorizer = CountVectorizer()

# Fit and transform the transaction names
transaction_vectors = vectorizer.fit_transform(transaction_features['supplier_name'])

# Transform the invoice names
invoice_vectors = vectorizer.transform(invoice_features['customer_name'])

# Calculate the cosine similarity matrix
similarity_matrix = cosine_similarity(transaction_vectors, invoice_vectors)

thresholds = {'supplier_name': 0.1, 'source_amount': 0.2, 'other_column': 0.3}

# Find the best matches
matches = []
for transaction_idx in range(len(transaction_data)):
    best_match_score = -1
    best_match_idx = -1
    for invoice_idx in range(len(invoice_data)):
        if invoice_idx not in matched_indices:  # Skip already matched invoices
            similarity_score = similarity_matrix[transaction_idx, invoice_idx]
            match_scores = []
            for column in transaction_columns:
                if column in invoice_columns:
                    column_index = invoice_columns.index(column)
                    threshold = thresholds[column]
                    column_similarity_score = cosine_similarity(
                        vectorizer.transform([transaction_features.loc[transaction_idx, column]]),
                        vectorizer.transform([invoice_features.loc[invoice_idx, column_index]])
                    )[0][0]
                    match_scores.append(column_similarity_score)
            if all(score >= threshold for score, threshold in zip(match_scores, thresholds.values())):
                if similarity_score > best_match_score:
                    best_match_score = similarity_score
                    best_match_idx = invoice_idx
    if best_match_idx != -1:
        matched_indices.add(best_match_idx)  # Add the matched invoice index to the set
        transaction_id = transaction_data.iloc[transaction_idx]['fin_record_key']
        invoice_id = invoice_data.iloc[best_match_idx]['inv_unmatch_inv_id']
        matched_column = [column for column, score in zip(transaction_columns, match_scores) if score >= thresholds[column]]
        transaction_values = transaction_data.iloc[transaction_idx][transaction_columns]
        invoice_values = invoice_data.iloc[best_match_idx][invoice_columns]
        matches.append((transaction_id, invoice_id, transaction_values, invoice_values, matched_column))

# Display and save the matches
output_file = 'matches.csv'
with open(output_file, 'w') as file:
    file.write("Transaction ID, Invoice ID, Matched Column, Transaction Values, Invoice Values\n")
    for match in matches:
        file.write(f"{match[0]}, {match[1]}, {match[4]}, {match[2]}, {match[3]}\n")
        print(f"Match found: Transaction ID {match[0]} -> Invoice ID {match[1]}")
        print(f"Matched Column: {match[4]}")
        print(f"Transaction Values: {match[2]}")
        print(f"Invoice Values: {match[3]}\n")

print(f"Matches saved to {output_file}")










import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Load your transaction and invoice data into pandas DataFrames
transaction_data = pd.read_csv('transaction_data.csv')
invoice_data = pd.read_csv('invoice_data.csv')

# Select relevant columns for matching
transaction_columns = ['supplier_name', 'source_amount', 'other_column']
invoice_columns = ['invoice_number', 'customer_name', 'invoice_amount']
transaction_features = transaction_data[transaction_columns].astype(str)
invoice_features = invoice_data[invoice_columns].astype(str)

# Create a set to store matched invoice indices
matched_indices = set()

# Create CountVectorizer to convert text data to vectors
vectorizer = CountVectorizer()

# Fit and transform the transaction names
transaction_vectors = vectorizer.fit_transform(transaction_features['supplier_name'])

# Transform the invoice names
invoice_vectors = vectorizer.transform(invoice_features['customer_name'])

# Calculate the cosine similarity matrix
similarity_matrix = cosine_similarity(transaction_vectors, invoice_vectors)

thresholds = {'supplier_name': 0.1, 'source_amount': None, 'other_column': 0.3}

# Find the best matches
matches = []
for transaction_idx in range(len(transaction_data)):
    best_match_score = -1
    best_match_idx = -1
    for invoice_idx in range(len(invoice_data)):
        if invoice_idx not in matched_indices:  # Skip already matched invoices
            similarity_score = similarity_matrix[transaction_idx, invoice_idx]
            match_scores = []
            for column in transaction_columns:
                if column in invoice_columns:
                    column_index = invoice_columns.index(column)
                    threshold = thresholds[column]
                    if column == 'source_amount':
                        if transaction_features.loc[transaction_idx, column] == invoice_features.loc[invoice_idx, column_index]:
                            match_scores.append(1.0)
                        else:
                            match_scores.append(0.0)
                    else:
                        column_similarity_score = cosine_similarity(
                            vectorizer.transform([transaction_features.loc[transaction_idx, column]]),
                            vectorizer.transform([invoice_features.loc[invoice_idx, column_index]])
                        )[0][0]
                        match_scores.append(column_similarity_score)
            if all(score >= threshold for score, threshold in zip(match_scores, thresholds.values())):
                if similarity_score > best_match_score:
                    best_match_score = similarity_score
                    best_match_idx = invoice_idx
    if best_match_idx != -1:
        matched_indices.add(best_match_idx)  # Add the matched invoice index to the set
        transaction_id = transaction_data.iloc[transaction_idx]['fin_record_key']
        invoice_id = invoice_data.iloc[best_match_idx]['inv_unmatch_inv_id']
        matched_columns = [column for column, score in zip(transaction_columns, match_scores) if score >= thresholds[column]]
        transaction_amt = transaction_data.iloc[transaction_idx]['source_amount']
        invoice_amt = invoice_data.iloc[best_match_idx]['invoice_amount']
        matches.append((transaction_id, invoice_id, matched_columns, transaction_amt, invoice_amt))

# Display and save the matches
output_file = 'matches.csv'
with open(output_file, 'w') as file:
    file.write("Fin Record Key, Invoice ID, Matched Columns, Transaction Amount, Invoice Amount\n")
    for match in matches:
        file.write(f"{match[0]}, {match[1]}, {', '.join(match[2])}, {match[3]}, {match[4]}\n")
        print(f"Match found: Fin Record Key {match[0]} -> Invoice ID {match[1]}")
        print(f"Matched Columns: {', '.join(match[2])}")
        print(f"Transaction Amount: {match[3]}")
        print(f"Invoice Amount: {match[4]}\n")

print(f"Matches saved to {output_file}")
















from sklearn.metrics.pairwise import cosine_similarity

threshold = 70
matches = []
count = 0

for transaction_idx, transaction_row in transaction_features.iterrows():
    amt = transaction_row['fin_source_amt']
    debit_credit_indicator = transaction_row['fin debit credit_ind']
    transaction_account_no = transaction_row['fin_acct_num (masked)']

    best_match_score = -1
    best_match_idx = -1

    for invoice_idx, invoice_row in invoice_features.items():
        if invoice_idx not in matched_indices: # Check if the invoice has already been matched
            invoice_amount = invoice_amt[invoice_idx]
            invoice_credit_debit = invoice_debit_credit_indicator[invoice_idx]
            inv_account = invoice_account_number[invoice_idx]
            inv_account_final = inv_account[-4:]

            source_amt_score = cosine_similarity(amt, invoice_amount)
            c_or_d_indicator_score = cosine_similarity(debit_credit_indicator, invoice_credit_debit)
            account_no_score = cosine_similarity(transaction_account_no, inv_account_final)

            average_score = (source_amt_score + c_or_d_indicator_score + account_no_score) / 3

            if source_amt_score >= threshold and c_or_d_indicator_score >= threshold:
                matches.append((transaction_idx, invoice_idx, average_score))
                count += 1

print(count)





import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

threshold = 70
matches = []
count = 0

for transaction_idx, transaction_row in transaction_features.iterrows():
    amt = np.array(transaction_row['fin_source_amt']).reshape(1, -1)
    debit_credit_indicator = np.array(transaction_row['fin debit credit_ind']).reshape(1, -1)
    transaction_account_no = np.array(transaction_row['fin_acct_num (masked)']).reshape(1, -1)

    best_match_score = -1
    best_match_idx = -1

    for invoice_idx, invoice_row in invoice_features.items():
        if invoice_idx not in matched_indices: # Check if the invoice has already been matched
            invoice_amount = np.array(invoice_amt[invoice_idx]).reshape(1, -1)
            invoice_credit_debit = np.array(invoice_debit_credit_indicator[invoice_idx]).reshape(1, -1)
            inv_account = invoice_account_number[invoice_idx]
            inv_account_final = np.array(inv_account[-4:]).reshape(1, -1)

            source_amt_score = cosine_similarity(amt, invoice_amount)[0][0]
            c_or_d_indicator_score = cosine_similarity(debit_credit_indicator, invoice_credit_debit)[0][0]
            account_no_score = cosine_similarity(transaction_account_no, inv_account_final)[0][0]

            average_score = (source_amt_score + c_or_d_indicator_score + account_no_score) / 3

            if source_amt_score >= threshold and c_or_d_indicator_score >= threshold:
                matches.append((transaction_idx, invoice_idx, average_score))
                count += 1

print(count)


import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder

threshold = 70
matches = []
count = 0

# Label encoding for debit_credit_indicator column
label_encoder = LabelEncoder()
label_encoder.fit(transaction_features['fin debit credit_ind'])

for transaction_idx, transaction_row in transaction_features.iterrows():
    amt = np.array(transaction_row['fin_source_amt']).reshape(1, -1)
    debit_credit_indicator = np.array(label_encoder.transform([transaction_row['fin debit credit_ind']])).reshape(1, -1)
    transaction_account_no = np.array(transaction_row['fin_acct_num (masked)']).reshape(1, -1)

    best_match_score = -1
    best_match_idx = -1

    for invoice_idx, invoice_row in invoice_features.items():
        if invoice_idx not in matched_indices: # Check if the invoice has already been matched
            invoice_amount = np.array(invoice_amt[invoice_idx]).reshape(1, -1)
            invoice_credit_debit = np.array(label_encoder.transform([invoice_debit_credit_indicator[invoice_idx]])).reshape(1, -1)
            inv_account = invoice_account_number[invoice_idx]
            inv_account_final = np.array(inv_account[-4:]).reshape(1, -1)

            source_amt_score = cosine_similarity(amt, invoice_amount)[0][0]
            c_or_d_indicator_score = cosine_similarity(debit_credit_indicator, invoice_credit_debit)[0][0]
            account_no_score = cosine_similarity(transaction_account_no, inv_account_final)[0][0]

            average_score = (source_amt_score + c_or_d_indicator_score + account_no_score) / 3

            if source_amt_score >= threshold and c_or_d_indicator_score >= threshold:
                matches.append((transaction_idx, invoice_idx, average_score))
                count += 1

print(count)





import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Load your transaction and invoice data into pandas DataFrames
transaction_data = pd.read_csv('transaction_data.csv')
invoice_data = pd.read_csv('invoice_data.csv')

# Select relevant columns for matching
transaction_columns = ['supplier_name', 'inv_number', 'source_amount', 'other_column']
transaction_features = transaction_data[transaction_columns].astype(str)
invoice_features = invoice_data[['ticket_number', 'inv_number']].astype(str).apply(lambda x: ' '.join(x), axis=1)

# Create a set to store matched invoice indices
matched_indices = set()

# Create CountVectorizer to convert text data to vectors
vectorizer = CountVectorizer()

# Fit and transform the transaction names
transaction_vectors = vectorizer.fit_transform(transaction_features['supplier_name'] + ' ' + transaction_features['inv_number'])

# Transform the invoice names
invoice_vectors = vectorizer.transform(invoice_features)

# Calculate the cosine similarity matrix
similarity_matrix = cosine_similarity(transaction_vectors, invoice_vectors)

threshold = 0.1
# Find the best matches
matches = []
for transaction_idx in range(len(transaction_data)):
    best_match_score = -1
    best_match_idx = -1
    for invoice_idx in range(len(invoice_data)):
        similarity_score = similarity_matrix[transaction_idx, invoice_idx]
        if similarity_score > best_match_score and similarity_score >= threshold:
            best_match_score = similarity_score
            best_match_idx = invoice_idx
    if best_match_idx != -1:
        transaction_id = transaction_data.iloc[transaction_idx]['fin_record_key']
        invoice_id = invoice_data.iloc[best_match_idx]['inv_unmatch_inv_id']
        # Get the matching column values
        transaction_values = transaction_data.iloc[transaction_idx][transaction_columns]
        invoice_values = invoice_data.iloc[best_match_idx][['ticket_number', 'inv_number']]
        # Add matched values to the matches list
        matches.append((transaction_id, invoice_id, transaction_values, invoice_values))

# Display and save the matches
output_file = 'matches.csv'
with open(output_file, 'w') as file:
    file.write("Transaction ID, Invoice ID, Supplier Name, Inv Number, Transaction Values, Invoice Values\n")
    for match in matches:
        file.write(f"{match[0]}, {match[1]}, {match[2]}, {match[3]}\n")
        print(f"Match found: Transaction ID {match[0]} -> Invoice ID {match[1]}")
        print(f"Transaction Values: {match[2]}")
        print(f"Invoice Values: {match[3]}\n")

print(f"Matches saved to {output_file}")










import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Load your transaction and invoice data into pandas DataFrames
transaction_data = pd.read_csv('transaction_data.csv')
invoice_data = pd.read_csv('invoice_data.csv')

# Select relevant columns for matching
transaction_columns = ['supplier_name', 'fin_purc_id', 'source_amt', 'other_column']
transaction_features = transaction_data[transaction_columns].astype(str)
invoice_features = invoice_data[['supplier_name', 'ticket_number', 'inv_number', 'source_amt']].astype(str)

# Set the thresholds for each column
thresholds = {
    'supplier_name': 0.7,
    'fin_purc_id': 0.7,
    'source_amt': 0.9
}

# Create a set to store matched invoice indices
matched_indices = set()

# Create CountVectorizer to convert text data to vectors
vectorizer = CountVectorizer()

# Fit and transform the transaction names
transaction_vectors = vectorizer.fit_transform(transaction_features['supplier_name'])
invoice_vectors = vectorizer.transform(invoice_features['supplier_name'])

# Calculate the cosine similarity matrix
similarity_matrix = cosine_similarity(transaction_vectors, invoice_vectors)

# Find the best matches
matches = []
for transaction_idx, transaction_row in transaction_features.iterrows():
    transaction_supplier = transaction_row['supplier_name']
    transaction_purc_id = transaction_row['fin_purc_id']
    transaction_source_amt = transaction_row['source_amt']
    best_match_score = -1
    best_match_idx = -1
    for invoice_idx, invoice_row in invoice_features.iterrows():
        if invoice_idx not in matched_indices:  # Check if the invoice has already been matched
            invoice_supplier = invoice_row['supplier_name']
            invoice_ticket = invoice_row['ticket_number']
            invoice_inv = invoice_row['inv_number']
            invoice_source_amt = invoice_row['source_amt']

            supplier_score = cosine_similarity(transaction_vectors[transaction_idx], invoice_vectors[invoice_idx])[0][0]
            purc_id_score = cosine_similarity(vectorizer.transform([transaction_purc_id]), vectorizer.transform([invoice_ticket]))[0][0]
            inv_id_score = cosine_similarity(vectorizer.transform([transaction_purc_id]), vectorizer.transform([invoice_inv]))[0][0]
            source_amt_score = 1.0 if transaction_source_amt == invoice_source_amt else 0.0

            average_score = (supplier_score + purc_id_score + inv_id_score + source_amt_score) / 4

            if (
                supplier_score >= thresholds['supplier_name'] and
                purc_id_score >= thresholds['fin_purc_id'] and
                inv_id_score >= thresholds['fin_purc_id'] and
                source_amt_score >= thresholds['source_amt']
            ):
                best_match_score = average_score
                best_match_idx = invoice_idx

    if best_match_idx != -1:
        transaction_id = transaction_data.iloc[transaction_idx]['fin_record_key']
        invoice_id = invoice_data.iloc[best_match_idx]['inv_unmatch_inv_id']
        # Get the matching column values
        transaction_values = transaction_data.iloc[transaction_idx][transaction_columns]
        invoice_values = invoice_data.iloc[best_match_idx][transaction_columns]
        # Add matched values to the matches list
        matches.append((transaction_id, invoice_id, best_match_score, transaction_values, invoice_values))
        matched_indices.add(best_match_idx)

# Display and save the matches
output_file = 'matches.csv'
with open(output_file, 'w') as file:
    file.write("Transaction ID, Invoice ID, Match Score, Transaction Values, Invoice Values\n")
    for match in matches:
        file.write(f"{match[0]}, {match[1]}, {match[2]}, {match[3]}, {match[4]}\n")
        print(f"Match found: Transaction ID {match[0]} -> Invoice ID {match[1]}")
        print(f"Transaction Values: {match[3]}")
        print(f"Invoice Values: {match[4]}\n")

print(f"Matches saved to {output_file}")












import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Load your transaction and invoice data into pandas DataFrames
transaction_data = pd.read_csv('transaction_data.csv')
invoice_data = pd.read_csv('invoice_data.csv')

# Select relevant columns for matching
transaction_columns = ['supplier_name', 'source_amount', 'auth_no']
invoice_columns = ['ticket_number', 'inv_number', 'auth_no', 'amount', 'company_id']

# Set thresholds for each condition
thresholds = {
    'supplier_name_ticket': 0.1,
    'supplier_name_inv': 0.1,
    'auth_no': 0.1,
    'source_amount': 0.2,
}

# Create CountVectorizer to convert text data to vectors
vectorizer = CountVectorizer()

# Fit and transform the transaction names
transaction_vectors = vectorizer.fit_transform(transaction_data['supplier_name'])

# Transform the invoice names
invoice_vectors = vectorizer.transform(invoice_data['ticket_number'].astype(str))

# Calculate the cosine similarity matrix
similarity_matrix = cosine_similarity(transaction_vectors, invoice_vectors)

# Find the best matches
matches = []
for transaction_idx, transaction_row in transaction_data.iterrows():
    transaction_id = transaction_row['fin_record_key']
    transaction_supplier_name = transaction_row['supplier_name']
    transaction_source_amount = transaction_row['source_amount']
    transaction_auth_no = transaction_row['auth_no']
    
    best_match_score = -1
    best_match_idx = -1
    
    for invoice_idx, invoice_row in invoice_data.iterrows():
        invoice_ticket_number = str(invoice_row['ticket_number'])
        invoice_supplier_name = str(invoice_row['inv_number'])
        invoice_auth_no = invoice_row['auth_no']
        invoice_amount = invoice_row['amount']
        invoice_company_id = invoice_row['company_id']
        
        similarity_score = similarity_matrix[transaction_idx, invoice_idx]
        
        # Check if any condition is a match
        if (transaction_supplier_name == invoice_ticket_number and similarity_score >= thresholds['supplier_name_ticket']) or \
           (transaction_supplier_name == invoice_supplier_name and similarity_score >= thresholds['supplier_name_inv']) or \
           (transaction_auth_no == invoice_auth_no and similarity_score >= thresholds['auth_no']):
            
            if float(transaction_source_amount) == float(invoice_amount) and \
               transaction_data.loc[transaction_idx, 'company_id'] == invoice_company_id:
                if similarity_score > best_match_score:
                    best_match_score = similarity_score
                    best_match_idx = invoice_idx
    
    if best_match_idx != -1:
        invoice_id = invoice_data.loc[best_match_idx, 'inv_unmatch_inv_id']
        transaction_values = transaction_row[transaction_columns]
        invoice_values = invoice_data.loc[best_match_idx, invoice_columns]
        matches.append((transaction_id, invoice_id, transaction_values, invoice_values, best_match_score))

# Display and save the matches
output_file = 'matches.csv'
with open(output_file, 'w') as file:
    file.write("Transaction ID, Invoice ID, Transaction Values, Invoice Values, Similarity Score\n")
    for match in matches:
        transaction_id, invoice_id, transaction_values, invoice_values, similarity_score = match
        file.write(f"{transaction_id}, {invoice_id}, {transaction_values}, {invoice_values}, {similarity_score}\n")













import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Load your transaction and invoice data into pandas DataFrames
transaction_data = pd.read_csv('transaction_data.csv')
invoice_data = pd.read_csv('invoice_data.csv')

# Select relevant columns for matching
transaction_columns = ['supplier_name', 'source_amount', 'auth_no']
invoice_columns = ['ticket_number', 'inv_number', 'auth_no', 'amount', 'company_id']

# Set thresholds for each condition
thresholds = {
    'supplier_name_ticket': 0.1,
    'supplier_name_inv': 0.1,
    'auth_no': 0.1,
    'source_amount': 0.2,
}

# Concatenate relevant columns into a single string representation for each row
transaction_data['transaction_string'] = transaction_data[transaction_columns].astype(str).agg(' '.join, axis=1)
invoice_data['invoice_string'] = invoice_data[invoice_columns].astype(str).agg(' '.join, axis=1)

# Create CountVectorizer to convert text data to vectors
vectorizer = CountVectorizer()

# Fit and transform the transaction strings
transaction_vectors = vectorizer.fit_transform(transaction_data['transaction_string'])

# Transform the invoice strings
invoice_vectors = vectorizer.transform(invoice_data['invoice_string'])

# Calculate the cosine similarity matrix
similarity_matrix = cosine_similarity(transaction_vectors, invoice_vectors)

# Find the best matches
matches = []
for transaction_idx, transaction_row in transaction_data.iterrows():
    transaction_id = transaction_row['fin_record_key']
    transaction_string = transaction_row['transaction_string']
    transaction_auth_no = transaction_row['auth_no']
    transaction_source_amount = transaction_row['source_amount']
    
    best_match_score = -1
    best_match_idx = -1
    
    for invoice_idx, invoice_row in invoice_data.iterrows():
        invoice_string = invoice_row['invoice_string']
        invoice_auth_no = invoice_row['auth_no']
        invoice_source_amount = invoice_row['amount']
        invoice_company_id = invoice_row['company_id']
        
        similarity_score = similarity_matrix[transaction_idx, invoice_idx]
        
        # Check if any condition is a match
        if (transaction_row['supplier_name'] == invoice_row['ticket_number'] and similarity_score >= thresholds['supplier_name_ticket']) or \
           (transaction_row['supplier_name'] == invoice_row['inv_number'] and similarity_score >= thresholds['supplier_name_inv']) or \
           (transaction_auth_no == invoice_auth_no and similarity_score >= thresholds['auth_no']) or \
           (abs(transaction_source_amount - invoice_source_amount) <= thresholds['source_amount']):
            
            if transaction_row['company_id'] == invoice_company_id:
                if similarity_score > best_match_score:
                    best_match_score = similarity_score
                    best_match_idx = invoice_idx
    
    if best_match_idx != -1:
        invoice_id = invoice_data.loc[best_match_idx, 'inv_unmatch_inv_id']
        transaction_values = transaction_row[transaction_columns]
        invoice_values = invoice_data.loc[best_match_idx, invoice_columns]
        matches.append((transaction_id, invoice_id, transaction_values, invoice_values, best_match_score))

# Display and save the matches
output_file = 'matches.csv'
with open(output_file, 'w') as file:
    file.write("Transaction ID, Invoice ID, Transaction Values, Invoice Values, Similarity Score\n")
    for match in matches:
        transaction_id, invoice_id, transaction_values, invoice_values, similarity_score = match
        file.write(f"{transaction_id}, {invoice_id}, {transaction_values}, {invoice_values}, {similarity_score}\n")










# Calculate the cosine similarity for each column
similarity_scores = {}
for column in transaction_columns:
    vectorizer = CountVectorizer()
    transaction_vectors = vectorizer.fit_transform(transaction_data[column].astype(str))
    invoice_vectors = vectorizer.transform(invoice_data[column].astype(str))
    similarity_matrix = cosine_similarity(transaction_vectors, invoice_vectors)
    similarity_scores[column] = similarity_matrix





final_match = pd.DataFrame(matches, columns=['Transaction ID', 'Invoice ID', 'Transaction Values', 'Invoice Values', 'Similarity Score'])
final_match = final_match.sort_values('Similarity Score', ascending=False)
final_match = final_match.drop_duplicates(subset='Invoice ID', keep='first')
print(final_match.head(50))







import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Load your transaction and invoice data into pandas DataFrames
transaction_data = pd.read_csv('transaction_data.csv')
invoice_data = pd.read_csv('invoice_data.csv')

# Select relevant columns for matching
transaction_columns = ['supplier_name', 'source_amount', 'auth_no']
invoice_columns = ['ticket_number', 'inv_number', 'auth_no', 'amount', 'company_id']

# Set thresholds for each condition
thresholds = {
    'supplier_name_ticket': 0.1,
    'supplier_name_inv': 0.1,
    'auth_no': 0.1,
    'source_amount': 0.2,
}

# Create CountVectorizer to convert text data to vectors
vectorizer = CountVectorizer()

# Calculate the cosine similarity for each relevant column
similarity_scores = {}
for column in transaction_columns + invoice_columns:
    transaction_vectors = vectorizer.fit_transform(transaction_data[column].astype(str))
    invoice_vectors = vectorizer.transform(invoice_data[column].astype(str))
    similarity_matrix = cosine_similarity(transaction_vectors, invoice_vectors)
    similarity_scores[column] = similarity_matrix

# Find the best matches
matches = []
matched_invoices = set()  # Set to store matched invoice IDs

for transaction_idx, transaction_row in transaction_data.iterrows():
    transaction_id = transaction_row['fin_record_key']
    transaction_auth_no = transaction_row['auth_no']
    transaction_source_amount = transaction_row['source_amount']
    
    best_match_score = -1
    best_match_idx = -1
    best_match_column = ''
    
    # Check if transaction is already matched
    if transaction_id in matched_invoices:
        continue
    
    for invoice_idx, invoice_row in invoice_data.iterrows():
        invoice_id = invoice_row['inv_unmatch_inv_id']
        
        # Skip already matched invoices
        if invoice_id in matched_invoices:
            continue
        
        invoice_auth_no = invoice_row['auth_no']
        invoice_source_amount = invoice_row['amount']
        invoice_company_id = invoice_row['company_id']
        
        similarity_scores_sum = 0
        num_matching_columns = 0
        
        for column in transaction_columns + invoice_columns:
            similarity_score = similarity_scores[column][transaction_idx, invoice_idx]
            similarity_scores_sum += similarity_score
            num_matching_columns += 1 if similarity_score >= thresholds[column] else 0
        
        average_similarity_score = similarity_scores_sum / num_matching_columns if num_matching_columns > 0 else 0
        
        # Check if all conditions are met
        if (transaction_row['supplier_name'] == invoice_row['ticket_number'] or
            transaction_row['supplier_name'] == invoice_row['supplier_name']) and \
           transaction_auth_no == invoice_auth_no and \
           abs(transaction_source_amount - invoice_source_amount) <= thresholds['source_amount'] and \
           transaction_row['company_id'] == invoice_company_id and \
           average_similarity_score >= thresholds['supplier_name_ticket']:
            
            if average_similarity_score > best_match_score:
                best_match_score = average_similarity_score
                best_match_idx = invoice_idx
                best_match_column = column
    
    if best_match_idx != -1:
        invoice_id = invoice_data.loc[best_match_idx, 'inv_unmatch_inv_id']
        
        if invoice_id not in matched_invoices:
            transaction_values = transaction_row[transaction_columns]
            invoice_values = invoice_data.loc[best_match_idx, invoice_columns]
            match_pattern = f"{best_match_column} - {transaction_values[best_match_column]} matched with {invoice_values[best_match_column]}"
            matches.append((transaction_id, invoice_id, transaction_values, invoice_values, best_match_score, match_pattern))
            matched_invoices.add(invoice_id)

# Create DataFrame for matches
final_match = pd.DataFrame(matches, columns=['transaction_id', 'invoice_id', 'transaction_values', 'invoice_values', 'similarity_score', 'match_pattern'])

# Sort matches by similarity score in descending order
final_match = final_match.sort_values('similarity_score', ascending=False)

# Drop duplicate invoices to keep only the best match for each transaction
final_match = final_match.drop_duplicates(subset='invoice_id', keep='first')

# Save the final matches to a CSV file
final_match.to_csv('final_matches.csv', index=False)










# Display and save the matches
output_file = 'matches.csv'
with open(output_file, 'w') as file:
    file.write("Transaction ID, Invoice ID, Transaction Values, Invoice Values, Similarity Score\n")
    for match in matches:
        transaction_id, invoice_id, transaction_values, invoice_values, similarity_score = match

        # Extract specific column values
        transaction_values_string = ', '.join([str(transaction_values[col]) for col in transaction_columns])
        invoice_values_string = ', '.join([str(invoice_values[col]) for col in invoice_columns])

        file.write(f"{transaction_id}, {invoice_id}, {transaction_values_string}, {invoice_values_string}, {similarity_score}\n")

# Create a DataFrame from the matches
final_match = pd.DataFrame(matches, columns=['Transaction ID', 'Invoice ID', 'Transaction Values', 'Invoice Values', 'Similarity Score'])

# Sort the DataFrame by 'Similarity Score' in ascending order
final_match = final_match.sort_values('Similarity Score', ascending=False)

# Drop duplicate invoices, keeping only the first occurrence
final_match = final_match.drop_duplicates(subset='Invoice ID', keep='first')

# Print the first 50 rows of the final matches DataFrame
print(final_match.head(50))





# Create CountVectorizer for each column separately
vectorizers = {}
for column in transaction_columns + invoice_columns:
    vectorizers[column] = CountVectorizer()

# Fit and transform the transaction and invoice data for each column
transaction_vectors = {}
invoice_vectors = {}
for column in transaction_columns:
    transaction_vectors[column] = vectorizers[column].fit_transform(transaction_data[column])
for column in invoice_columns:
    invoice_vectors[column] = vectorizers[column].transform(invoice_data[column].astype(str))

# Calculate the cosine similarity matrix for each column
similarity_matrices = {}
for column in transaction_columns:
    similarity_matrices[column] = cosine_similarity(transaction_vectors[column], invoice_vectors[column])

for transaction_idx, transaction_row in transaction_data.iterrows():
    transaction_id = transaction_row['fin_record_key']
    transaction_values = transaction_row[transaction_columns]
    
    best_match_score = -1
    best_match_idx = -1
    
    for invoice_idx, invoice_row in invoice_data.iterrows():
        invoice_id = invoice_row['inv_unmatch_inv_id']
        invoice_values = invoice_row[invoice_columns]
        
        similarity_scores = []
        
        # Calculate similarity scores for each column
        for column in transaction_columns:
            similarity_matrix = similarity_matrices[column]
            similarity_score = similarity_matrix[transaction_idx, invoice_idx]
            similarity_scores.append(similarity_score)
        
        # Check if all conditions are matched
        if all(score >= thresholds[column] for score, column in zip(similarity_scores, transaction_columns)):
            average_score = sum(similarity_scores) / len(similarity_scores)
            if average_score > best_match_score:
                best_match_score = average_score
                best_match_idx = invoice_idx
    
    if best_match_idx != -1:
        invoice_id = invoice_data.loc[best_match_idx, 'inv_unmatch_inv_id']
        invoice_values = invoice_data.loc[best_match_idx, invoice_columns]
        matches.append((transaction_id, invoice_id, transaction_values, invoice_values, best_match_score))








import pandas as pd
import numpy as np

def cosine_similarity(vector1, vector2):
    dot_product = np.dot(vector1, vector2)
    norm_vector1 = np.linalg.norm(vector1)
    norm_vector2 = np.linalg.norm(vector2)
    similarity = dot_product / (norm_vector1 * norm_vector2)
    return similarity

# Load your transaction and invoice data into pandas DataFrames
transaction_data = pd.read_csv('transaction_data.csv')
invoice_data = pd.read_csv('invoice_data.csv')

# Select relevant columns for matching
transaction_columns = ['supplier_name', 'source_amount', 'auth_no']
invoice_columns = ['ticket_number', 'inv_number', 'auth_no', 'amount', 'company_id']

# Set thresholds for each condition
thresholds = {
    'supplier_name': 0.1,
    'auth_no': 0.1,
    'source_amount': 0.2,
}

# Create CountVectorizer for each column separately
vectorizers = {}
for column in transaction_columns + invoice_columns:
    vectorizers[column] = CountVectorizer()

# Fit and transform the transaction and invoice data for each column
transaction_vectors = {}
invoice_vectors = {}
for column in transaction_columns:
    transaction_vectors[column] = vectorizers[column].fit_transform(transaction_data[column])
for column in invoice_columns:
    invoice_vectors[column] = vectorizers[column].transform(invoice_data[column].astype(str))

# Calculate the cosine similarity matrix for each column
similarity_matrices = {}
for column in transaction_columns:
    transaction_values = transaction_vectors[column].toarray()
    invoice_values = invoice_vectors[column].toarray()
    similarity_matrices[column] = np.zeros((transaction_values.shape[0], invoice_values.shape[0]))
    for i in range(transaction_values.shape[0]):
        for j in range(invoice_values.shape[0]):
            similarity_matrices[column][i, j] = cosine_similarity(transaction_values[i], invoice_values[j])




import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Example data
transaction_row = {
    'supplier_name': 'ABC',
    'auth_no': '123',
    'source_amount': 100,
    'company_id': 'XYZ'
}

invoice_row = {
    'ticket_number': '123',
    'supplier_name': 'ABC',
    'auth_no': '123',
    'source_amount': 102,
    'company_id': 'XYZ'
}

thresholds = {
    'source_amount': 2,
    'supplier_name_ticket': 0.8
}

# Function to calculate match based on given conditions
def calculate_match(transaction_row, invoice_row, thresholds):
    # Check condition 1: supplier_name match
    condition_1 = (transaction_row['supplier_name'] == invoice_row['ticket_number'] or
                  transaction_row['supplier_name'] == invoice_row['supplier_name'])
    
    # Check condition 2: auth_no match
    condition_2 = transaction_row['auth_no'] == invoice_row['auth_no']
    
    # Check condition 3: source_amount difference within threshold
    condition_3 = abs(transaction_row['source_amount'] - invoice_row['source_amount']) <= thresholds['source_amount']
    
    # Check condition 4: company_id match
    condition_4 = transaction_row['company_id'] == invoice_row['company_id']
    
    # Calculate average similarity score (assuming you have the code for this)
    average_similarity_score = calculate_average_similarity_score()
    
    # Check condition 5: average_similarity_score >= supplier_name_ticket threshold
    condition_5 = average_similarity_score >= thresholds['supplier_name_ticket']
    
    # Determine match based on all conditions
    match = condition_1 and condition_2 and condition_3 and condition_4 and condition_5
    
    return match

# Calculate match based on given conditions
is_match = calculate_match(transaction_row, invoice_row, thresholds)
print(is_match)




import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Example data
transaction_data = {
    'supplier_name': ['ABC', 'DEF', 'GHI'],
    'auth_no': ['123', '456', '789'],
    'source_amount': [100, 200, 300],
    'company_id': ['XYZ', 'ABC', 'DEF']
}

invoice_data = {
    'ticket_number': ['123', '456', '789'],
    'supplier_name': ['ABC', 'DEF', 'XYZ'],
    'auth_no': ['123', '456', '789'],
    'source_amount': [110, 190, 310],
    'company_id': ['XYZ', 'ABC', 'DEF']
}

thresholds = {
    'source_amount': 10,
    'supplier_name_ticket': 0.8
}

# Convert data to pandas DataFrames
transaction_df = pd.DataFrame(transaction_data)
invoice_df = pd.DataFrame(invoice_data)

# Function to convert columns to n-grams
def convert_to_ngrams(df, n=2):
    ngram_columns = []
    for column in df:
        ngram_vectorizer = CountVectorizer(ngram_range=(n, n))
        ngram_matrix = ngram_vectorizer.fit_transform(df[column])
        ngram_columns.append(ngram_matrix)
    return ngram_columns

# Function to calculate cosine similarity between two matrices
def calculate_cosine_similarity(matrix1, matrix2):
    similarity = cosine_similarity(matrix1, matrix2)
    return similarity

# Function to calculate average similarity score
def calculate_average_similarity_score(transaction_matrices, invoice_matrices):
    similarity_scores = []
    for transaction_matrix, invoice_matrix in zip(transaction_matrices, invoice_matrices):
        similarity_matrix = calculate_cosine_similarity(transaction_matrix, invoice_matrix)
        similarity_scores.append(np.mean(similarity_matrix))
    average_similarity_score = np.mean(similarity_scores)
    return average_similarity_score

# Function to calculate match based on given conditions
def calculate_match(transaction_row, invoice_row, thresholds):
    # Check condition 1: supplier_name match
    condition_1 = (transaction_row['supplier_name'] == invoice_row['ticket_number'] or
                  transaction_row['supplier_name'] == invoice_row['supplier_name'])
    
    # Check condition 2: auth_no match
    condition_2 = transaction_row['auth_no'] == invoice_row['auth_no']
    
    # Check condition 3: source_amount difference within threshold
    condition_3 = abs(transaction_row['source_amount'] - invoice_row['source_amount']) <= thresholds['source_amount']
    
    # Check condition 4: company_id match
    condition_4 = transaction_row['company_id'] == invoice_row['company_id']
    
    # Convert transaction and invoice data to n-grams
    transaction_matrices = convert_to_ngrams(transaction_row)
    invoice_matrices = convert_to_ngrams(invoice_row)
    
    # Calculate average similarity score
    average_similarity_score = calculate_average_similarity_score(transaction_matrices, invoice_matrices)
    
    # Check condition 5: average_similarity_score >= supplier_name_ticket threshold
    condition_5 = average_similarity_score >= thresholds['supplier_name_ticket']
    
    # Determine match based on all conditions
    match = condition_1 and condition_2 and condition_3 and condition_4 and condition_5
    
    return match

# Calculate match based on given conditions
transaction_row = transaction_df.iloc[0]
invoice_row = invoice_df.iloc[0]
is_match = calculate_match(transaction_row, invoice_row, thresholds)
print(is_match)





import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def calculate_cosine_similarity(table1, table2):
    # Convert the tables to numpy arrays
    table1_array = np.array(table1)
    table2_array = np.array(table2)

    # Calculate cosine similarity column-wise
    cosine_similarities = cosine_similarity(table1_array.T, table2_array.T)

    # Create a list to store the column-wise similarities
    similarities = []

    # Iterate over the columns
    for i in range(cosine_similarities.shape[0]):
        similarity_values = cosine_similarities[i]
        similarities.append(similarity_values)

    # Return the column-wise similarities
    return similarities



import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Function to calculate cosine similarity between two columns
def calculate_cosine_similarity(column1, column2):
    vec1 = column1.values.reshape(1, -1)
    vec2 = column2.values.reshape(1, -1)
    similarity = cosine_similarity(vec1, vec2)[0][0]
    return similarity

# Function to compare columns of two tables using cosine similarity
def compare_tables(table1, table2, thresholds):
    result = pd.DataFrame()

    for column1 in table1.columns:
        if column1 in table2.columns:
            column2 = table2[column1]
            similarity_score = calculate_cosine_similarity(table1[column1], column2)
            result[column1] = similarity_score

            # Check if similarity score meets the threshold for the current column
            if column1 in thresholds and similarity_score >= thresholds[column1]:
                result[column1 + '_match'] = True
            else:
                result[column1 + '_match'] = False

    return result

# Define the thresholds for each column
thresholds = {
    'source_amount': 0.9,
    'supplier_name_ticket': 0.8
}

# Assuming you have two tables named 'transaction' and 'invoice'
transaction = pd.read_csv('transaction.csv')
invoice = pd.read_csv('invoice.csv')

# Apply the conditions and calculate cosine similarity
result = compare_tables(transaction, invoice, thresholds)

# Apply additional conditions
filtered_result = result[(transaction['supplier_name'] == invoice['ticket_number'] or
                          transaction['supplier_name'] == invoice['supplier_name']) and
                         transaction['transaction_auth_no'] == invoice['invoice_auth_no'] and
                         abs(transaction['transaction_source_amount'] - invoice['invoice_source_amount']) <= thresholds['source_amount'] and
                         transaction['company_id'] == invoice['invoice_company_id'] and
                         result['supplier_name_ticket_match'] >= thresholds['supplier_name_ticket']]

print(filtered_result)





# Create CountVectorizer to convert text data to vectors
vectorizer = CountVectorizer()

# Fit and transform the transaction columns
transaction_vectors = vectorizer.fit_transform(transaction_data[transaction_columns].astype(str).values.flatten())

# Transform the invoice columns
invoice_vectors = vectorizer.transform(invoice_data[invoice_columns].astype(str).values.flatten())

# Calculate the cosine similarity matrix
similarity_matrix = cosine_similarity(transaction_vectors, invoice_vectors)




https://usa.visa.com/content/dam/VCOM/regional/na/us/products/documents/vdi-nyc-and-co-case-study.pdf






# Calculate the cosine similarity matrix for each column individually
similarity_matrix = {}

# Create CountVectorizer to convert text data to vectors
vectorizer = CountVectorizer()

for column in transaction_data.columns:
    # Fit and transform the transaction column
    transaction_vector = vectorizer.fit_transform(transaction_data[column].astype(str))

    # Transform the invoice column
    invoice_vector = vectorizer.transform(invoice_data[column].astype(str))

    # Calculate the cosine similarity matrix
    similarity_matrix[column] = cosine_similarity(transaction_vector, invoice_vector)












from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def convert_to_vectors(data):
    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform(data)
    return vectors

def find_matching_rows(table1, table2, thresholds):
    # Convert columns of both tables to vectors
    table1_vectors = convert_to_vectors(table1)
    table2_vectors = convert_to_vectors(table2)

    # Get the number of rows in each table
    table1_rows = table1_vectors.shape[0]
    table2_rows = table2_vectors.shape[0]

    # Perform cosine similarity calculations and find matching rows
    for i in range(table1_rows):
        for j in range(table2_rows):
            transaction_row = table1[i]
            invoice_row = table2[j]
            transaction_auth_no = transaction_row['auth_no']
            invoice_auth_no = invoice_row['auth_no']
            transaction_source_amount = transaction_row['source_amount']
            invoice_source_amount = invoice_row['source_amount']
            transaction_company_id = transaction_row['company_id']
            invoice_company_id = invoice_row['company_id']
            similarity_score = cosine_similarity(table1_vectors[i], table2_vectors[j])[0][0]

            if (transaction_row['supplier_name'] == invoice_row['ticket_number'] or
                    transaction_row['supplier_name'] == invoice_row['supplier_name']) and \
                    transaction_auth_no == invoice_auth_no and \
                    abs(transaction_source_amount - invoice_source_amount) <= thresholds['source_amount'] and \
                    transaction_company_id == invoice_company_id and \
                    similarity_score >= thresholds['supplier_name_ticket']:
                print(f"Matching score: {similarity_score}")
                print(f"Row {i} in Table 1 matches Row {j} in Table 2")

# Example usage
table1 = [
    {'supplier_name': 'Apple Inc.', 'auth_no': '1234', 'source_amount': 1000, 'company_id': 'C1'},
    {'supplier_name': 'Banana Corp.', 'auth_no': '5678', 'source_amount': 2000, 'company_id': 'C2'},
    {'supplier_name': 'Carrot Ltd.', 'auth_no': '9101', 'source_amount': 1500, 'company_id': 'C3'}
]

table2 = [
    {'ticket_number': '1234', 'supplier_name': 'Apple Inc.', 'auth_no': '1234', 'source_amount': 1000, 'company_id': 'C1'},
    {'ticket_number': '5678', 'supplier_name': 'Banana Corp.', 'auth_no': '5678', 'source_amount': 2000, 'company_id': 'C2'},
    {'ticket_number': '9101', 'supplier_name': 'Carrot Ltd.', 'auth_no': '9101', 'source_amount': 1500, 'company_id': 'C3'}
]

thresholds = {
    'source_amount': 100,
    'supplier_name_ticket': 0.9
}

find_matching_rows(table1, table2, thresholds)



def find_matching_rows(table1, table2, thresholds):
    # Convert columns of both tables to vectors
    table1_vectors = convert_to_vectors([row['supplier_name'].lower() for row in table1])
    table2_vectors = convert_to_vectors([row['supplier_name'].lower() for row in table2])

    # Get the number of rows in each table
    table1_rows = table1_vectors.shape[0]
    table2_rows = table2_vectors.shape[0]

    # Perform cosine similarity calculations and find matching rows
    for i in range(table1_rows):
        for j in range(table2_rows):
            transaction_row = table1[i]
            invoice_row = table2[j]
            transaction_auth_no = transaction_row['auth_no']
            invoice_auth_no = invoice_row['auth_no']
            transaction_source_amount = transaction_row['source_amount']
            invoice_source_amount = invoice_row['source_amount']
            transaction_company_id = transaction_row['company_id']
            invoice_company_id = invoice_row['company_id']
            similarity_score = cosine_similarity(table1_vectors[i], table2_vectors[j])[0][0]

            if (transaction_row['supplier_name'].lower() == invoice_row['ticket_number'].lower() or
                    transaction_row['supplier_name'].lower() == invoice_row['supplier_name'].lower()) and \
                    transaction_auth_no == invoice_auth_no and \
                    abs(transaction_source_amount - invoice_source_amount) <= thresholds['source_amount'] and \
                    transaction_company_id == invoice_company_id and \
                    similarity_score >= thresholds['supplier_name_ticket']:
                print(f"Matching score: {similarity_score}")
                print(f"Row {i} in Table 1 matches Row {j} in Table 2")





def one_to_one_match(T5, Invoice_table):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice_table):
            return True, i + 1
    return False, "Unmatched"

def bundle_match(T5, Invoice_table):
    matching_invoices = []
    total_invoice_source_amt = 0
    match_rule = None

    for i, Invoice in enumerate(Invoice_table):
        for rule_index, rule in enumerate(rules, start=1):
            if rule(T5, Invoice=Invoice):
                matching_invoices.append((Invoice, rule_index))
                total_invoice_source_amt += Invoice['inv_match_source_amt']
                match_rule = rule_index

    if total_invoice_source_amt == T5['fin_source_amt']:
        return matching_invoices, match_rule
    return [], None

def determine_match_type(matching_invoices):
    num_matching_invoices = len(matching_invoices)
    if num_matching_invoices == 1:
        return "one-to-one"
    elif num_matching_invoices > 1:
        return "bundle match"
    else:
        return "Unmatched"

# Process matches
output_rows = []
matched_invoices = []
unmatched_output_rows = []

for T5 in T5_table:
    match_found, match_rule = one_to_one_match(T5, IV_table)
    matching_invoices, bundle_rule = bundle_match(T5, IV_table)

    if match_found:
        matching_invoice, rule_index = match_found
        description = rule_descriptions.get(match_rule)
        output_row = {column: T5[column] if column in T5 else matching_invoice[column] for column in output_columns}
        output_row['Match Rule'] = rule_numbers.get(match_rule)
        output_row['description'] = description
        output_row['Match Type'] = determine_match_type([matching_invoice])
        output_rows.append(output_row)
        matched_invoices.append(matching_invoice)
    elif matching_invoices:
        for invoice, rule_index in matching_invoices:
            description = rule_descriptions.get(rule_index)
            output_row = {column: T5[column] if column in T5 else invoice[column] for column in output_columns}
            output_row['Match Rule'] = rule_numbers.get(bundle_rule)
            output_row['description'] = description
            output_row['Match Type'] = determine_match_type(matching_invoices)
            output_rows.append(output_row)
            matched_invoices.append(invoice)
    else:
        unmatched_output_row = {column: T5[column] if column in T5 else None for column in unmatched_output_columns}
        unmatched_output_rows.append(unmatched_output_row)

# Create a DataFrame from the output rows
output_df = pd.DataFrame(output_rows, columns=output_columns)

# Remove duplicates from unmatched_output_df
unmatched_output_df.drop_duplicates(subset=['fin_record_key'], inplace=True)

# Create a DataFrame for unmatched T5 rows
unmatched_t5_df = pd.DataFrame(unmatched_output_rows, columns=unmatched_output_columns)

# Write the unmatched T5 rows to a separate CSV file
unmatched_t5_df.to_csv("unmatched_t5.csv", index=False)

# Write the output data to a CSV file
output_df.to_csv(config['output_files']['output_file'], index=False)

# Write the aggregated data to Excel
with pd.ExcelWriter(config['output_files']['output_file']) as writer:
    output_df.to_excel(writer, sheet_name='Output', index=False)




import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

# Load the dataset
data = pd.read_csv('your_dataset.csv')

# Convert text columns into vectors using TF-IDF
text_columns = data.select_dtypes(include=['object']).columns

tfidf_vectorizer = TfidfVectorizer()
text_vectors = tfidf_vectorizer.fit_transform(data[text_columns].values.astype('U'))
text_vectors_df = pd.DataFrame(text_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names())

# Convert categorical columns into vectors using TF-IDF
categorical_columns = data.select_dtypes(include=['category']).columns

for column in categorical_columns:
    cat_vectors = tfidf_vectorizer.fit_transform(data[column].values.astype('U'))
    cat_vectors_df = pd.DataFrame(cat_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names())
    text_vectors_df = pd.concat([text_vectors_df, cat_vectors_df], axis=1)

# Combine the vectors with the original dataset
vectors_df = pd.concat([data.select_dtypes(include=['int64', 'float64']), text_vectors_df], axis=1)

# Print the vectorized dataset
print(vectors_df)



from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# Example strings
string1 = "I love cats"
string2 = "I adore dogs"

# Convert strings into vectors using TF-IDF
tfidf_vectorizer = TfidfVectorizer()
string_vectors = tfidf_vectorizer.fit_transform([string1, string2])
string_vectors = string_vectors.toarray()

# Compute cosine similarity between the vectors
similarity_score = cosine_similarity([string_vectors[0]], [string_vectors[1]])

# Print the similarity score
print("Similarity score:", similarity_score[0][0])












from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(str1, str2, n):
    # Preprocess the strings by converting them to lowercase and removing non-alphanumeric characters
    str1 = ''.join(ch.lower() for ch in str1 if ch.isalnum())
    str2 = ''.join(ch.lower() for ch in str2 if ch.isalnum())

    # Create a TfidfVectorizer object with character n-gram range
    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(n, n))

    # Fit and transform the strings
    tfidf_matrix = vectorizer.fit_transform([str1, str2])

    # Calculate the cosine similarity
    similarity_matrix = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])

    # Get the similarity percentage
    similarity_percentage = similarity_matrix[0][0] * 100

    return similarity_percentage

# Example strings
str1 = 'ABCD123'
str2 = '789ABCD1234'

# Set the character n-gram range
n = 1  # For individual characters

# Calculate the similarity percentage
similarity_percentage = calculate_similarity(str1, str2, n)

# Print the similarity percentage
print(f"The similarity between the strings is: {similarity_percentage:.2f}%")











def calculate_similarity_percentage(str1, str2):
    mapping = {c: i for i, c in enumerate(set(str1 + str2))}
    vec1 = [mapping[c] for c in str1]
    vec2 = [mapping[c] for c in str2]
    print('Initial Vector1',vec1)
    print('Initial Vector2',vec2)

    max_len = max(len(vec1), len(vec2))
    vec1 += [0] * (max_len - len(vec1))
    vec2 += [0] * (max_len - len(vec2))
    print(vec1)
    print(vec2)

    distance = sum(abs(a - b) for a, b in zip(vec1, vec2))
    print('distance',distance)

    max_distance = max(abs(max(vec1) - min(vec1)), abs(max(vec2) - min(vec2))) * max_len
    print('max_distance',max_distance)
    similarity_percentage = (1 - (distance / max_distance)) * 100

    return similarity_percentage

# Example usage
string1 = "3451"
string2 = "A34515"
similarity = calculate_similarity_percentage(string1, string2)
print(f"Similarity percentage: {similarity}%")





import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(str1, str2, n):
    # Preprocess the strings by converting them to lowercase and removing non-alphanumeric characters
    str1 = ''.join(ch.lower() for ch in str1 if ch.isalnum())
    str2 = ''.join(ch.lower() for ch in str2 if ch.isalnum())

    # Create a TfidfVectorizer object with character n-gram range
    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(n, n))

    # Fit and transform the strings
    tfidf_matrix = vectorizer.fit_transform([str1, str2])

    # Calculate the cosine similarity
    similarity_matrix = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])

    # Get the similarity percentage
    similarity_percentage = similarity_matrix[0][0] * 100
    return similarity_percentage


def process_data(transaction_file, invoice_file):
    transaction_data = pd.read_excel(transaction_file)
    invoice_data = pd.read_excel(invoice_file)

    count = 0
    matches = []
    matched_invoices = []

    for transaction_idx, transaction_row in transaction_data.iterrows():
        transaction_id = transaction_row['T5_RECORD_KEY']
        transaction_supplier_name = transaction_row['T5_ORIG_SUPPLIER_NM']
        transaction_source_amount = transaction_row['5_SOURCE_AMT']
        transaction_auth_no = transaction_row['T5_AUTH_NBR']
        transaction_purchase = transaction_row['PURCH_ID']
        transaction_acct = transaction_row['ACCOUNT']
        transaction_company_id = transaction_row['fin_company_id']
        transaction_D_C = transaction_row['fin_debit_credit_ind']
        best_match_score = -1
        best_match_idx = -1

        for invoice_idx, invoice_row in invoice_data.iterrows():
            invoice_string = invoice_row['invoice_string']
            invoice_ticket_number = str(invoice_row['TICKET_NUM'])
            invoice_inv_no = str(invoice_row['INVNUMBER'])
            invoice_auth_no = invoice_row['inv_authn_num']
            invoice_amount = invoice_row['MATCH_SOURCE_AMT']
            invoice_company_id = invoice_row['inv_company_id']
            invoice_acct = invoice_row['ACCOUNT']
            invoice_D_C = invoice_row['INVTYPECD']

            supp_similarity_score_tkt = calculate_similarity(transaction_supplier_name, invoice_ticket_number, 3)
            supp_similarity_score_inv = calculate_similarity(transaction_supplier_name, invoice_inv_no, 3)
            auth_no_score = calculate_similarity(transaction_auth_no, invoice_auth_no, 3)
            purd_id_score_tkt = calculate_similarity(transaction_purchase, invoice_ticket_number, 3)
            purd_id_score_inv = calculate_similarity(transaction_purchase, invoice_inv_no, 3)
            similarity_score = max(supp_similarity_score_tkt, supp_similarity_score_inv, auth_no_score, purd_id_score_inv)

            condition1 = supp_similarity_score_tkt > -thresholds['supp-similarity_score-_tkt']
            condition2 = supp_similarity_score_inv > -thresholds['supp-similarity_score-inv']

            if condition1 and condition2:
                if similarity_score > best_match_score:
                    best_match_score = similarity_score
                    best_match_idx = invoice_idx

        if best_match_idx != -1:
            invoice_id = invoice_data.loc[best_match_idx, 'inv_unmatch_inv_id']
            transaction_values = transaction_row[transaction_columns]
            invoice_values = invoice_data.loc[best_match_idx, invoice_columns]
            matches.append((transaction_id, invoice_id, transaction_values, invoice_values, best_match_score))
            count += 1
            print(count)
            matched_invoices.append(invoice_id)

    return matches, matched_invoices

# Example usage
transaction_file = 'transaction_data.xlsx'
invoice_file = 'invoice_data.xlsx'
matches, matched_invoices = process_data(transaction_file, invoice_file)














import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from multiprocessing import Pool

def calculate_similarity(str1, str2, n):
    # Preprocess the strings by converting them to lowercase and removing non-alphanumeric characters
    str1 = ''.join(ch.lower() for ch in str1 if ch.isalnum())
    str2 = ''.join(ch.lower() for ch in str2 if ch.isalnum())

    # Create a TfidfVectorizer object with character n-gram range
    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(n, n))

    # Fit and transform the strings
    tfidf_matrix = vectorizer.fit_transform([str1, str2])

    # Calculate the cosine similarity
    similarity_matrix = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])

    # Get the similarity percentage
    similarity_percentage = similarity_matrix[0][0] * 100
    return similarity_percentage

def process_transaction(transaction_row, invoice_data):
    transaction_id = transaction_row['T5_RECORD_KEY']
    transaction_supplier_name = transaction_row['T5_ORIG_SUPPLIER_NM']
    transaction_source_amount = transaction_row['5_SOURCE_AMT']
    transaction_auth_no = transaction_row['T5_AUTH_NBR']
    transaction_purchase = transaction_row['PURCH_ID']
    transaction_acct = transaction_row['ACCOUNT']
    transaction_company_id = transaction_row['fin_company_id']
    transaction_D_C = transaction_row['fin_debit_credit_ind']
    best_match_score = -1
    best_match_idx = -1

    for invoice_idx, invoice_row in invoice_data.iterrows():
        invoice_string = invoice_row['invoice_string']
        invoice_ticket_number = str(invoice_row['TICKET_NUM'])
        invoice_inv_no = str(invoice_row['INVNUMBER'])
        invoice_auth_no = invoice_row['inv_authn_num']
        invoice_amount = invoice_row['MATCH_SOURCE_AMT']
        invoice_company_id = invoice_row['inv_company_id']
        invoice_acct = invoice_row['ACCOUNT']
        invoice_D_C = invoice_row['INVTYPECD']

        supp_similarity_score_tkt = calculate_similarity(transaction_supplier_name, invoice_ticket_number, 3)
        supp_similarity_score_inv = calculate_similarity(transaction_supplier_name, invoice_inv_no, 3)
        auth_no_score = calculate_similarity(transaction_auth_no, invoice_auth_no, 3)
        purd_id_score_tkt = calculate_similarity(transaction_purchase, invoice_ticket_number, 3)
        purd_id_score_inv = calculate_similarity(transaction_purchase, invoice_inv_no, 3)
        similarity_score = max(supp_similarity_score_tkt, supp_similarity_score_inv, auth_no_score, purd_id_score_inv)

        condition1 = supp_similarity_score_tkt > -thresholds['supp-similarity_score-_tkt']
        condition2 = supp_similarity_score_inv > -thresholds['supp-similarity_score-inv']

        if condition1 and condition2:
            if similarity_score > best_match_score:
                best_match_score = similarity_score
                best_match_idx = invoice_idx

    if best_match_idx != -1:
        invoice_id = invoice_data.loc[best_match_idx, 'inv_unmatch_inv_id']
        transaction_values = transaction_row[transaction_columns]
        invoice_values = invoice_data.loc[best_match_idx, invoice_columns]
        return (transaction_id, invoice_id, transaction_values, invoice_values, best_match_score)
    else:
        return None

def process_data(transaction_file, invoice_file):
    transaction_data = pd.read_excel(transaction_file)
    invoice_data = pd.read_excel(invoice_file)

    # Fill empty values with "I"
    transaction_data.fillna("I", inplace=True)
    invoice_data.fillna("I", inplace=True)

    count = 0
    matches = []
    matched_invoices = []

    transaction_columns = []  # Replace with the actual transaction column names you want to extract
    invoice_columns = []  # Replace with the actual invoice column names you want to extract

    # Use multiprocessing for parallel processing
    with Pool() as pool:
        result_list = []
        for _, transaction_row in transaction_data.iterrows():
            result = pool.apply_async(process_transaction, (transaction_row, invoice_data))
            result_list.append(result)

        for result in result_list:
            match = result.get()
            if match is not None:
                transaction_id, invoice_id, transaction_values, invoice_values, best_match_score = match
                matches.append((transaction_id, invoice_id, transaction_values, invoice_values, best_match_score))
                count += 1
                print(count)
                matched_invoices.append(invoice_id)

    return matches, matched_invoices




import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import OneHotEncoder

# Sample data
table1_column = ['apple', 'banana', 'orange', 'apple']
table2_column = ['orange', 'grape', 'banana', 'apple']

# Preprocess the data
# In this example, we'll use one-hot encoding to represent the categorical data

# Create an instance of the encoder
encoder = OneHotEncoder()

# Fit and transform table1_column
table1_column_encoded = encoder.fit_transform(np.array(table1_column).reshape(-1, 1))

# Fit and transform table2_column
table2_column_encoded = encoder.fit_transform(np.array(table2_column).reshape(-1, 1))

# Iterate over each row and compute cosine similarity
for i in range(len(table1_column_encoded.toarray())):
    for j in range(len(table2_column_encoded.toarray())):
        similarity_score = cosine_similarity(
            table1_column_encoded[i], table2_column_encoded[j]
        )
        print(
            f"Cosine similarity between row {i+1} in table 1 and row {j+1} in table 2: {similarity_score[0][0]}"
        )

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(str1, str2, n):
    # Create a CountVectorizer object with character n-gram range
    vectorizer = CountVectorizer(analyzer='char', ngram_range=(n, n))

    # Fit and transform the strings
    count_matrix = vectorizer.fit_transform([str1, str2])

    # Calculate the cosine similarity
    similarity_matrix = cosine_similarity(count_matrix[0], count_matrix[1])

    # Get the similarity percentage
    similarity_percentage = similarity_matrix[0][0] * 100

    return similarity_percentage






def calculate_similarity(str1, str2, n):
    # Preprocess the strings by converting them to lowercase
    str1 = str1.lower()
    str2 = str2.lower()

    # Generate character n-grams for each string
    ngrams_str1 = [str1[i:i+n] for i in range(len(str1)-n+1)]
    ngrams_str2 = [str2[i:i+n] for i in range(len(str2)-n+1)]

    # Count the occurrences of each n-gram in each string
    counts_str1 = {gram: ngrams_str1.count(gram) for gram in ngrams_str1}
    counts_str2 = {gram: ngrams_str2.count(gram) for gram in ngrams_str2}

    # Calculate the dot product of the two count vectors
    dot_product = sum(counts_str1.get(gram, 0) * counts_str2.get(gram, 0) for gram in set(ngrams_str1 + ngrams_str2))

    # Calculate the Euclidean norm of each count vector
    norm_str1 = math.sqrt(sum(counts_str1.get(gram, 0) ** 2 for gram in set(ngrams_str1)))
    norm_str2 = math.sqrt(sum(counts_str2.get(gram, 0) ** 2 for gram in set(ngrams_str2)))

    # Calculate the cosine similarity
    similarity = dot_product / (norm_str1 * norm_str2) if norm_str1 * norm_str2 != 0 else 0

    return similarity*100






def calculate_similarity(str1, str2, n):
    # Preprocess the strings by converting them to lowercase
    str1 = str1.lower()
    str2 = str2.lower()

    # Generate character n-grams for each string
    ngrams_str1 = set(str1[i:i+n] for i in range(len(str1)-n+1))
    ngrams_str2 = set(str2[i:i+n] for i in range(len(str2)-n+1))


    # Calculate the dot product
    dot_product = len(ngrams_str1.intersection(ngrams_str2))

    # Calculate the Euclidean norms
    norm_str1 = math.sqrt(len(ngrams_str1))
    norm_str2 = math.sqrt(len(ngrams_str2))

    # Calculate the cosine similarity
    similarity = dot_product / (norm_str1 * norm_str2) if norm_str1 * norm_str2 != 0 else 0

    return similarity*100













def calculate_similarity(str1, str2, n):
    # Preprocess the strings by converting them to lowercase
    str1 = str1.lower()
    str2 = str2.lower()

    # Generate character n-grams for each string as lists
    ngrams_str1 = [str1[i:i+n] for i in range(len(str1)-n+1)]
    ngrams_str2 = [str2[i:i+n] for i in range(len(str2)-n+1)]

    # Calculate the dot product
    dot_product = sum(1 for gram in ngrams_str1 if gram in ngrams_str2)

    # Calculate the Euclidean norms
    norm_str1 = math.sqrt(len(ngrams_str1))
    norm_str2 = math.sqrt(len(ngrams_str2))

    # Calculate the cosine similarity
    similarity = dot_product / (norm_str1 * norm_str2) if norm_str1 * norm_str2 != 0 else 0

    return similarity












def match_bundle(T5, Invoice_table):
    matching_invoices = []
    unmatched_invoices = Invoice_table[:]
    total_invoice_source_amt = 0
    match_rule = None
    rule_index = 1

    while rule_index <= len(rules) and unmatched_invoices:
        invoices_to_remove = []
        for i in range(len(unmatched_invoices)):
            invoice = unmatched_invoices[i]
            if not any(value == '' or pd.isna(value) for value in invoice.values()):
                if rules[rule_index - 1](T5, Invoice=invoice):
                    invoice_amt = float(invoice['inv_match_source_amt'])
                    remaining_amt = float(T5['fin_source_amt']) - total_invoice_source_amt
                    if invoice_amt <= remaining_amt:
                        matching_invoices.append((invoice, rule_index))
                        total_invoice_source_amt += invoice_amt
                        invoices_to_remove.append(i)

        for index in sorted(invoices_to_remove, reverse=True):
            unmatched_invoices.pop(index)

        if total_invoice_source_amt == float(T5['fin_source_amt']):
            match_rule = rule_index
            break

        rule_index += 1

    if total_invoice_source_amt == float(T5['fin_source_amt']):
        return matching_invoices, match_rule
    else:
        return [], unmatched_invoices



def is_empty_match(T5, Invoice):
    # Check if both T5 and Invoice values are empty
    return (T5 == '') and (Invoice == '')

def match_bundle(T5, Invoice_table):
    matching_invoices = []
    unmatched_invoices = Invoice_table[:]
    total_invoice_source_amt = 0
    match_rule = None
    rule_index = 1

    while rule_index <= len(rules) and unmatched_invoices:
        invoices_to_remove = []
        for i in range(len(unmatched_invoices)):
            invoice = unmatched_invoices[i]
            if not is_empty_match(T5['t5_auth'], invoice['inv_auth']):
                if rules[rule_index - 1](T5, Invoice=invoice):
                    invoice_amt = float(invoice['inv_match_source_amt'])
                    remaining_amt = float(T5['fin_source_amt']) - total_invoice_source_amt
                    if invoice_amt <= remaining_amt:
                        matching_invoices.append((invoice, rule_index))
                        total_invoice_source_amt += invoice_amt
                        invoices_to_remove.append(i)

        for index in sorted(invoices_to_remove, reverse=True):
            unmatched_invoices.pop(index)

        if total_invoice_source_amt == float(T5['fin_source_amt']):
            match_rule = rule_index
            break

        rule_index += 1

    if total_invoice_source_amt == float(T5['fin_source_amt']):
        return matching_invoices, match_rule
    else:
        return [], unmatched_invoices











import pandas as pd

# Read the dataset from a CSV file
df = pd.read_csv('your_dataset.csv')  # Replace 'your_dataset.csv' with the actual file path

# Specify the column names
acct_column = 'AcctNumber'
rule_column = 'RuleIDs'

# Group the RuleIDs by Account Number
grouped_df = df.groupby(acct_column)[rule_column].apply(list).reset_index()

# Rename the column
grouped_df = grouped_df.rename(columns={rule_column: rule_column})

# Print the resulting DataFrame
print(grouped_df)




grouped_df['Count'] = df.groupby(acct_column)[rule_column].size().reset_index(drop=True)

# Rename the columns
grouped_df = grouped_df.rename(columns={rule_column: rule_column, 'Count': 'Count'})












import pandas as pd

# Read Excel file into pandas DataFrames
df_rules = pd.read_excel('path/to/excel/file.xlsx', sheet_name='fst_rules_mI')
df_seq = pd.read_excel('path/to/excel/file.xlsx', sheet_name='fst_rule_sequence')

# Apply transformations
df_rules['flag'] = 'Y'

df_rule_seq = df_rules.merge(df_seq, left_on='rule_no', right_on='rule_id', how='inner')

def get_category(rule_description):
    if 'Ticket' in rule_description:
        return 'Ticket'
    elif 'Invoice' in rule_description:
        return 'Invoice'
    elif 'Auth' in rule_description:
        return 'Auth'
    elif 'PNR' in rule_description:
        return 'PNR'
    else:
        return 'Company'

df_rule_seq['category'] = df_rule_seq['rule_description'].apply(get_category)

df_rule_seq_ctg = df_rule_seq[['rule_no', 'rule', 'rule_description', 'company_id', 'transaction_count', 'flag', 'category']]

df_rule_seq_ctg_group = df_rule_seq_ctg.groupby('category').agg({'transaction_count': 'max'}).reset_index()

df_rule_seq_ctg_group['sequence2'] = df_rule_seq_ctg_group['transaction_count'].rank(ascending=False)

df_rule_seq_ctg_group.sort_values('sequence2', ascending=True, inplace=True)

f_final_rule = df_rule_seq_ctg.merge(df_rule_seq_ctg_group[['category', 'sequence2']], on='category', how='inner').sort_values('sequence2')









# Load existing data if the file exists, otherwise create an empty DataFrame
if os.path.exists('transaction_counts.csv'):
    transaction_counts_df = pd.read_csv('transaction_counts.csv')
else:
    transaction_counts_df = pd.DataFrame(columns=['fin_company_id', 'Match Rule', 'Transaction Count'])

# Calculate the new transaction counts and update the DataFrame
aggregated = output_df.groupby(['fin_company_id', 'Match Rule']).size().reset_index(name='Transaction Count')
total = aggregated['Transaction Count'].sum()
aggregated = aggregated.append(pd.Series(['Total', None, total], index=aggregated.columns), ignore_index=True)

# Update the existing data or add new rows
for i, row in aggregated.iterrows():
    company_id = row['fin_company_id']
    rule_number = row['Match Rule']
    transaction_count = row['Transaction Count']

    # Check if the combination of company ID and rule number already exists in the DataFrame
    existing_row = transaction_counts_df[
        (transaction_counts_df['fin_company_id'] == company_id) &
        (transaction_counts_df['Match Rule'] == rule_number)
    ]

    if not existing_row.empty:
        # Update the existing row with the new transaction count
        transaction_counts_df.loc[existing_row.index, 'Transaction Count'] += transaction_count
    else:
        # Add a new row for the company ID and rule number with the transaction count
        new_row = pd.DataFrame([[company_id, rule_number, transaction_count]], columns=['fin_company_id', 'Match Rule', 'Transaction Count'])
        transaction_counts_df = transaction_counts_df.append(new_row, ignore_index=True)

# Save the updated data to the file
transaction_counts_df.to_csv('transaction_counts.csv', index=False)













import pytest

# Import the function to be tested (make sure the function is in the same directory or module)
from your_module import match_bundle

# Define some sample data for testing
sample_T5 = {
    'fin_source_amt': '1000.00',  # Sample 'fin_source_amt' value for T5
}

sample_invoice_table = [
    {'inv_match_source_amt': '300.00'},  # Sample invoice with 'inv_match_source_amt'
    {'inv_match_source_amt': '200.00'},  # Sample invoice with 'inv_match_source_amt'
    {'inv_match_source_amt': '400.00'},  # Sample invoice with 'inv_match_source_amt'
    {'inv_match_source_amt': '100.00'},  # Sample invoice with 'inv_match_source_amt'
]

# Define some sample rules to be used by the function
# In your actual implementation, make sure to import the rules from your module
def rule_1(T5, Invoice):
    return True

def rule_2(T5, Invoice):
    return True

# Test cases
def test_match_bundle_with_matching_invoices():
    # Test the case where there are matching invoices
    matching_invoices, match_rule = match_bundle(sample_T5, sample_invoice_table)
    assert matching_invoices != []
    assert match_rule is not None
    assert sum(float(invoice['inv_match_source_amt']) for invoice, _ in matching_invoices) == float(sample_T5['fin_source_amt'])

def test_match_bundle_with_unmatched_invoices():
    # Test the case where there are no matching invoices
    T5 = {'fin_source_amt': '10000.00'}  # A larger amount to ensure no matching invoices
    unmatched_invoices = sample_invoice_table[:]  # Make a copy of the sample_invoice_table
    matching_invoices, match_rule = match_bundle(T5, unmatched_invoices)
    assert matching_invoices == []
    assert unmatched_invoices == sample_invoice_table  # The original sample_invoice_table should not be modified

def test_match_bundle_with_custom_rules():
    # Test the case where custom rules are used
    custom_rules = [rule_1, rule_2]
    matching_invoices, match_rule = match_bundle(sample_T5, sample_invoice_table, rules=custom_rules)
    assert matching_invoices != []
    assert match_rule is not None
    assert sum(float(invoice['inv_match_source_amt']) for invoice, _ in matching_invoices) == float(sample_T5['fin_source_amt'])

def test_match_bundle_with_empty_invoice_table():
    # Test the case where the invoice table is empty
    T5 = {'fin_source_amt': '1000.00'}  # Sample T5 data
    invoice_table = []  # Empty invoice table
    matching_invoices, match_rule = match_bundle(T5, invoice_table)
    assert matching_invoices == []
    assert match_rule is None

# You can add more test cases to cover other scenarios and edge cases as needed

# Run the tests
if __name__ == '__main__':
    pytest.main()



import pytest

@pytest.fixture
def sample_invoice_table():
    # Replace this with a sample invoice table in the required format
    return []

def test_match_bundle_success(sample_invoice_table):
    # Replace the following with actual values for T5 and Invoice_table
    T5 = {'fin_source_amt': '1000'}
    Invoice_table = sample_invoice_table

    matching_invoices, match_rule = match_bundle(T5, Invoice_table)

    # Assertions
    assert len(matching_invoices) > 0
    assert match_rule is not None
    assert sum(float(invoice['inv_match_source_amt']) for invoice, _ in matching_invoices) == float(T5['fin_source_amt'])







def match_bundle(T5, Invoice_table):
    matching_invoices = []
    unmatched_invoices = Invoice_table[:]
    total_invoice_source_amt = 0
    match_rule = None
    rule_index = 1

    while rule_index <= len(rules) and unmatched_invoices:
        invoices_to_remove = []
        for i in range(len(unmatched_invoices)):
            invoice = unmatched_invoices[i]
            if rules[rule_index - 1](T5, Invoice=invoice):
                invoice_amt = float(invoice['inv_match_source_amt'])
                remaining_amt = float(T5['fin_source_amt']) - total_invoice_source_amt
                if invoice_amt <= remaining_amt:
                    # Check if the invoice number is the same for the current transaction
                    if matching_invoices and matching_invoices[0][0]['inv_number'] != invoice['inv_number']:
                        continue  # Skip this invoice, as it has a different invoice number for this transaction
                    
                    matching_invoices.append((invoice, rule_index))
                    total_invoice_source_amt += invoice_amt
                    invoices_to_remove.append(i)

        for index in sorted(invoices_to_remove, reverse=True):
            unmatched_invoices.pop(index)

        if total_invoice_source_amt == float(T5['fin_source_amt']):
            match_rule = rule_index
            break

        rule_index += 1

    if total_invoice_source_amt == float(T5['fin_source_amt']):
        return matching_invoices, match_rule
    else:
        return [], unmatched_invoices







def is_empty_match(T5, Invoice):
    # Check if both T5 auth and Invoice auth are empty
    is_auth_empty = (T5['t5_auth'] == '') and (Invoice['inv_auth'] == '')
    
    # Check if both T5 ticket number and Invoice ticket number are empty
    is_ticket_empty = (T5['ticket_number'] == '') and (Invoice['ticket_number'] == '')
    
    # Return True if both auth and ticket number are empty in both T5 and Invoice
    return is_auth_empty and is_ticket_empty

def match_bundle(T5, Invoice_table):
    matching_invoices = []
    unmatched_invoices = Invoice_table[:]
    total_invoice_source_amt = 0
    match_rule = None
    rule_index = 1

    while rule_index <= len(rules) and unmatched_invoices:
        invoices_to_remove = []
        for i in range(len(unmatched_invoices)):
            invoice = unmatched_invoices[i]
            if not is_empty_match(T5, invoice):
                if rules[rule_index - 1](T5, Invoice=invoice):
                    invoice_amt = float(invoice['inv_match_source_amt'])
                    remaining_amt = float(T5['fin_source_amt']) - total_invoice_source_amt
                    if invoice_amt <= remaining_amt:
                        matching_invoices.append((invoice, rule_index))
                        total_invoice_source_amt += invoice_amt
                        invoices_to_remove.append(i)

        for index in sorted(invoices_to_remove, reverse=True):
            unmatched_invoices.pop(index)

        if total_invoice_source_amt == float(T5['fin_source_amt']):
            match_rule = rule_index
            break

        rule_index += 1

    if total_invoice_source_amt == float(T5['fin_source_amt']):
        return matching_invoices, match_rule
    else:
        return [], unmatched_invoices

# Rest of the code...
# Process one-to-one matches
# Process bundle matches
# Create a DataFrame from the output rows
# Write the unmatched transactions to a separate CSV file
# Write the output data to a CSV file
# Write the aggregated data to Excel





rules_df = pd.read_csv('rules.csv')
rules = []
rule_descriptions = {}
rule_numbers = {}

for i, row in rules_df.iterrows():
    rule_string = row['rule']
    rule_description = row['description']
    rule_no = row['Rule No']
    if rule_string.strip():  # Check if the rule_string is not empty or contains only whitespace
        rule = eval(f"lambda row, Invoice: ({rule_string})")
        rules.append(rule)
        rule_descriptions[i + 1] = rule_description
        rule_numbers[i + 1] = rule_no





def match_one_to_one(T5, Invoice_table, rules):
    matching_invoices = []
    match_rule = None
    invoices_to_remove = []

    for Invoice in Invoice_table:
        for rule_index, rule in enumerate(rules, start=1):
            if not T5 or not Invoice:  # Check if T5 or Invoice is empty
                continue

            if rule(T5, Invoice=Invoice) and \
               T5.get('fin_source_amt') and Invoice.get('inv_match_source_amt') and \
               float(T5['fin_source_amt']) == float(Invoice['inv_match_source_amt']) and \
               T5.get('fin_debit_credit') and T5['fin_debit_credit'] == Invoice['fin_debit_credit']:
                matching_invoices.append((Invoice, rule_index))
                match_rule = rule_index
                invoices_to_remove.append(Invoice)

                # We can break the inner loop if we only want to match one rule.
                # If you want to match multiple rules for the same Invoice, remove the break statement.

                break

    if matching_invoices:
        return matching_invoices, match_rule
    else:
        return [], None










                # Add your additional logic to check for empty conditions in rule
                d = re.findall(regex_pattern, rule_descriptions[rule_index])
                if not pd.isna(row[d[0]]) and not pd.isna(row[d[111]]):
                    matching_invoices.append((Invoice, rule_index))
                    match_rule = rule_index
                    invoices_to_remove.append(Invoice)


            if not any(pd.isna(val) or val.lower() == 'empty' for val in rule(T5, Invoice=Invoice)):



regex_pattern = r'[A-Z]+-\d+'

def match_one_to_one(T5, Invoice_table):
    matching_invoices = []
    match_rule = None
    invoices_to_remove = []
    
    for Invoice in Invoice_table:
        for rule_index, rule in enumerate(rules, start=1):
            if rule(T5, Invoice=Invoice) and \
                    float(T5['fin_source_amt']) == float(Invoice['inv_match_source_amt']) and \
                    T5['fin_debit_credit'] == Invoice['fin_debit_credit']:
                
                # Fetch the rule_string from the rules_df DataFrame using the rule_index
                rule_string = rules_df.loc[rules_df['RuLe No'] == rule_index, 'rule'].values[0]
                
                # Use the regex_pattern to find the matching values from the rule_string
                d = re.findall(regex_pattern, rule_string)
                if len(d) == 2:  # Assuming you expect two matching values
                    if not pd.isna(row[d[0]]) and not pd.isna(row[d[1]]):
                        matching_invoices.append((Invoice, rule_index))
                        match_rule = rule_index
                        invoices_to_remove.append(Invoice)
                        # break  # If you want to break after the first match, uncomment this line
                
    if matching_invoices:
        return matching_invoices, match_rule
    else:
        return [], None












import pandas as pd
import ast

# Assuming you have already read the rules.csv file into rules_df using pd.read_csv()

rules = []
rule_descriptions = {}
rule_numbers = {}
for i, row in rules_df.iterrows():
    rule_string = row['rule']
    rule_description = row['description']
    rule_no = row['Rule No']
    rule = ast.literal_eval(f"lambda row, Invoice: ({rule_string})") if rule_string.strip() else None
    rules.append(rule)
    rule_descriptions[i + 1] = rule_description
    rule_numbers[i + 1] = rule_no


def is_empty_rule(rule_string):
    return not bool(rule_string.strip())


    for Invoice in Invoice_table:
        empty_fields_in_T5 = [field for field in T5 if pd.isnull(T5[field]) or T5[field] == ""]
        empty_fields_in_Invoice = [field for field in Invoice if pd.isnull(Invoice[field]) or Invoice[field] == ""]
        
        # If both T5 and Invoice have empty fields, skip the matching process for this Invoice
        if empty_fields_in_T5 and empty_fields_in_Invoice:
            continue







    for rule_index, rule in enumerate(rules, start=1):
        # Check if T5 has empty auth or ticket number
        isAuthEmpty, isTicketEmpty = isEmptyMatch(T5, Invoice={})  # Empty Invoice object for checking T5
        if isAuthEmpty or isTicketEmpty:
            continue  # Skip this rule and move to the next one

        for Invoice in Invoice_table:
            # Check if Invoice has empty auth or ticket number
            isAuthEmpty, isTicketEmpty = isEmptyMatch({}, Invoice=Invoice)  # Empty T5 object for checking Invoice
            if isAuthEmpty or isTicketEmpty:
                continue 



A comprehensive overview of Qatar's market dynamics and performance derived from the top 7 issuers operating within the CEMEA (Central and Eastern Europe, Middle East, and Africa) region.


 "0" in T5[COLUMN_HEADER.FIN_TICKET_NO.value].split('0'):



def append_matching_invoices(matching_invoices, rule_index, rule_descriptions):
    for invoice, rule_idx in matching_invoices:
        if invoice not in matched_invoices:
            description = rule_descriptions.get(rule_index)
            output_row = {column: T5[column] if column in T5 else invoice[column] for column in output_columns}
            output_row['Match Rule'] = rule_index
            output_row['description'] = description
            output_row['Match Type'] = 'one-to-one'
            output_rows_one_to_one.append(output_row)
            matched_invoices.append(invoice)










def process_transaction(T5, Invoice_table, matched_invoices, output_rows_one_to_one, unmatched_output_rows_one_to_one, rule_descriptions):
    matching_invoices, match_rule = match_one_to_one(T5, Invoice_table)
    if matching_invoices:
        for invoice, rule_index in matching_invoices:
            if invoice not in matched_invoices:
                description = rule_descriptions.get(rule_index)
                output_row = {
                    column: T5[column] if column in T5 else invoice[column] for column in output_columns
                }
                output_row['Match Rule'] = rule_index
                output_row['description'] = description
                output_row['Match Type'] = 'one-to-one'
                output_rows_one_to_one.append(output_row)
                matched_invoices.append(invoice)

        remove_matched_invoices(Invoice_table, matched_invoices)

    else:
        add_to_unmatched(T5, unmatched_fin_record_keys, unmatched_output_rows_one_to_one)


def remove_matched_invoices(Invoice_table, matched_invoices):
    Invoice_table = [inv for inv in Invoice_table if inv not in matched_invoices]


def remove_matched_transaction(T5_table, matched_fin_record_keys, unmatched_output_df, T5):
    T5_table = [T5_row for T5_row in T5_table if T5_row[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] != T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value]]
    matched_fin_record_keys.remove(T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value])
    unmatched_indices = unmatched_output_df[
        (unmatched_output_df[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] == T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value]) &
        (unmatched_output_df[COLUMN_HEADER.T5_SOURCE_AMT.value] == T5[COLUMN_HEADER.T5_SOURCE_AMT.value]) &
        (unmatched_output_df[COLUMN_HEADER.FIN_DEB_CRED_IND.value] == T5[COLUMN_HEADER.FIN_DEB_CRED_IND.value])
    ].index
    unmatched_output_df.drop(unmatched_indices, inplace=True)


def add_to_unmatched(T5, unmatched_fin_record_keys, unmatched_output_rows_one_to_one):
    unmatched_output_row = {column: T5[column] if column in T5 else None for column in unmatched_output_columns}
    unmatched_output_rows_one_to_one.append(unmatched_output_row)
    unmatched_fin_record_keys.add(T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value])


# Now you can use the functions in your main code:

# Initialize the lists and sets
matched_invoices = []
unmatched_fin_record_keys = set()
output_rows_one_to_one = []
unmatched_output_rows_one_to_one = []

# Loop through the T5_table
for count, T5 in enumerate(T5_table, start=1):
    print(f"Processed Transactions: {count}")
    process_transaction(T5, Invoice_table, matched_invoices, output_rows_one_to_one, unmatched_output_rows_one_to_one, rule_descriptions)

    try:
        # Perform other operations if needed
        pass
    except Exception as e:
        error_messages.append(f"Error processing T5: {T5}\n{str(e)}")

# Now the matched_invoices, unmatched_fin_record_keys, output_rows_one_to_one, and unmatched_output_rows_one_to_one lists/sets will be updated with the processed data.




def process_transaction(T5, Invoice_table, matched_invoices, output_rows_one_to_one, unmatched_output_rows_one_to_one, rule_descriptions, processed_transactions):
    t5_record_key = T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value]
    if t5_record_key in processed_transactions:
        return  # Skip processing if the transaction has already been processed

    processed_transactions.add(t5_record_key)

    matching_invoices, match_rule = match_one_to_one(T5, Invoice_table)
    if matching_invoices:
        for invoice, rule_index in matching_invoices:
            if invoice not in matched_invoices:
                description = rule_descriptions.get(rule_index)
                output_row = {
                    column: T5[column] if column in T5 else invoice[column] for column in output_columns
                }
                output_row['Match Rule'] = rule_index
                output_row['description'] = description
                output_row['Match Type'] = 'one-to-one'
                output_rows_one_to_one.append(output_row)
                matched_invoices.append(invoice)

        remove_matched_invoices(Invoice_table, matched_invoices)

    else:
        add_to_unmatched(T5, unmatched_fin_record_keys, unmatched_output_rows_one_to_one)




for count, T5 in enumerate(T5_table, start=1):
    print(f"Processed Transactions: {count}")
    matching_invoices, match_rule = match_one_to_one(T5, Invoice_table)
    if matching_invoices:
        for invoice, rule_index in matching_invoices:
            if invoice not in matched_invoices:
                description = rule_descriptions.get(rule_index)
                output_row = {
                    column: T5[column] if column in T5 else invoice[column] for column in output_columns
                }
                output_row['Match Rule'] = rule_index
                output_row['description'] = description
                output_row['Match Type'] = 'one-to-one'
                output_rows_one_to_one.append(output_row)
                matched_invoices.append(invoice)

        remove_matched_invoices(Invoice_table, matched_invoices)
        remove_matched_transaction(T5_table, matched_fin_record_keys, unmatched_output_df, T5)

    else:
        add_to_unmatched(T5, unmatched_fin_record_keys, unmatched_output_rows_one_to_one)

    try:
        # Perform other operations if needed
        pass
    except Exception as e:
        error_messages.append(f"Error processing T5: {T5}\n{str(e)}")




def append_matched_invoices(matched_invoices, invoice, output_row):
    if invoice not in matched_invoices:
        matched_invoices.append(invoice)
        return True
    return False

def append_unmatched_invoices(unmatched_fin_record_keys, T5, unmatched_output_row):
    unmatched_fin_record_keys.add(T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value])
    unmatched_output_rows_one_to_one.append(unmatched_output_row)



def process_t5_record(T5, Invoice_table, matched_invoices, unmatched_fin_record_keys,
                      output_rows_one_to_one, unmatched_output_rows_one_to_one):
    matching_invoices, match_rule = match_one_to_one(T5, Invoice_table)
    try:
        if matching_invoices:
            for invoice, rule_index in matching_invoices:
                description = rule_descriptions.get(rule_index)
                output_row = {
                    column: T5[column] if column in T5 else invoice[column]
                    for column in output_columns
                }
                output_row['Match Rule'] = rule_index
                output_row['description'] = description
                output_row['Match Type'] = 'one - to - one'

                if append_matched_invoices(matched_invoices, invoice, output_row):
                    output_rows_one_to_one.append(output_row)

                Invoice_table = [inv for inv in Invoice_table if inv not in matched_invoices]
        else:
            unmatched_output_row = {
                column: T5[column] if column in T5 else None for column in unmatched_output_columns
            }
            if append_unmatched_invoices(unmatched_fin_record_keys, T5, unmatched_output_row):
                unmatched_indices = unmatched_output_df[
                    (unmatched_output_df[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] == T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value]) &
                    (unmatched_output_df[COLUMN_HEADER.T5_SOURCE_AMT.value] == T5[COLUMN_HEADER.T5_SOURCE_AMT.value]) &
                    (unmatched_output_df[COLUMN_HEADER.FIN_DEB_CRED_IND.value] == T5[COLUMN_HEADER.FIN_DEB_CRED_IND.value])
                ].index
                unmatched_output_df.drop(unmatched_indices, inplace=True)
    except Exception as e:
        error_messages.append(f"Error processing T5: {T5}\n{str(e)}")





def match_one_to_one(T5, Invoice_table):
    # Implementation of the match_one_to_one function
    # (You need to provide the actual implementation or replace this with your existing function)

def process_transactions(T5_table, Invoice_table):
    output_rows_one_to_one = []
    unmatched_output_rows_one_to_one = []
    matched_transaction_keys = set()
    matched_invoice_keys = set()
    unmatched_fin_record_keys = set(T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] for T5 in T5_table)
    count = 0

    for T5 in T5_table:
        count += 1
        print(f"Processed Transactions: {count}")

        if T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] in matched_transaction_keys:
            continue  # Skip already matched transactions

        matching_invoices, match_rule = match_one_to_one(T5, Invoice_table)

        try:
            if matching_invoices:
                for invoice, rule_index in matching_invoices:
                    if invoice[COLUMN_HEADER.INVOICE_KEY.value] in matched_invoice_keys:
                        continue  # Skip already matched invoices

                    description = rule_descriptions.get(rule_index)
                    output_row = {
                        column: T5[column] if column in T5 else invoice[column] for column in output_columns
                    }
                    output_row['Match Rule'] = rule_index
                    output_row['Description'] = description
                    output_row['Match Type'] = 'one-to-one'
                    output_rows_one_to_one.append(output_row)

                    matched_transaction_keys.add(T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value])
                    matched_invoice_keys.add(invoice[COLUMN_HEADER.INVOICE_KEY.value])

                # Remove the matched invoices from Invoice_table
                Invoice_table = [inv for inv in Invoice_table if inv[COLUMN_HEADER.INVOICE_KEY.value] not in matched_invoice_keys]

            else:
                unmatched_output_row = {column: T5[column] if column in T5 else None for column in unmatched_output_columns}
                unmatched_output_rows_one_to_one.append(unmatched_output_row)
                unmatched_fin_record_keys.add(T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value])

        except Exception as e:
            error_messages.append(f"Error processing T5: {T5}\n{str(e)}")

    return output_rows_one_to_one, unmatched_output_rows_one_to_one, Invoice_table, unmatched_fin_record_keys

def remove_unmatched_indices(unmatched_output_df, T5):
    unmatched_indices = unmatched_output_df[
        (unmatched_output_df[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] == T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value]) &
        (unmatched_output_df[COLUMN_HEADER.T5_SOURCE_AMT.value] == T5[COLUMN_HEADER.T5_SOURCE_AMT.value]) &
        (unmatched_output_df[COLUMN_HEADER.FIN_DEB_CRED_IND.value] == T5[COLUMN_HEADER.FIN_DEB_CRED_IND.value])
    ].index
    unmatched_output_df.drop(unmatched_indices, inplace=True)

# Usage:
# Call the functions with appropriate parameters

# Separate the T5_table and Invoice_table from your data source or inputs
# T5_table = ...
# Invoice_table = ...

# Process transactions
output_rows_one_to_one, unmatched_output_rows_one_to_one, Invoice_table, unmatched_fin_record_keys = process_transactions(T5_table, Invoice_table)

# Remove unmatched indices
remove_unmatched_indices(unmatched_output_df, T5)





def match_one_to_one(T5, Invoice_table):
    # Your implementation of the match_one_to_one function goes here
    # This function should return matching_invoices and match_rule
    pass


def remove_matched_invoices(Invoice_table, matched_invoices):
    updated_invoice_table = [inv for inv in Invoice_table if inv not in matched_invoices]
    return updated_invoice_table


def remove_matched_transactions(T5_table, matched_invoices):
    updated_T5_table = [T5_row for T5_row in T5_table if T5_row[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] not in matched_invoices]
    return updated_T5_table


def process_data(T5_table, Invoice_table):
    matched_invoices = []
    output_rows_one_to_one = []
    unmatched_output_rows_one_to_one = []
    rule_descriptions = {0: "Rule description 0", 1: "Rule description 1", 2: "Rule description 2"}
    rule_numbers = {0: "Rule 0", 1: "Rule 1", 2: "Rule 2"}

    # Set to store unmatched fin_record_keys
    unmatched_fin_record_keys = set(T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] for T5 in T5_table)

    count = 0
    for T5 in T5_table:
        count += 1
        print(f"Processed Transactions: {count}")

        matching_invoices, match_rule = match_one_to_one(T5, Invoice_table)

        try:
            if matching_invoices:
                for invoice, rule_index in matching_invoices:
                    if invoice not in matched_invoices:
                        description = rule_descriptions.get(rule_index)
                        output_row = {column: T5[column] if column in T5 else invoice[column] for column in output_columns}
                        output_row['Match Rule'] = rule_numbers.get(match_rule)
                        output_row['description'] = description
                        output_row['Match Type'] = 'one-to-one'
                        output_rows_one_to_one.append(output_row)
                        matched_invoices.append(invoice)

                # Remove the matched invoices from Invoice_table
                Invoice_table = remove_matched_invoices(Invoice_table, matched_invoices)

            else:
                unmatched_output_row = {column: T5[column] if column in T5 else None for column in unmatched_output_columns}
                unmatched_output_rows_one_to_one.append(unmatched_output_row)
                unmatched_fin_record_keys.add(T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value])

        except Exception as e:
            error_messages.append(f"Error processing T5: {T5}\n{str(e)}")

        # Remove the matched transactions from T5_table
        T5_table = remove_matched_transactions(T5_table, matched_invoices)

    return output_rows_one_to_one, unmatched_output_rows_one_to_one, unmatched_fin_record_keys, Invoice_table, T5_table


# Now you can call the `process_data` function with your T5_table and Invoice_table data
# and it will process the data using the same logic you provided.

# Example usage:
if __name__ == "__main__":
    T5_table = [...]
    Invoice_table = [...]
    output_columns = [...]
    unmatched_output_columns = [...]
    error_messages = []
    COLUMN_HEADER = {  # Define your column headers here }
    
    output_rows_one_to_one, unmatched_output_rows_one_to_one, unmatched_fin_record_keys, updated_invoice_table, updated_T5_table = process_data(T5_table, Invoice_table)
    
    # Now you can work with the processed data in the variables above.
    # For example, you can save them to a file or use them for further processing.
    
    # Update the T5_table and Invoice_table with the updated versions
    T5_table = updated_T5_table
    Invoice_table = updated_invoice_table







def get_matching_invoices(T5, Invoice_table):
    return match_one_to_one(T5, Invoice_table)

def create_output_row(T5, invoice, match_rule, description):
    output_row = {
        column: T5[column] if column in T5 else invoice[column]
        for column in output_columns
    }
    output_row['Match Rule'] = rule_numbers.get(match_rule)
    output_row['description'] = description
    output_row['Match Type'] = 'one - to - one'
    return output_row

def process_one_to_one_match(T5, Invoice_table, matched_invoices_one_to_one, unmatched_fin_record_keys,
                             output_rows_one_to_one, unmatched_output_rows_one_to_one):
    matching_invoices, match_rule = get_matching_invoices(T5, Invoice_table)
    if matching_invoices:
        for invoice, rule_index in matching_invoices:
            description = rule_descriptions.get(rule_index)
            output_row = create_output_row(T5, invoice, match_rule, description)
            output_rows_one_to_one.append(output_row)
            append_matched_invoices(matched_invoices_one_to_one, invoice)

        Invoice_table = remove_matched_invoices(Invoice_table, matched_invoices_one_to_one)
    else:
        unmatched_output_row = {
            column: T5[column] if column in T5 else None for column in unmatched_output_columns
        }
        unmatched_output_rows_one_to_one.append(unmatched_output_row)

# Main loop for one-to-one matches
matched_invoices_one_to_one = []
unmatched_fin_record_keys_one_to_one = set()

for count, T5 in enumerate(T5_table, start=1):
    print(f"Processed Transactions: {count}")
    process_one_to_one_match(T5, Invoice_table, matched_invoices_one_to_one, unmatched_fin_record_keys_one_to_one,
                             output_rows_one_to_one, unmatched_output_rows_one_to_one)







# ... (Previous code for one-to-one and bundle matching)

def sort_data_by_account_number(data_table, account_number_column):
    sorted_data = sorted(data_table, key=lambda x: x[account_number_column])
    return sorted_data

# Assuming the 'acct_number' column is used for sorting in both datasets
sorted_T5_table = sort_data_by_account_number(T5_table, 'acct_number')
sorted_Invoice_table = sort_data_by_account_number(Invoice_table, 'acct_number')

# Main loop for one-to-one matches
matched_invoices_one_to_one = []
unmatched_fin_record_keys_one_to_one = set()

for count, T5 in enumerate(sorted_T5_table, start=1):
    print(f"Processed Transactions: {count}")
    sorted_Invoice_table, sorted_T5_table = process_one_to_one_match(T5, sorted_Invoice_table, matched_invoices_one_to_one, unmatched_fin_record_keys_one_to_one,
                                                                     output_rows_one_to_one, unmatched_output_rows_one_to_one)

# Main loop for bundle matches with the remaining T5 records and invoices
matched_invoices_bundle = []
unmatched_fin_record_keys_bundle = set()
matched_count = 0

for T5 in sorted_T5_table:
    sorted_T5_table, sorted_Invoice_table = process_bundle_match(T5, sorted_Invoice_table, matched_invoices_bundle, sorted_T5_table, unmatched_fin_record_keys_bundle,
                                                                 output_rows_bundle, unmatched_output_rows_bundle, matched_count)

# The updated sorted_T5_table and sorted_Invoice_table are available after the loops for further processing or updates.
# Use sorted_T5_table and sorted_Invoice_table here for any further processing or updates.




def sort_data_by_account_number(data_table, account_number_column):
    sorted_data = sorted(data_table, key=lambda x: x[account_number_column])
    return sorted_data

# Assuming the 'acct_number' column is used for sorting in both datasets
sorted_T5_table = sort_data_by_account_number(T5_table, 'acct_number')
sorted_Invoice_table = sort_data_by_account_number(Invoice_table, 'acct_number')






import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Lambda
from tensorflow.keras.models import Model
import math

# Preprocessing
def preprocess_string(s):
    return s.lower()

# Siamese Neural Network Architecture
def build_siamese_model(embedding_dim, vocab_size, max_sequence_length):
    input_layer = Input(shape=(max_sequence_length,))
    embedding_layer = Embedding(vocab_size, embedding_dim)(input_layer)
    flatten_layer = Flatten()(embedding_layer)
    dense_layer = Dense(128, activation='relu')(flatten_layer)
    similarity_output = Dense(1, activation='sigmoid')(dense_layer)  # Output similarity score
    
    return Model(inputs=input_layer, outputs=similarity_output)

# Calculate Similarity Percentage
def calculate_similarity_percentage(str1, str2, model, tokenizer, max_sequence_length):
    str1 = preprocess_string(str1)
    str2 = preprocess_string(str2)
    
    # Tokenize strings and convert to sequences
    seq1 = tokenizer.texts_to_sequences([str1])
    seq2 = tokenizer.texts_to_sequences([str2])
    
    # Pad sequences to a fixed length
    seq1_padded = tf.keras.preprocessing.sequence.pad_sequences(seq1, maxlen=max_sequence_length)
    seq2_padded = tf.keras.preprocessing.sequence.pad_sequences(seq2, maxlen=max_sequence_length)
    
    # Calculate similarity score using the Siamese model
    similarity_score = model.predict([seq1_padded, seq2_padded])
    similarity_percentage = similarity_score[0][0] * 100
    
    return similarity_percentage

# Example usage
embedding_dim = 50  # Choose an appropriate embedding dimension
vocab_size = 10000  # Choose an appropriate vocabulary size
max_sequence_length = 20  # Choose a maximum sequence length
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)

siamese_model = build_siamese_model(embedding_dim, vocab_size, max_sequence_length)
siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

similarity_percentage = calculate_similarity_percentage("hello world", "world hello", siamese_model, tokenizer, max_sequence_length)
print(f"Similarity Percentage: {similarity_percentage:.2f}%")







import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Sample data for demonstration purposes
data_t5 = {
    'AuthNo': [123, 124, 125, 126],
    'SupplierName': ['Supplier A', 'Supplier B', 'Supplier C', 'Supplier D'],
    'SourceAmt': [100, 200, 150, 300]
}

data_invoice = {
    'InvNumber': ['INV001', 'INV002', 'INV003', 'INV004'],
    'AuthNo': [123, 124, 127, 126],
    'SourceAmt': [100, 180, 140, 300]
}

# Create dataframes
df_t5 = pd.DataFrame(data_t5)
df_invoice = pd.DataFrame(data_invoice)

# Create a dictionary to map unique supplier names to indices
unique_supplier_names = np.unique(df_t5['SupplierName'].values)
supplier_name_to_index = {name: idx for idx, name in enumerate(unique_supplier_names)}

# Prepare training data
pairs = []
labels = []

for idx, row in df_t5.iterrows():
    supplier_name = row['SupplierName']
    inv_number = df_invoice[df_invoice['AuthNo'] == row['AuthNo']]['InvNumber'].values[0]
    
    pairs.append([supplier_name_to_index[supplier_name], inv_number])
    labels.append(1)  # Positive pair
    
    # Creating negative pairs by shuffling supplier names and inv numbers
    shuffled_supplier = np.random.choice(unique_supplier_names)
    shuffled_inv_number = np.random.choice(df_invoice['InvNumber'].values)
    pairs.append([supplier_name_to_index[shuffled_supplier], shuffled_inv_number])
    labels.append(0)  # Negative pair

pairs = np.array(pairs)
labels = np.array(labels)

# Neural Network model
num_unique_supplier_names = len(unique_supplier_names)
embedding_dim = 50

supplier_name_input = Input(shape=(1,))
supplier_name_embedding = Embedding(input_dim=num_unique_supplier_names, output_dim=embedding_dim)(supplier_name_input)
supplier_name_flatten = Flatten()(supplier_name_embedding)

inv_number_input = Input(shape=(1,))
inv_number_embedding = Embedding(input_dim=len(df_invoice['InvNumber'].unique()), output_dim=embedding_dim)(inv_number_input)
inv_number_flatten = Flatten()(inv_number_embedding)

merged_output = Concatenate()([supplier_name_flatten, inv_number_flatten])
dense_1 = Dense(128, activation='relu')(merged_output)
output_layer = Dense(1, activation='sigmoid')(dense_1)

siamese_model = Model(inputs=[supplier_name_input, inv_number_input], outputs=output_layer)
siamese_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

# Training the model
siamese_model.fit([pairs[:, 0], pairs[:, 1]], labels, epochs=10, batch_size=16)

# Use the trained model for predictions
predictions = siamese_model.predict([pairs[:, 0], pairs[:, 1]])

# You can use predictions to identify matched pairs based on a threshold

# For example:
threshold = 0.5
matched_indices = np.where(predictions > threshold)[0]
matched_pairs = pairs[matched_indices]

for idx in matched_indices:
    print(f"Matched: Supplier '{unique_supplier_names[pairs[idx, 0]]}' with Invoice Number '{pairs[idx, 1]}'")










import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Generate a mock dataset
def generate_mock_data(num_samples, max_sequence_length, vocabulary_size):
    data_a = np.random.randint(0, vocabulary_size, size=(num_samples, max_sequence_length))
    data_b = np.random.randint(0, vocabulary_size, size=(num_samples, max_sequence_length))
    labels = np.random.randint(0, 2, size=num_samples)
    return data_a, data_b, labels

# Siamese network architecture
def siamese_model(input_shape, vocabulary_size, embedding_dim):
    input_a = Input(shape=input_shape)
    input_b = Input(shape=input_shape)
    
    embedding_layer = Embedding(vocabulary_size, embedding_dim)
    
    encoded_a = embedding_layer(input_a)
    encoded_a = LSTM(128)(encoded_a)
    
    encoded_b = embedding_layer(input_b)
    encoded_b = LSTM(128)(encoded_b)
    
    distance = Lambda(lambda x: tf.abs(x[0] - x[1]))([encoded_a, encoded_b])
    output = Dense(1, activation='sigmoid')(distance)
    
    model = Model(inputs=[input_a, input_b], outputs=output)
    return model

# Generate mock dataset
num_samples = 1000
max_sequence_length = 20
vocabulary_size = 10000
embedding_dim = 100

data_a, data_b, labels = generate_mock_data(num_samples, max_sequence_length, vocabulary_size)

# Create and compile the Siamese model
input_shape = (max_sequence_length,)
model = siamese_model(input_shape, vocabulary_size, embedding_dim)
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit([data_a, data_b], labels, batch_size=64, epochs=10, validation_split=0.2)

# Print predictions
predictions = model.predict([data_a, data_b])
for i in range(10):  # Print predictions for the first 10 samples
    print(f"Pair {i+1}: Actual Label = {labels[i]}, Predicted Label = {predictions[i][0]:.4f}")















import pandas as pd

def vids_preprocessing(input_file, output_file):
    # Read data from the input sheet into a DataFrame
    vids = pd.read_excel(input_file)

    # Filter rows with specific conditions (Transaction Type Code and Merchant Category Code)
    vids = vids[vids['Transaction Type Code'].isin([10, 111])]
    vids = vids[vids['Merchant Category Code'] != 0]

    # Create a new DataFrame containing rows with blank invoice numbers
    blank_invoice_rows = vids[vids['Invoice Number'].isnull()]

    # Write the filtered DataFrame to a new sheet in the output Excel file
    # Delete rows with blank invoice numbers from the main sheet
    vids = vids.dropna(subset=['Invoice Number'])

    # Create a Pandas ExcelWriter object
    with pd.ExcelWriter(output_file) as writer:
        # Write the filtered DataFrame (main sheet) to a new sheet in the output Excel file
        vids.to_excel(writer, sheet_name='Main Sheet', index=False)

        # Write the DataFrame containing blank invoice rows to another sheet
        blank_invoice_rows.to_excel(writer, sheet_name='Blank Invoice Rows', index=False)

    print("Blank invoice rows have been copied to a new sheet, and blank rows have been deleted from the main sheet.")


# Example usage:
input_file = 'Z:/Desktop/Recon_automation/FORD 3143.xlsx'  # Replace with your input file path
output_file = 'output.xlsx'  # Replace with your desired output file name
vids_preprocessing(input_file, output_file)







def modify_main_sheet(input_file, output_file):
    # Read data from the input sheet into a DataFrame
    xls = pd.ExcelFile(input_file)
    sheet_names = xls.sheet_names

    # Create a Pandas ExcelWriter object
    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
        # Process each sheet in the input Excel file
        for sheet_name in sheet_names:
            # Read data from the current sheet into a DataFrame
            sheet_df = pd.read_excel(input_file, sheet_name=sheet_name)
            
            # Modify the "Main Sheet" by concatenating "-" for rows where Transaction Code is 11
            if sheet_name == 'Main Sheet':
                sheet_df.loc[sheet_df['Transaction Code'] == 11, 'Source Amount'] = '-' + sheet_df.loc[sheet_df['Transaction Code'] == 11, 'Source Amount']
                
            # Write the modified DataFrame to the output Excel file
            sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)

    print("Modified the 'Main Sheet' by concatenating '-' for rows where Transaction Code is 11 and saved to the output file.")




    # Modify the "Main Sheet" by concatenating "-" for rows where Transaction Code is 11
    vids.loc[vids['Transaction Code'] == 11, 'Source Amount'] = '-' + vids.loc[vids['Transaction Code'] == 11, 'Source Amount']






class InvoiceMatcher:
    def __init__(self, t5_table, invoice_table):
        self.t5_table = t5_table
        self.invoice_table = invoice_table
        self.matched_invoices = []
        self.error_messages = []
        self.unmatched_output_rows_one_to_one = []
        self.unmatched_fin_record_keys = set()

    def process_t5_table(self):
        for t5 in self.t5_table:
            try:
                self.process_transaction(t5)
            except Exception as e:
                self.error_messages.append(f"Error processing T5: {t5} \n{str(e)}")

    def process_transaction(self, t5):
        matching_invoices, match_rule = self.match_one_to_one(t5)

        if matching_invoices:
            for invoice, rule_index in matching_invoices:
                if invoice not in self.matched_invoices:
                    description = self.rule_descriptions.get(rule_index)
                    output_row = self.create_output_row(t5, invoice, rule_index, description)
                    self.unmatched_output_rows_one_to_one.append(output_row)
                    self.matched_invoices.append(invoice)
                    self.remove_matched_invoice(invoice)
            self.remove_matched_transaction(t5)

    def match_one_to_one(self, t5):
        # Placeholder for matching logic - Replace with actual matching logic
        matching_invoices = []
        match_rule = "SampleRule"
        return matching_invoices, match_rule

    def create_output_row(self, t5, invoice, rule_index, description):
        # Define your output_columns
        output_columns = ['column1', 'column2', 'column3']
        output_row = {column: t5[column] if column in t5 else invoice[column] for column in output_columns}
        output_row['Match Rule'] = rule_index
        output_row['description'] = description
        output_row['Match Type'] = 'one-to-one'
        return output_row

    def remove_matched_invoice(self, invoice):
        self.invoice_table = [inv for inv in self.invoice_table if inv != invoice]

    def remove_matched_transaction(self, t5):
        t5_record_key = t5['INPUT_T5_RECORD_KEY']
        self.t5_table = [t5_row for t5_row in self.t5_table if t5_row['INPUT_T5_RECORD_KEY'] != t5_record_key]
        self.unmatched_fin_record_keys.remove(t5_record_key)

    def process_unmatched_output(self):
        # Placeholder for processing unmatched output - Replace with actual logic
        pass

    def run(self):
        self.process_t5_table()
        self.process_unmatched_output()


# Example Data
sample_t5_table = [
    {'INPUT_T5_RECORD_KEY': 'T5_1', 'column1': 'value1', 'column2': 'value2'},
    {'INPUT_T5_RECORD_KEY': 'T5_2', 'column1': 'value3', 'column2': 'value4'},
]

sample_invoice_table = [
    {'invoice_id': 'INV_1', 'column1': 'value5', 'column2': 'value6'},
    {'invoice_id': 'INV_2', 'column1': 'value7', 'column2': 'value8'},
]

# Instantiate and run the InvoiceMatcher
matcher = InvoiceMatcher(sample_t5_table, sample_invoice_table)
matcher.run()

# Access the results
print("Matched Invoices:", matcher.matched_invoices)
print("Error Messages:", matcher.error_messages)
print("Unmatched Output Rows (One-to-One):", matcher.unmatched_output_rows_one_to_one)
















def process_matching_invoices(T5, Invoice_table, matched_invoices, output_rows_one_to_one, rule_descriptions, rule_numbers):
    matching_invoices, match_rule = match_one_to_one(T5, Invoice_table)  # You should pass T5 and Invoice_table here
    if matching_invoices:
        for invoice, rule_index in matching_invoices:
            if invoice not in matched_invoices:
                description = rule_descriptions.get(rule_index)
                output_row = {column: T5[column] if column in T5 else invoice[column] for column in output_columns}
                output_row['Match Rule'] = rule_index
                output_row['description'] = description
                output_row['Match Type'] = 'one-to-one'
                output_rows_one_to_one.append(output_row)
                matched_invoices.append(invoice)
    
    return matched_invoices

def remove_matched_invoices_from_tables(matched_invoices, Invoice_table):
    Invoice_table = [inv for inv in Invoice_table if inv not in matched_invoices]
    return Invoice_table

def remove_matched_transactions_from_T5(T5_table, matched_fin_record_keys):
    T5_table = [T5_row for T5_row in T5_table if T5_row[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] not in matched_fin_record_keys]
    return T5_table

def process_unmatched_output(T5, unmatched_output_df, unmatched_output_rows_one_to_one):
    unmatched_indices = unmatched_output_df[
        (unmatched_output_df[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] == T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value]) &
        (unmatched_output_df[COLUMN_HEADER.T5_SOURCE_AMT.value] == T5[COLUMN_HEADER.T5_SOURCE_AMT.value]) &
        (unmatched_output_df[COLUMN_HEADER.FIN_DEB_CRED_IND.value] == T5[COLUMN_HEADER.FIN_DEB_CRED_IND.value])
    ].index

    unmatched_output_df.drop(unmatched_indices, inplace=True)
    unmatched_output_row = {column: T5[column] if column in T5 else None for column in unmatched_output_columns}
    unmatched_output_rows_one_to_one.append(unmatched_output_row)

def process_T5_records(T5_table, unmatched_fin_record_keys, unmatched_output_df, unmatched_output_rows_one_to_one, matched_invoices, output_rows_one_to_one, rule_descriptions, rule_numbers, output_columns, unmatched_output_columns):
    error_messages = []
    
    for T5 in T5_table:
        try:
            matched_invoices = process_matching_invoices(T5, Invoice_table, matched_invoices, output_rows_one_to_one, rule_descriptions, rule_numbers)
            Invoice_table = remove_matched_invoices_from_tables(matched_invoices, Invoice_table)
            T5_table = remove_matched_transactions_from_T5(T5_table, matched_fin_record_keys)
            unmatched_fin_record_keys.remove(T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value])
            process_unmatched_output(T5, unmatched_output_df, unmatched_output_rows_one_to_one)
        
        except Exception as e:
            error_messages.append(f"Error processing T5: {T5} \n{str(e)}")
            continue

    return matched_invoices, Invoice_table, T5_table, error_messages



# Define your data and variables
T5_table = [...]  # Your T5 table data
Invoice_table = [...]  # Your Invoice table data
# Define other variables such as output_columns, unmatched_output_columns, rule_descriptions, rule_numbers, etc.

# Initialize variables
matched_invoices = []
unmatched_fin_record_keys = set(T5[COLUMN_HEADER.INPUT_T5_RECORD_KEY.value] for T5 in T5_table)
unmatched_output_df = ...  # Initialize your unmatched output DataFrame
unmatched_output_rows_one_to_one = []
error_messages = []

# Call the processing function
matched_invoices, Invoice_table, T5_table, error_messages = process_T5_records(
    T5_table, unmatched_fin_record_keys, unmatched_output_df, unmatched_output_rows_one_to_one,
    matched_invoices, output_rows_one_to_one, rule_descriptions, rule_numbers,
    output_columns, unmatched_output_columns
)

# Now, you can handle the results, such as printing error messages or working with the updated data structures.
for message in error_messages:
    print(message)

# You can also use the updated Invoice_table, T5_table, and other data structures as needed.





   # Copy the "Amount" column values to the respective "Source Amount" column rows
    vids['Source Amount'] = vids.apply(lambda row: row['Amount'] if pd.notnull(row['Amount']) else row['Source Amount'], axis=1)



 # If Transaction Type Code is 11, make the Amount column value negative
    vids.loc[vids['Transaction Code'] == 11, 'Amount'] = -vids.loc[vids['Transaction Code'] == 11, 'Amount']


    # Remove duplicates from the "vids" DataFrame based on 'trn', 'amount', and 'auth' columns
    vids.drop_duplicates(subset=['trn', 'amount', 'auth'], keep='first', inplace=True)

    # Remove duplicates from the "blank_invoice_rows" DataFrame based on 'trn', 'amount', and 'auth' columns
    blank_invoice_rows.drop_duplicates(subset=['trn', 'amount', 'auth'], keep='first', inplace=True)




        # Check if there are empty invoice rows before adding column names
        if not blank_invoice_rows.empty:
            # Write the "Blank Invoice Rows" DataFrame to the same sheet with column names
            blank_invoice_rows.to_excel(writer, sheet_name='Main Sheet', index=False, startrow=start_row, header=False)

            # Write the column names for "Blank Invoice Rows" below the gap
            for col_num, value in enumerate(blank_invoice_rows.columns.values):
                writer.sheets['Main Sheet'].write(start_row, col_num, value)


            # Calculate the sum of the "Source Amount" column for the "blank_invoice_rows" DataFrame
            sum_blank_invoice_rows = blank_invoice_rows['Source Amount'].sum()

            # Leave 4 empty lines before showing the sum value
            start_row += 4

            # Write the sum value to the "Main Sheet"
            writer.sheets['Main Sheet'].write(start_row, 0, 'Sum of Source Amount for Blank Invoice Rows:')
            writer.sheets['Main Sheet'].write(start_row, 1, sum_blank_invoice_rows)













import pandas as pd

def vids_preprocessing_and_modify(input_file, output_file):
    # Read data from the input sheet into a DataFrame
    vids = pd.read_excel(input_file)

    # Remove duplicate rows from the "vids" DataFrame
    vids = vids.drop_duplicates()

    # Create a new DataFrame containing rows with blank invoice numbers
    blank_invoice_rows = vids[vids['Invoice Number'].isnull()]

    # Remove duplicate rows from the "blank_invoice_rows" DataFrame
    blank_invoice_rows = blank_invoice_rows.drop_duplicates()

    # Filter rows with specific conditions (Transaction Type Code and Merchant Category Code)
    vids = vids[
        (vids['Transaction Type Code'].isin([10, 111])) &
        (vids['Merchant Category Code'] != 0) &
        (~vids['Invoice Number'].isnull())  # Remove rows with blank invoice numbers
    ]

    # Modify the "Main Sheet" by concatenating "-" for rows where Transaction Code is 11
    vids.loc[vids['Transaction Code'] == 11, 'Source Amount'] = '-' + vids.loc[vids['Transaction Code'] == 11, 'Source Amount']

    # If Transaction Type Code is 11, make the Amount column value negative
    vids.loc[vids['Transaction Code'] == 11, 'Amount'] = -vids.loc[vids['Transaction Code'] == 11, 'Amount']

    # Copy the "Amount" column values to the respective "Source Amount" column rows
    vids['Source Amount'] = vids.apply(lambda row: row['Amount'] if pd.notnull(row['Amount']) else row['Source Amount'], axis=1)

    # Calculate the sum of the "Source Amount" column for both DataFrames combined
    combined_source_amount_sum = vids['Source Amount'].sum() + blank_invoice_rows['Source Amount'].sum()









import pandas as pd
import openpyxl

def vids_preprocessing_and_modify(input_file, output_file):
    # Read data from the input sheet into a DataFrame
    vids = pd.read_excel(input_file)

    # Remove duplicate rows from the "vids" DataFrame
    vids = vids.drop_duplicates()

    # Create a new DataFrame containing rows with blank invoice numbers
    blank_invoice_rows = vids[vids['Invoice Number'].isnull()]

    # Remove duplicate rows from the "blank_invoice_rows" DataFrame
    blank_invoice_rows = blank_invoice_rows.drop_duplicates()

    # Filter rows with specific conditions (Transaction Type Code and Merchant Category Code)
    vids = vids[
        (vids['Transaction Type Code'].isin([10, 111])) &
        (vids['Merchant Category Code'] != 0) &
        (~vids['Invoice Number'].isnull())  # Remove rows with blank invoice numbers
    ]

    # Modify the "Main Sheet" by concatenating "-" for rows where Transaction Code is 11
    vids.loc[vids['Transaction Code'] == 11, 'Source Amount'] = '-' + vids.loc[vids['Transaction Code'] == 11, 'Source Amount']

    # If Transaction Type Code is 11, make the Amount column value negative
    vids.loc[vids['Transaction Code'] == 11, 'Amount'] = -vids.loc[vids['Transaction Code'] == 11, 'Amount']

    # Copy the "Amount" column values to the respective "Source Amount" column rows
    vids['Source Amount'] = vids.apply(lambda row: row['Amount'] if pd.notnull(row['Amount']) else row['Source Amount'], axis=1)

    # Calculate the sum of "Source Amount" for both DataFrames
    vids_source_amount_sum = vids['Source Amount'].sum()
    blank_invoice_source_amount_sum = blank_invoice_rows['Source Amount'].sum()

    # Create a Pandas ExcelWriter object with the openpyxl engine
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        # Write the filtered DataFrame (main sheet) to a new sheet in the output Excel file
        vids.to_excel(writer, sheet_name='Main Sheet', index=False)

        # Calculate the start row for the "Blank Invoice Rows" insertion
        start_row = vids.shape[0] + 10  # Start 10 rows below the end of "Main Sheet"

        # Check if there are empty invoice rows before adding column names
        if not blank_invoice_rows.empty:
            # Write the "Blank Invoice Rows" DataFrame to the same sheet with column names
            blank_invoice_rows.to_excel(writer, sheet_name='Main Sheet', index=False, startrow=start_row, header=False)

            # Write the column names for "Blank Invoice Rows" below the gap
            for col_num, value in enumerate(blank_invoice_rows.columns.values):
                writer.sheets['Main Sheet'].cell(row=start_row, column=col_num + 1, value=value)

            # Add a two-line gap
            start_row += 2

        # Write the sum of "Source Amount" for both DataFrames with SUM formula
        # Write the sum of "Source Amount" for both DataFrames with SUM formula
        worksheet = writer.sheets['Main Sheet']
        worksheet.cell(row=start_row, column=1, value='Total Source Amount (vids):')
        worksheet.cell(row=start_row, column=2, value=f'=SUM(Source Amount)')
        worksheet.cell(row=start_row + 1, column=1, value='Total Source Amount (blank_invoice_rows):')
        worksheet.cell(row=start_row + 1, column=2, value=f'=SUM(Source Amount)')

    writer.save()
    print("Duplicate rows have been removed from both the 'vids' and 'blank_invoice_rows' DataFrames. Blank invoice rows have been copied to the 'Main Sheet' with a gap of 10 lines (if they exist), and the 'Main Sheet' has been modified. The sum of 'Source Amount' for both DataFrames has been added to the output file.")

# Example usage:
input_file = 'Z:/Desktop/Recon_automation/FORD 3143.xlsx'  # Replace with your input file path
output_file = 'output.xlsx'  # Replace with your desired output file name
vids_preprocessing_and_modify(input_file, output_file)






 # Calculate the sum of the "Source Amount" column for both DataFrames
    combined_source_amt_sum = vids['Source Amount'].sum() + blank_invoice_rows['Source Amount'].sum()

    # Create a Pandas ExcelWriter object
    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
        # Write the filtered DataFrame (main sheet) to a new sheet in the output Excel file
        vids.to_excel(writer, sheet_name='Main Sheet', index=False)

        # Calculate the start row for the "Blank Invoice Rows" insertion
        start_row = len(vids) + 12  # Start 12 rows below the end of "Main Sheet"

        # Check if there are empty invoice rows before adding column names
        if not blank_invoice_rows.empty:
            # Write the "Blank Invoice Rows" DataFrame to the same sheet with column names
            blank_invoice_rows.to_excel(writer, sheet_name='Main Sheet', index=False, startrow=start_row, header=False)

            # Write the column names for "Blank Invoice Rows" below the gap
            for col_num, value in enumerate(blank_invoice_rows.columns.values):
                writer.sheets['Main Sheet'].write(start_row, col_num, value)

        # Add the combined sum of "Source Amount" to the sheet
        worksheet = writer.sheets['Main Sheet']
        worksheet.write(len(vids) + 2, 1, 'Combined Total Source Amount:')
        worksheet.write_number(len(vids) + 2, 2, combined_source_amt_sum)




1. One-to-One matching is a process that associates a transaction with one specific invoice, based on a predefined set of rules.


2.Bundle matching is a procedure that pairs a transaction with multiple invoices, where the total source amount of these invoices equals the source amount of 
the transaction, all in accordance with established rules.











import pandas as pd

def remove_duplicate_rows(df):
    return df.drop_duplicates()

def filter_rows(df):
    return df[
        (df['Transaction Type Code'].isin([10, 111])) &
        (df['Merchant Category Code'] != 0) &
        (~df['Invoice Number'].isnull())
    ]

def modify_rows(df):
    df.loc[df['Transaction Code'] == 11, 'Source Amount'] = -df.loc[df['Transaction Code'] == 11, 'Source Amount']
    df.loc[df['Transaction Code'] == 11, 'Amount'] = -df.loc[df['Transaction Code'] == 11, 'Amount']
    df['Source Amount'].fillna(df['Amount'], inplace=True)
    return df

def combine_and_sum(source_df, blank_invoice_df):
    combined_df = pd.concat([source_df, blank_invoice_df])
    combined_source_amt_sum = combined_df['Source Amount'].sum()
    return combined_df, combined_source_amt_sum

def write_to_excel(df, output_file):
    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
        df.to_excel(writer, sheet_name='Main Sheet', index=False)
        worksheet = writer.sheets['Main Sheet']
        worksheet.write(len(df) + 2, 1, 'Combined Total Source Amount:')
        worksheet.write_number(len(df) + 2, 2, combined_source_amt_sum)

def main(input_file, output_file):
    # Read data from the input sheet into a DataFrame
    vids = pd.read_excel(input_file)

    # Remove duplicate rows from the DataFrame
    vids = remove_duplicate_rows(vids)

    # Separate rows with blank invoice numbers
    blank_invoice_rows = vids[vids['Invoice Number'].isnull()]
    blank_invoice_rows = remove_duplicate_rows(blank_invoice_rows)

    # Filter rows based on specific conditions
    source_df = filter_rows(vids)

    # Modify rows based on Transaction Code
    source_df = modify_rows(source_df)

    # Combine DataFrames and calculate the sum of Source Amount
    combined_df, combined_source_amt_sum = combine_and_sum(source_df, blank_invoice_rows)

    # Write the result to the output Excel file
    write_to_excel(combined_df, output_file)

    print("Duplicate rows have been removed from both the 'vids' and 'blank_invoice_rows' DataFrames. Blank invoice rows have been copied to the 'Main Sheet' (if they exist), and the 'Main Sheet' has been modified with a combined sum for 'Source Amount'. The result is saved to the output file.")

# Example usage:
input_file = 'example_data.xlsx'  # Replace with your input file path
output_file = 'output.xlsx'  # Replace with your desired output file name
main(input_file, output_file)









def insert_empty_rows(df, num_rows):
    empty_rows = pd.DataFrame('', index=range(num_rows), columns=df.columns)
    return pd.concat([empty_rows, df])

def write_to_excel(df, output_file, combined_source_amt_sum):
    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
        df.to_excel(writer, sheet_name='Main Sheet', index=False)
        worksheet = writer.sheets['Main Sheet']
        worksheet.write(len(df) + 12, 1, 'Combined Total Source Amount:')
        worksheet.write_number(len(df) + 12, 2, combined_source_amt_sum)

def main(input_file, output_file):
    # Read data from the input sheet into a DataFrame
    vids = pd.read_excel(input_file)

    # Remove duplicate rows from the DataFrame
    vids = remove_duplicate_rows(vids)

    # Get rows with blank invoice numbers
    blank_invoice_rows = get_blank_invoice_rows(vids)

    # Filter rows based on specific conditions
    source_df = filter_rows(vids)

    # Modify rows based on Transaction Code
    source_df = modify_rows(source_df)

    # Insert empty rows above blank invoice rows
    num_empty_rows = 10  # You can adjust the number of empty rows as needed
    blank_invoice_rows = insert_empty_rows(blank_invoice_rows, num_empty_rows)

    # Combine DataFrames and calculate the sum of Source Amount
    combined_df, combined_source_amt_sum = combine_and_sum(source_df, blank_invoice_rows)

    # Write the result to the output Excel file
    write_to_excel(combined_df, output_file, combined_source_amt_sum)





















Explanation:

1. The function calculate_similarity_percentage takes three parameters:

str1: The first input string for which similarity needs to be calculated.
str2: The second input string for which similarity needs to be calculated.
n: The size of character n-grams to be used in the similarity calculation.

2. Both input strings str1 and str2 are converted to lowercase using the lower() method to ensure case-insensitive comparison.

3. Character n-grams are generated for both strings. An n-gram is a contiguous sequence of 'n' characters. 
These n-grams are created by iterating through the strings and extracting substrings of length 'n'. These substrings are stored in the lists ngrams_str1 and ngrams_str2.

4. The dot product between the n-gram vectors is calculated. The dot product is a measure of how many n-grams are common to both strings. 
This is done by iterating through ngrams_str1 and counting how many n-grams are present in ngrams_str2.

5. The Euclidean norms for both n-gram vectors are calculated. The Euclidean norm of a vector is the square root of the sum of the squares of its elements. 
In this context, it's used to normalize the vectors.

6. The cosine similarity between the two n-gram vectors is calculated using the dot product and normalized vectors. Cosine similarity is a measure 
of how similar two vectors are in a multi-dimensional space.

7. The result is returned as a percentage by multiplying the cosine similarity by 100.

8. If the denominator (norm_str1 * norm_str2) is zero (indicating one of the strings has no n-grams), the function returns 0 to avoid division by zero.







import os
import pandas as pd

# Load dataset 1
dataset1 = pd.read_csv('dataset1.csv')

# Load dataset 2
dataset2 = pd.read_csv('dataset2.csv')

# Create a folder for batches
if not os.path.exists('batch1'):
    os.makedirs('batch1')

# Identify unique account numbers in both datasets
unique_account_numbers1 = dataset1['Account Number'].unique()
unique_account_numbers2 = dataset2['Account Number'].unique()

# Find common account numbers
common_account_numbers = set(unique_account_numbers1) & set(unique_account_numbers2)

# Split into batches and save in "batch1" folder
for account_number in common_account_numbers:
    batch1 = dataset1[dataset1['Account Number'] == account_number]
    batch2 = dataset2[dataset2['Account Number'] == account_number]
    
    # Save each batch as a separate CSV file in the "batch1" folder
    batch1.to_csv(f'batch1/batch_{account_number}.csv', index=False)
    batch2.to_csv(f'batch1/batch_{account_number}_dataset2.csv', index=False)
















import numpy as np
import pandas as pd
import os

# Load dataset 1
dataset1 = pd.read_csv('dataset1.csv')

# Load dataset 2
dataset2 = pd.read_csv('dataset2.csv')

# Get unique account numbers from both datasets
unique_account_numbers1 = dataset1['Account Number'].unique()
unique_account_numbers2 = dataset2['Account Number'].unique()

# Combine the unique account numbers into one set
all_account_numbers = set(unique_account_numbers1) | set(unique_account_numbers2)

# Calculate the number of account numbers per batch
num_account_numbers_per_batch = len(all_account_numbers) // 5

# Create a folder for batches
if not os.path.exists('batches'):
    os.makedirs('batches')

# Split account numbers into 5 batches
batches = np.array_split(list(all_account_numbers), 5)

# Process and save batches for dataset 1
for i, batch_account_numbers in enumerate(batches):
    # Select rows from dataset 1 with account numbers in the current batch
    batch1 = dataset1[dataset1['Account Number'].isin(batch_account_numbers)]
    
    # Save the batch from dataset 1 as a separate CSV file
    batch1.to_csv(f'batches/batch_{i + 1}_dataset1.csv', index=False)

# Process and save batches for dataset 2
for i, batch_account_numbers in enumerate(batches):
    # Select rows from dataset 2 with account numbers in the current batch
    batch2 = dataset2[dataset2['Account Number'].isin(batch_account_numbers)]
    
    # Save the batch from dataset 2 as a separate CSV file
    batch2.to_csv(f'batches/batch_{i + 1}_dataset2.csv', index=False)








import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split

# Example data
data1 = {
    "ID": [1, 2, 3, 4, 5],
    "Name": ["John", "Alice", "Bob", "Charlie", "David"],
    "Age": [25, 30, 22, 35, 28],
}

data2 = {
    "ID": [6, 7, 8, 9, 10],
    "Name": ["John", "Alice", "Eve", "David", "Frank"],
    "Age": [25, 30, 23, 28, 32],
}

dataset1 = pd.DataFrame(data1)
dataset2 = pd.DataFrame(data2)

# Create a labeled dataset for training (1 for matching, 0 for non-matching)
positive_pairs = []
negative_pairs = []

# Assuming "Name" is the column you want to match
for i, row1 in dataset1.iterrows():
    for j, row2 in dataset2.iterrows():
        similarity_score = int(row1["Name"] == row2["Name"])
        pair = (row1, row2, similarity_score)
        if similarity_score == 1:
            positive_pairs.append(pair)
        else:
            negative_pairs.append(pair)

# Combine positive and negative pairs
pairs = positive_pairs + negative_pairs
np.random.shuffle(pairs)
pairs = np.array(pairs)

# Split the dataset into training and validation
X = pairs[:, [0, 1]]
y = pairs[:, 2].astype(int)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the Siamese network architecture
def create_siamese_network(input_shape):
    input_layer = Input(shape=input_shape)
    embedding_layer = Embedding(input_dim=10000, output_dim=128)(input_layer)
    flatten_layer = Flatten()(embedding_layer)
    dense_layer = Dense(128, activation='relu')(flatten_layer)
    return Model(inputs=input_layer, outputs=dense_layer)

# Create the Siamese network
input_shape = (2,)  # Each input is a pair of records
siamese_network = create_siamese_network(input_shape)

# Define the final model that computes similarity
input_a = Input(shape=input_shape)
input_b = Input(shape=input_shape)
output_a = siamese_network(input_a)
output_b = siamese_network(input_b)

distance = Lambda(lambda x: tf.norm(x[0] - x[1], axis=1), output_shape=(1,))([output_a, output_b])

model = Model(inputs=[input_a, input_b], outputs=distance)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit([X_train[:, 0], X_train[:, 1]], y_train, validation_data=([X_val[:, 0], X_val[:, 1]], y_val), epochs=10, batch_size=32)

# Use the trained model for matching
similarity_scores = model.predict([X_val[:, 0], X_val[:, 1]])

# Threshold the similarity scores to determine matches
threshold = 0.5  # You can adjust this threshold
predictions = (similarity_scores < threshold).astype(int)

# Print the predicted matches
for i in range(len(X_val)):
    print(f"Record 1: {X_val[i, 0]} | Record 2: {X_val[i, 1]} | Predicted Match: {predictions[i]}")











import pandas as pd
import ibm_db  # You may need to use a different DB2 driver if you prefer.

def push_dataframe_to_db2(dataframe, table_name):
    # Replace these with your DB2 credentials
    database = "your_database_name"
    hostname = "your_host_name"
    port = "your_port_number"
    username = "your_username"
    password = "your_password"

    dsn = (
        f"DATABASE={database};"
        f"HOSTNAME={hostname};"
        f"PORT={port};"
        f"PROTOCOL=TCPIP;"
        f"UID={username};"
        f"PWD={password};"
    )

    try:
        conn = ibm_db.connect(dsn, "", "")
        if conn:
            # Create a DataFrame connection
            conn_df = ibm_db_dbi.Connection(conn)

            # Push the DataFrame to the DB2 table
            dataframe.to_sql(table_name, conn_df, if_exists='replace', index=False)
            print(f"Data pushed to table '{table_name}' successfully.")
        else:
            print("DB2 connection not established.")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        if conn:
            ibm_db.close(conn)

# Sample data for testing
data1 = {
    "ID": [1, 2, 3],
    "Name": ["Alice", "Bob", "Charlie"]
}

data2 = {
    "ID": [4, 5, 6],
    "Name": ["David", "Eve", "Frank"]
}

# Create dataframes
dataframe1 = pd.DataFrame(data1)
dataframe2 = pd.DataFrame(data2)

if __name__ == "__main__":
    # Import your dataframes from main.py
    from main import dataframe1, dataframe2

    # Specify the table names in your DB2 database
    table_name1 = "existing_table_name1"
    table_name2 = "existing_table_name2"

    # Push dataframes to DB2 tables
    push_dataframe_to_db2(dataframe1, table_name1)
    push_dataframe_to_db2(dataframe2, table_name2)





def match_bundle(T5, Invoice_table, rules):
    matching_invoices = []
    unmatched_invoices = Invoice_table[:]
    total_invoice_source_amt = 0
    match_rule = None
    rule_index = 0
    
    while rule_index < len(rules) and unmatched_invoices:
        invoices_to_remove = []
        for i in range(len(unmatched_invoices)):
            Invoice = unmatched_invoices[i]
            if rules[rule_index](T5, Invoice):
                invoice_amt = float(Invoice['inv_match_source_amt'])
                remaining_amt = float(T5['fin_source_amt']) - total_invoice_source_amt
                if invoice_amt <= remaining_amt:
                    matching_invoices.append((Invoice, rule_index))
                    total_invoice_source_amt += invoice_amt
                    invoices_to_remove.append(i)
        
        for index in sorted(invoices_to_remove, reverse=True):
            unmatched_invoices.pop(index)
        
        if total_invoice_source_amt == float(T5['fin_source_amt']):
            match_rule = rule_index
            break
        
        rule_index += 1
    
    if total_invoice_source_amt == float(T5['fin_source_amt']):
        return matching_invoices, match_rule
    else:
        return [], unmatched_invoices




def match_bundle(T5, Invoice_table, rules):
    matching_invoices = []
    unmatched_invoices = Invoice_table[:]
    total_invoice_source_amt = 0
    match_rule = None
    rule_index = 0
    
    while rule_index < len(rules) and unmatched_invoices:
        invoices_to_remove = []
        
        for i in range(len(unmatched_invoices)):
            Invoice = unmatched_invoices[i]
            if rules[rule_index](T5, Invoice):  # Call the appropriate rule function
                invoice_amt = float(Invoice['inv_match_source_amt'])
                remaining_amt = float(T5['fin_source_amt']) - total_invoice_source_amt
                
                if invoice_amt <= remaining_amt:
                    matching_invoices.append((Invoice, rule_index))
                    total_invoice_source_amt += invoice_amt
                    invoices_to_remove.append(i)
        
        for index in sorted(invoices_to_remove, reverse=True):
            unmatched_invoices.pop(index)
        
        if total_invoice_source_amt == float(T5['fin_source_amt']):
            match_rule = rule_index
            break
        
        rule_index += 1
    
    if total_invoice_source_amt == float(T5['fin_source_amt']):
        return matching_invoices, match_rule
    else:
        return [], unmatched_invoices



def match_bundle(T5, Invoice_table, rules):
    matching_invoices = []
    unmatched_invoices = Invoice_table[:]
    total_invoice_source_amt = 0
    match_rule = None  # Initialize match_rule outside the loop

    for rule_index, rule in enumerate(rules):
        for i, Invoice in enumerate(unmatched_invoices):
            if rule(T5, Invoice):
                invoice_amt = float(Invoice['inv_match_source_amt'])
                remaining_amt = float(T5['fin_source_amt']) - total_invoice_source_amt
                if invoice_amt <= remaining_amt:
                    matching_invoices.append((Invoice, rule_index))
                    total_invoice_source_amt += invoice_amt
                    unmatched_invoices.pop(i)
                    match_rule = rule_index  # Update match_rule when a match is found
                    break  # Exit inner loop after finding a match

        if total_invoice_source_amt == float(T5['fin_source_amt']):
            break  # Exit outer loop when the target amount is reached

    return matching_invoices, match_rule, unmatched_invoices
















import csv

# Assuming your file is named 'company_comm_mapping.csv'
def read_company_comm_mapping(filename):
    company_comm_mapping = {}
    with open(filename, 'r') as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            company_id, comm_file_id = row
            company_comm_mapping[company_id] = comm_file_id
    return company_comm_mapping

# Read the company and comm file id mappings
company_comm_mapping = read_company_comm_mapping('company_comm_mapping.csv')



def match_bundle(T5, Invoice_table, rules, company_comm_mapping):
    matching_invoices = []
    unmatched_invoices = Invoice_table[:]
    total_invoice_source_amt = 0
    rule_index = 1

    while rule_index <= len(rules) and unmatched_invoices:
        invoices_to_remove = []
        for i in range(len(unmatched_invoices)):
            Invoice = unmatched_invoices[i]
            company_id = Invoice['company_id']
            comm_file_id = Invoice['comm_file_id']

            # Check if the company id and comm file id match the mapping
            if company_id in company_comm_mapping and comm_file_id == company_comm_mapping[company_id]:
                if rules[rule_index - 1](T5, Invoice):
                    invoice_amt = float(Invoice['inv_match_source_amt'])
                    remaining_amt = float(T5['fin_source_amt']) - total_invoice_source_amt
                    if invoice_amt <= remaining_amt:
                        matching_invoices.append((Invoice, rule_index))
                        total_invoice_source_amt += invoice_amt
                        invoices_to_remove.append(i)

        for index in sorted(invoices_to_remove, reverse=True):
            unmatched_invoices.pop(index)

        if total_invoice_source_amt == float(T5['fin_source_amt']):
            match_rule = rule_index
            break

        rule_index += 1

    if total_invoice_source_amt == float(T5['fin_source_amt']):
        return matching_invoices, match_rule
    else:
        return matching_invoices, None, unmatched_invoices




import pandas as pd

# Load the Excel file into a DataFrame
company_comm_mapping = pd.read_excel('company_comm_mapping.xlsx')  # Replace 'company_comm_mapping.xlsx' with your actual file name


def match_bundle(T5, Invoice_table, rules, company_comm_mapping):
    # ... (existing code)

    while rule_index <= len(rules) and unmatched_invoices:
        invoices_to_remove = []
        for i in range(len(unmatched_invoices)):
            Invoice = unmatched_invoices[i]
            company_id = Invoice['company_id']  # Assuming the column name for company ID in Invoice_table is 'company_id'
            comm_file_id = company_comm_mapping.loc[company_comm_mapping['company_id'] == company_id, 'comm_file_id'].values[0]

            if rules[rule_index - 1](T5, Invoice) and comm_file_id == Invoice['comm_file_id']:
                # ... (existing code)

# Assuming you have T5, Invoice_table, and rules defined somewhere before this point
result, match_rule, unmatched_invoices = match_bundle(T5, Invoice_table, rules, company_comm_mapping)

# Process the result, match_rule, and unmatched_invoices as needed











# !pip install --upgrade protobuf==3.20.1
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Lambda,Bidirectional,LSTM
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
#from tensorflow.keras.losses import contrastive_loss
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

def generate_random_string(length):
    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))

# Generate a larger dataset
num_samples = 10000  # Number of samples in the dataset

# Function to create middle match pairs
def generate_middle_match_pairs(num_samples):
    texts_a = [generate_random_string(random.randint(5, 12)) for _ in range(num_samples)]
    texts_b = [generate_random_string(random.randint(5, 12)) for _ in range(num_samples)]
    middle_index = random.randint(2, 8)
    texts_b = [text_b[:middle_index] + text_a[middle_index:] for text_a, text_b in zip(texts_a, texts_b)]
    return list(zip(texts_a, texts_b))

# Function to create right match pairs
def generate_right_match_pairs(num_samples):
    texts_a = [generate_random_string(random.randint(5, 12)) for _ in range(num_samples)]
    texts_b = [generate_random_string(random.randint(5, 12)) for _ in range(num_samples)]
    right_index = random.randint(2, 8)
    texts_b = [text_b[:-right_index] + text_a[-right_index:] for text_a, text_b in zip(texts_a, texts_b)]
    return list(zip(texts_a, texts_b))

# Function to create left match pairs
def generate_left_match_pairs(num_samples):
    texts_a = [generate_random_string(random.randint(5, 12)) for _ in range(num_samples)]
    texts_b = [generate_random_string(random.randint(5, 12)) for _ in range(num_samples)]
    left_index = random.randint(2, 8)
    texts_b = [text_a[:left_index] + text_b[left_index:] for text_a, text_b in zip(texts_a, texts_b)]
    return list(zip(texts_a, texts_b))

# Function to create exact match pairs
def generate_exact_match_pairs(num_samples):
    texts = [generate_random_string(random.randint(5, 12)) for _ in range(num_samples)]
    return list(zip(texts, texts))

# Function to create no match pairs
def generate_no_match_pairs(num_samples):
    texts_a = [generate_random_string(random.randint(5, 12)) for _ in range(num_samples)]
    texts_b = [generate_random_string(random.randint(5, 12)) for _ in range(num_samples)]
    return list(zip(texts_a, texts_b))

# Function to create 60% match pairs
def generate_60_percent_match_pairs(num_samples):
    texts_a = [generate_random_string(random.randint(5, 12)) for _ in range(num_samples)]
    texts_b = [text_a[:int(0.6*len(text_a))] + generate_random_string(int(0.4*len(text_a))) for text_a in texts_a]
    return list(zip(texts_a, texts_b))

# Generate samples for each category
middle_match_pairs = generate_middle_match_pairs(num_samples)
right_match_pairs = generate_right_match_pairs(num_samples)
left_match_pairs = generate_left_match_pairs(num_samples)
exact_match_pairs = generate_exact_match_pairs(num_samples)
no_match_pairs = generate_no_match_pairs(num_samples)
sixty_percent_match_pairs = generate_60_percent_match_pairs(num_samples)

# Combine the samples and labels
pairs = middle_match_pairs + right_match_pairs + left_match_pairs + exact_match_pairs + no_match_pairs + sixty_percent_match_pairs
labels = [1] * num_samples + [1] * num_samples + [1] * num_samples + [1] * num_samples + [0] * num_samples + [1] * num_samples

# Shuffle the pairs and corresponding labels
combined_data = list(zip(pairs, labels))
random.shuffle(combined_data)
pairs, labels = zip(*combined_data)

# Extract texts_a and texts_b from pairs
texts_a, texts_b = zip(*pairs)

# Convert characters to integers
char_set = set(''.join(texts_a + texts_b))
char_to_idx = {char: idx + 1 for idx, char in enumerate(char_set)}  # Add 1 to leave 0 for padding
max_sequence_length = max(max(len(text) for text in texts_a), max(len(text) for text in texts_b))

sequences_a = [[char_to_idx[char] for char in text] for text in texts_a]
sequences_b = [[char_to_idx[char] for char in text] for text in texts_b]

# Pad sequences
sequences_a = pad_sequences(sequences_a, maxlen=max_sequence_length, padding='post')
sequences_b = pad_sequences(sequences_b, maxlen=max_sequence_length, padding='post')


# This code defines a Siamese network for processing text data. It starts with an input layer, applies an embedding layer to 
# convert words into dense vectors, processes the embedded data with a GRU layer, and then uses a dense layer for further processing.

def build_siamese_network(input_shape, embedding_dim):
    input_text = Input(shape=input_shape)
    embedding = Embedding(input_dim=len(char_set) + 1, output_dim=embedding_dim)(input_text)
    lstm = Bidirectional(LSTM(units=64))(embedding)  # Bidirectional LSTM for capturing context from both directions
    output = Dense(64, activation='relu')(lstm)
    model = Model(inputs=input_text, outputs=output)
    return model


# Sets the embedding dimension to 128, determining the size of dense vectors representing words or tokens.
# Creates two Siamese neural networks, siamese_net_a and siamese_net_b, both with embedding dimension 128.
# These networks will process input sequences of maximum length max_sequence_length and map them to fixed-dimensional representations.

embedding_dim = 128  # You can adjust the dimensionality of the character embeddings
siamese_net_a = build_siamese_network((max_sequence_length,), embedding_dim)
siamese_net_b = build_siamese_network((max_sequence_length,), embedding_dim)


# Merge the outputs of the two Siamese networks
merged_output = Lambda(lambda x: tf.abs(x[0] - x[1]))([siamese_net_a.output, siamese_net_b.output])

# Add a dense layer for similarity prediction (0 for dissimilar, 1 for similar)
prediction = Dense(1, activation='sigmoid')(merged_output)

# Create the Siamese-LSTM model
siamese_lstm_model = Model(inputs=[siamese_net_a.input, siamese_net_b.input], outputs=prediction)

# Compile the model with binary cross-entropy loss
siamese_lstm_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

# Ensure that the labels are numpy arrays
labels = np.array(labels)

# Train the Siamese-LSTM model
siamese_lstm_model.fit([sequences_a, sequences_b], labels, epochs=10, batch_size=1)  # Adjust batch_size as needed


# Evaluate the model on new text pairs
new_texts_a = ["A123478", "HG97489","Aravind", "546789","AB12367ch", "ARAVINDSAIF","ABG87Bkjh","12345678"]
new_texts_b = ["A123478", "GF13329","Aravind", "010101","jAB12367ch9", "ARAVINDSAIF","ytre87Bkjh","abchedkj"]

new_sequences_a = [[char_to_idx.get(char, 0) for char in text] for text in new_texts_a]
new_sequences_b = [[char_to_idx.get(char, 0) for char in text] for text in new_texts_b]

new_sequences_a = pad_sequences(new_sequences_a, maxlen=max_sequence_length, padding='post')
new_sequences_b = pad_sequences(new_sequences_b, maxlen=max_sequence_length, padding='post')

predictions = siamese_lstm_model.predict([new_sequences_a, new_sequences_b])

for i in range(len(new_texts_a)):
    similarity_score = predictions[i][0]
    print(f"Similarity between '{new_texts_a[i]}' and '{new_texts_b[i]}': {similarity_score:.2f}")


import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Lambda, Concatenate
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Read unmatched transaction data
unmatched_transactions = pd.read_csv("unmatched_transactions.csv")

# Read unmatched invoice data
unmatched_invoices = pd.read_csv("unmatched_invoices.csv")

# Extract relevant columns
transaction_descriptions = unmatched_transactions['transaction_description'].astype(str).values
invoice_descriptions = unmatched_invoices['invoice_description'].astype(str).values

# Assuming all pairs are dissimilar, assign labels as 0
labels = np.zeros(len(transaction_descriptions))

# Tokenize and pad the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(np.concatenate((transaction_descriptions, invoice_descriptions), axis=0))
vocab_size = len(tokenizer.word_index) + 1
max_sequence_length = max(max(map(len, transaction_descriptions)), max(map(len, invoice_descriptions)))

transaction_descriptions = tokenizer.texts_to_sequences(transaction_descriptions)
invoice_descriptions = tokenizer.texts_to_sequences(invoice_descriptions)

transaction_descriptions = pad_sequences(transaction_descriptions, maxlen=max_sequence_length)
invoice_descriptions = pad_sequences(invoice_descriptions, maxlen=max_sequence_length)

# Function to create Siamese pairs
def create_pairs(transaction_data, invoice_data, labels):
    pairs, labels = [], []
    for i in range(len(transaction_data)):
        pairs.append([transaction_data[i], invoice_data[i]])
        labels.append(labels[i])
        # Negative pair (choose a different random text)
        j = np.random.randint(0, len(transaction_data))
        while labels[j] == labels[i]:
            j = np.random.randint(0, len(transaction_data))
        pairs.append([transaction_data[i], invoice_data[j]])
        labels.append(0)
    return np.array(pairs), np.array(labels)

# Create Siamese pairs from both transaction and invoice data
train_pairs, train_labels = create_pairs(transaction_descriptions, invoice_descriptions, labels)

# Define the Siamese network architecture for text data
def create_siamese_network(vocab_size, max_sequence_length):
    input_layer = Input(shape=(max_sequence_length,))
    embedding_layer = Embedding(input_dim=vocab_size, output_dim=128)(input_layer)
    flatten_layer = Flatten()(embedding_layer)
    dense_layer_1 = Dense(128, activation='relu')(flatten_layer)
    dropout_layer_1 = Dropout(0.5)(dense_layer_1)
    dense_layer_2 = Dense(128, activation='relu')(dropout_layer_1)
    dropout_layer_2 = Dropout(0.5)(dense_layer_2)
    output_layer = Dense(128)(dropout_layer_2)
    return Model(input_layer, output_layer)

# Siamese network definition
base_network = create_siamese_network(vocab_size, max_sequence_length)

input_a = Input(shape=(max_sequence_length,))
input_b = Input(shape=(max_sequence_length,))

# Get the feature vectors from the base network
output_a = base_network(input_a)
output_b = base_network(input_b)

# Calculate the Euclidean distance between the feature vectors
distance = Lambda(lambda x: tf.norm(x[0] - x[1], axis=-1), output_shape=(1,))([output_a, output_b])

# Create the Siamese model
siamese_model = Model(inputs=[input_a, input_b], outputs=distance)

# Loss function for Siamese network
def contrastive_loss(y_true, y_pred, margin=1.0):
    squared_pred = tf.square(y_pred)
    margin_square = tf.square(tf.maximum(margin - y_pred, 0))
    return tf.reduce_mean(y_true * squared_pred + (1 - y_true) * margin_square)

# Compile the Siamese model with the contrastive loss
siamese_model.compile(loss=contrastive_loss, optimizer=Adam(0.0001))

# Train the Siamese network
siamese_model.fit([train_pairs[:, 0], train_pairs[:, 1]], train_labels, epochs=10, batch_size=32)
