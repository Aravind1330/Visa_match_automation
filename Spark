import pyspark.sql.functions as F
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName('Matching').getOrCreate()

# Read the rules from a text file
with open("D:/Users/avasudevan/Desktop/APAC_Matching_automation/Rules.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(row1, row2):
    for i, rule in enumerate(rules):
        if rule(row1, row2):
            return i+1
    return 0
    
# Read the data into Spark DataFrames
T5_df = spark.read.format('csv').option('header', True).option('inferSchema', True).load('D:/Users/avasudevan/Desktop/APAC_Matching_automation/Sample1.xlsx')
Invoice_df = spark.read.format('csv').option('header', True).option('inferSchema', True).load('D:/Users/avasudevan/Desktop/APAC_Matching_automation/Sample2.xlsx')

# Add a column to T5_df for each rule, indicating whether the row matches the rule
for i, rule in enumerate(rules):
    udf_match_row = F.udf(lambda row: match_row(row, Invoice_df.collect()), returnType=IntegerType())
    T5_df = T5_df.withColumn(f'rule_{i+1}', udf_match_row(F.struct([T5_df[col] for col in T5_df.columns])))
    


# Join T5_df and Invoice_df on the rule columns
join_cols = [f'rule_{i+1}' for i in range(len(rules))]
joined_df = T5_df.join(Invoice_df, join_cols)

# Filter out rows where no rule matches
matched_df = joined_df.filter(F.col('rule_1') + F.col('rule_2') > 0)

# Add columns for the rule index and description to the matched DataFrame
udf_rule_description = F.udf(lambda rule_index: rule_descriptions.get(rule_index, 'Unmatched'), returnType=StringType())
matched_df = matched_df.withColumn('Match Rule', F.greatest(*join_cols)).withColumn('description', udf_rule_description(F.greatest(*join_cols)))

# Select the columns we want in the output DataFrame
output_df = matched_df.select('Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no')

# Write the output data to an Excel file
output_df.toPandas().to_excel('Z:/Desktop/output2.xlsx', index=False)

# Aggregate the output data by Company ID & Name and Match Rule
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').count().withColumnRenamed('count', 'TRF')
total = aggregated.agg(F.sum('TRF')).collect()[0][0]
aggregated = aggregated.union(spark.createDataFrame([('Total', '-', total)], ['Company ID & Name', 'Match Rule', 'TRF']))

# Write the aggregated data to a separate sheet in the output file
aggregated.toPandas().to_excel('Z:/Desktop/output2.xlsx', sheet_name='Aggregated', index=False)    
   
    





from pyspark.sql.functions import expr
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType

# Create a SparkSession
spark = SparkSession.builder.appName('Rule Matching').getOrCreate()

# Define the schema for the input tables
T5_schema = StructType([
    StructField('Company ID & Name', StringType(), True),
    StructField('fin_orig_supplier_nm', StringType(), True),
    StructField('fin_source_amt', StringType(), True),
    StructField('Ticket_no', StringType(), True),
    StructField('Acct_no', StringType(), True),
    StructField('Amount', StringType(), True),
    StructField('Currency', StringType(), True),
    StructField('Date', StringType(), True),
    StructField('C&D indicator', StringType(), True),
    StructField('RPIC', StringType(), True)
])

Invoice_schema = StructType([
    StructField('Invoice ID', StringType(), True),
    StructField('inv_match_source_amt', StringType(), True),
    StructField('inv_erp_vend_no', StringType(), True),
    StructField('inv_po_no', StringType(), True)
])

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = expr(f"CASE WHEN {rule_string} THEN 1 ELSE 0 END")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice, used_invoice_ids):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            # Check if the invoice ID has already been used
            if Invoice['Invoice ID'] not in used_invoice_ids:
                return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as Spark DataFrames
T5_df = spark.read.format("excel").option("header", "true").option("inferSchema", "false").schema(T5_schema).load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_df = spark.read.format("excel").option("header", "true").option("inferSchema", "false").schema(Invoice_schema).load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty list for the output data
output_data = []

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Create a list of used invoice IDs
used_invoice_ids = []

# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.rdd.toLocalIterator():
    matched = False
    for Invoice_row in Invoice_df.rdd.toLocalIterator():
        matched, rule_index = match_row(T5_row, Invoice_row, used_invoice_ids)
        if matched:
            description = rule_descriptions[rule_index]
            output_data.append({'Company ID & Name': T5_row['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5_row['fin_orig_supplier_nm'], 'fin_source_amt': T5_row['fin_source_amt'],


        'Ticket_no': T5_row['Ticket_no'], 'Acct_no': T5_row['Acct_no'], 'Amount': T5_row['Amount'], 'Currency': T5_row['Currency'], 'Date': T5_row['Date'], 'C&D indicator': T5_row['C&D indicator'], 'RPIC': T5_row['RPIC'], 'Invoice ID': Invoice_row['Invoice ID'], 'inv_match_source_amt': Invoice_row['inv_match_source_amt'], 'inv_erp_vend_no': Invoice_row['inv_erp_vend_no'], 'inv_po_no': Invoice_row['inv_po_no']})
        used_invoice_ids.append(Invoice_row['Invoice ID'])
        break
output_df = spark.createDataFrame(output_data)
output_df.write.format("excel").option("header", "true").save("Z:/Downloads/Output_Data_1.xlsx")
