import pyspark.sql.functions as F
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName('Matching').getOrCreate()

# Read the rules from a text file
with open("D:/Users/avasudevan/Desktop/APAC_Matching_automation/Rules.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(row1, row2):
    for i, rule in enumerate(rules):
        if rule(row1, row2):
            return i+1
    return 0
    
# Read the data into Spark DataFrames
T5_df = spark.read.format('csv').option('header', True).option('inferSchema', True).load('D:/Users/avasudevan/Desktop/APAC_Matching_automation/Sample1.xlsx')
Invoice_df = spark.read.format('csv').option('header', True).option('inferSchema', True).load('D:/Users/avasudevan/Desktop/APAC_Matching_automation/Sample2.xlsx')

# Add a column to T5_df for each rule, indicating whether the row matches the rule
for i, rule in enumerate(rules):
    udf_match_row = F.udf(lambda row: match_row(row, Invoice_df.collect()), returnType=IntegerType())
    T5_df = T5_df.withColumn(f'rule_{i+1}', udf_match_row(F.struct([T5_df[col] for col in T5_df.columns])))
    


# Join T5_df and Invoice_df on the rule columns
join_cols = [f'rule_{i+1}' for i in range(len(rules))]
joined_df = T5_df.join(Invoice_df, join_cols)

# Filter out rows where no rule matches
matched_df = joined_df.filter(F.col('rule_1') + F.col('rule_2') > 0)

# Add columns for the rule index and description to the matched DataFrame
udf_rule_description = F.udf(lambda rule_index: rule_descriptions.get(rule_index, 'Unmatched'), returnType=StringType())
matched_df = matched_df.withColumn('Match Rule', F.greatest(*join_cols)).withColumn('description', udf_rule_description(F.greatest(*join_cols)))

# Select the columns we want in the output DataFrame
output_df = matched_df.select('Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no')

# Write the output data to an Excel file
output_df.toPandas().to_excel('Z:/Desktop/output2.xlsx', index=False)

# Aggregate the output data by Company ID & Name and Match Rule
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').count().withColumnRenamed('count', 'TRF')
total = aggregated.agg(F.sum('TRF')).collect()[0][0]
aggregated = aggregated.union(spark.createDataFrame([('Total', '-', total)], ['Company ID & Name', 'Match Rule', 'TRF']))

# Write the aggregated data to a separate sheet in the output file
aggregated.toPandas().to_excel('Z:/Desktop/output2.xlsx', sheet_name='Aggregated', index=False)    
   
    





from pyspark.sql.functions import expr
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType

# Create a SparkSession
spark = SparkSession.builder.appName('Rule Matching').getOrCreate()

# Define the schema for the input tables
T5_schema = StructType([
    StructField('Company ID & Name', StringType(), True),
    StructField('fin_orig_supplier_nm', StringType(), True),
    StructField('fin_source_amt', StringType(), True),
    StructField('Ticket_no', StringType(), True),
    StructField('Acct_no', StringType(), True),
    StructField('Amount', StringType(), True),
    StructField('Currency', StringType(), True),
    StructField('Date', StringType(), True),
    StructField('C&D indicator', StringType(), True),
    StructField('RPIC', StringType(), True)
])

Invoice_schema = StructType([
    StructField('Invoice ID', StringType(), True),
    StructField('inv_match_source_amt', StringType(), True),
    StructField('inv_erp_vend_no', StringType(), True),
    StructField('inv_po_no', StringType(), True)
])

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = expr(f"CASE WHEN {rule_string} THEN 1 ELSE 0 END")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice, used_invoice_ids):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            # Check if the invoice ID has already been used
            if Invoice['Invoice ID'] not in used_invoice_ids:
                return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as Spark DataFrames
T5_df = spark.read.format("excel").option("header", "true").option("inferSchema", "false").schema(T5_schema).load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_df = spark.read.format("excel").option("header", "true").option("inferSchema", "false").schema(Invoice_schema).load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty list for the output data
output_data = []

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Create a list of used invoice IDs
used_invoice_ids = []

# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.rdd.toLocalIterator():
    matched = False
    for Invoice_row in Invoice_df.rdd.toLocalIterator():
        matched, rule_index = match_row(T5_row, Invoice_row, used_invoice_ids)
        if matched:
            description = rule_descriptions[rule_index]
            output_data.append({'Company ID & Name': T5_row['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5_row['fin_orig_supplier_nm'], 'fin_source_amt': T5_row['fin_source_amt'],


        'Ticket_no': T5_row['Ticket_no'], 'Acct_no': T5_row['Acct_no'], 'Amount': T5_row['Amount'], 'Currency': T5_row['Currency'], 'Date': T5_row['Date'], 'C&D indicator': T5_row['C&D indicator'], 'RPIC': T5_row['RPIC'], 'Invoice ID': Invoice_row['Invoice ID'], 'inv_match_source_amt': Invoice_row['inv_match_source_amt'], 'inv_erp_vend_no': Invoice_row['inv_erp_vend_no'], 'inv_po_no': Invoice_row['inv_po_no']})
        used_invoice_ids.append(Invoice_row['Invoice ID'])
        break
output_df = spark.createDataFrame(output_data)
output_df.write.format("excel").option("header", "true").save("Z:/Downloads/Output_Data_1.xlsx")









from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType, StringType, IntegerType, StructType, StructField
import os

# Initialize a Spark session
spark = SparkSession.builder.appName("Invoice Matching").getOrCreate()

# Define the schema for T5 and Invoice dataframes
t5_schema = StructType([
    StructField("Company ID & Name", StringType(), True),
    StructField("fin_orig_supplier_nm", StringType(), True),
    StructField("fin_source_amt", StringType(), True),
    StructField("Ticket_no", StringType(), True),
    StructField("Acct_no", StringType(), True),
    StructField("Amount", StringType(), True),
    StructField("Currency", StringType(), True),
    StructField("Date", StringType(), True),
    StructField("C&D indicator", StringType(), True),
    StructField("RPIC", StringType(), True)
])

invoice_schema = StructType([
    StructField("inv_erp_vend_no", StringType(), True),
    StructField("inv_po_no", StringType(), True),
    StructField("inv_match_source_amt", StringType(), True),
    StructField("Invoice ID", StringType(), True)
])

# Define the function to apply the rules to each row pair
def match_row(t5_row, invoice_row, used_invoice_ids):
    for i, rule in enumerate(rules):
        if rule(t5_row, invoice_row):
            # Check if the invoice ID has already been used
            if invoice_row['Invoice ID'] not in used_invoice_ids:
                return True, i+1
    return False, "Unmatched"

# Register the match_row function as a UDF
match_row_udf = udf(match_row, returnType=StructType([
    StructField("matched", BooleanType()),
    StructField("rule_index", StringType())
]))

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)

# Read in the two Excel sheets as dataframes
t5_df = spark.read.format("com.crealytics.spark.excel") \
            .option("header", "true") \
            .option("treatEmptyValuesAsNulls", "true") \
            .option("inferSchema", "false") \
            .option("schema", t5_schema.json()) \
            .load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")

invoice_df = spark.read.format("com.crealytics.spark.excel") \
            .option("header", "true") \
            .option("treatEmptyValuesAsNulls", "true") \
            .option("inferSchema", "false") \
            .option("schema", invoice_schema.json()) \
            .load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty list for the output data
output_data = []

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Create a list of used invoice IDs
used_invoice_ids

matched_rows_df = t5_df.crossJoin(invoice_df)
.withColumn("match_result", match_row_udf(t5_df.colRegex(".*"), invoice_df.colRegex(".*"), used_invoice_ids))
Filter for only the matched rows

matched_rows_df = matched_rows_df.filter(matched_rows_df['match_result']['matched'])
Loop through the matched rows and add them to the output data list

for row in matched_rows_df.collect():
output_data.append({
'Company ID & Name': row['Company ID & Name'],
'fin_orig_supplier_nm': row['fin_orig_supplier_nm'],
'fin_source_amt': row['fin_source_amt'],
'Ticket_no': row['Ticket_no'],
'Acct_no': row['Acct_no'],
'Amount': row['Amount'],
'Currency': row['Currency'],
'Date': row['Date'],
'C&D indicator': row['C&D indicator'],
'RPIC': row['RPIC'],
'inv_erp_vend_no': row['inv_erp_vend_no'],
'inv_po_no': row['inv_po_no'],
'inv_match_source_amt': row['inv_match_source_amt'],
'Invoice ID': row['Invoice ID'],
'rule': rule_descriptions[int(row['match_result']['rule_index'])]
})
Create a dataframe from the output data list

output_df = spark.createDataFrame(output_data)
Write the output dataframe to a CSV file

output_df.coalesce(1).write.csv("Z:/Desktop/Invoice_Matching_Output", header=True, mode="overwrite")
Stop the Spark session

spark.stop()






from pyspark.sql.functions import col, lit, when
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("Rule Matching").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice, used_invoice_ids):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            # Check if the invoice ID has already been used
            if Invoice['Invoice ID'] not in used_invoice_ids:
                return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as DataFrames
T5_df = spark.read.format("com.crealytics.spark.excel") \
            .option("dataAddress", "'Sheet1'!") \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .load("Z:/Downloads/Unmatched_trans_Data_1.xlsx") \
            .select([col(c).cast("string") for c in ["Company ID & Name", "fin_orig_supplier_nm", "fin_source_amt"]])

Invoice_df = spark.read.format("com.crealytics.spark.excel") \
                .option("dataAddress", "'Sheet1'!") \
                .option("header", "true") \
                .option("inferSchema", "true") \
                .load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx") \
                .select([col(c).cast("string") for c in ["Invoice ID", "inv_match_source_amt", "inv_erp_vend_no", "inv_po_no"]])

# Create an empty list for the output data
output_data = []

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Create a list of used invoice IDs
used_invoice_ids = []

# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.collect():
    matched = False
    for Invoice_row in Invoice_df.collect():
        matched, rule_index = match_row(T5_row.asDict(), Invoice_row.asDict(), used_invoice_ids)
        if matched:
            description = rule_descriptions[rule_index]
            output_data.append({'Company ID & Name': T5_row['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5_row['fin_orig_supplier_nm'], 'fin_source_amt': T5_row['fin_source_amt'], 'inv_match_source_amt': Invoice_row['inv_match_source_amt'], 'inv_erp_vend_no': Invoice_row['inv_erp_vend_no'], 'inv_po_no': Invoice_row['inv_po_no']})
            used_invoice_ids.append(Invoice_row['Invoice ID'])
            break

# Convert output_data to a DataFrame
output_df = spark.createDataFrame(output_data)

# Write the output data to an Excel file
output_df.select(col('Company
ID & Name').alias('Company ID and Name'), col('Match Rule'), col('description'), col('fin_orig_supplier_nm').alias('Financial Orig. Supplier Name'), col('fin_source_amt').alias('Financial Source Amount'), col('inv_match_source_amt').alias('Invoice Matched Source Amount'), col('inv_erp_vend_no').alias('Invoice ERP Vendor Number'), col('inv_po_no').alias('Invoice PO Number'))
.write.format('com.crealytics.spark.excel')
.option('header', 'true')
.option('dataAddress', "'Sheet1'!")
.mode('overwrite')
.save('Z:/Downloads/Matched_Data_1.xlsx')
Stop SparkSession

spark.stop()









from pyspark.sql.functions import col

def match_row(T5, Invoice, used_invoice_ids):
    for i, rule in enumerate(rules):
        # Join T5 and Invoice dataframes on common column
        joined_df = T5.join(Invoice, T5.fin_orig_supplier_nm[-13:] == Invoice.inv_ticket_num[-13:], 'inner')
        
        # Apply the rule function to the joined dataframe
        matched_df = joined_df.filter(rule(col('T5'), col('Invoice')))
        
        # Check if any rows matched the rule
        if matched_df.count() > 0:
            # Check if the invoice ID has already been used
            if Invoice['Invoice ID'] not in used_invoice_ids:
                return True, i+1
    return False, "Unmatched"




from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("InvoiceMatching").getOrCreate()

# Read in the two Excel sheets as Spark dataframes
T5_table = spark.read.format("com.crealytics.spark.excel").option("header", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_table = spark.read.format("com.crealytics.spark.excel").option("header", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Convert data types to string
T5_table = T5_table.select([col(c).cast("string") for c in T5_table.columns])
Invoice_table = Invoice_table.select([col(c).cast("string") for c in Invoice_table.columns])








import pyspark.sql.functions as F
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("matching").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice, used_invoice_ids):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            # Check if the invoice ID has already been used
            if Invoice['Invoice ID'] not in used_invoice_ids:
                return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as Spark DataFrames
T5_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty list for the output data
output_data = []

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Create a list of used invoice IDs
used_invoice_ids = []

# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.rdd.collect():
    matched = False
    for Invoice_row in Invoice_df.rdd.collect():
        matched, rule_index = match_row(T5_row, Invoice_row, used_invoice_ids)
        if matched:
            description = rule_descriptions[rule_index]
            output_data.append({'Company ID & Name': T5_row['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5_row['fin_orig_supplier_nm'], 'fin_source_amt': T5_row['fin_source_amt'], 'inv_match_source_amt': Invoice_row['inv_match_source_amt'], 'inv_erp_vend_no': Invoice_row['inv_erp_vend_no'], 'inv_po_no': Invoice_row['inv_po_no']})
            used_invoice_ids.append(Invoice_row['Invoice ID'])
            break

# Create a Spark DataFrame from the output data
output_df = spark.createDataFrame(output_data)

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no']
output_df.select(output_columns).write.format("excel").option("header", "true").option("dateFormat", "yyyy-mm-dd").option("timestampFormat", "yyyy-mm-dd hh:mm:ss").save("Z:/Desktop/output2.xlsx")

# Calculate the total number of matches for each company and rule
aggregated = output_df.groupBy(['Company ID & Name', 'Match Rule']).agg(F.count('*').alias('Match Count'))
Write the aggregated data to an Excel file

aggregated.select(['Company ID & Name', 'Match Rule', 'Match Count']).write.format("excel").option("header", "true").option("dateFormat", "yyyy-mm-dd").option("timestampFormat", "yyyy-mm-dd hh:mm:ss").save("Z:/Desktop/aggregated_output2.xlsx")



T5_df = T5_df.withColumn("fin_orig_supplier_nm", F.col("fin_orig_supplier_nm").cast("string"))





# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice, used_invoice_ids):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            # Check if the invoice ID has already been used
            if Invoice['Invoice ID'] not in used_invoice_ids:
                return True, i+1
    return False, None  # return None instead of "Unmatched"

# ...

# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.rdd.collect():
    matched = False
    for Invoice_row in Invoice_df.rdd.collect():
        matched, rule_index = match_row(T5_row, Invoice_row, used_invoice_ids)
        if matched:
            description = rule_descriptions[rule_index]
            invoice_id = Invoice_row['Invoice ID']
            if invoice_id is not None:  # check if Invoice ID is None
                output_data.append({'Company ID & Name': T5_row['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5_row['fin_orig_supplier_nm'], 'fin_source_amt': T5_row['fin_source_amt'], 'inv_match_source_amt': Invoice_row['inv_match_source_amt'], 'inv_erp_vend_no': Invoice_row['inv_erp_vend_no'], 'inv_po_no': Invoice_row['inv_po_no'], 'Invoice ID': invoice_id})
                used_invoice_ids.append(invoice_id)
            break


















# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.rdd.collect():
    matched = False
    for Invoice_row in Invoice_df.rdd.collect():
        matched, rule_index = match_row(T5_row, Invoice_row, used_invoice_ids)
        if matched:
            description = rule_descriptions[rule_index]
            invoice_id = Invoice_row['Invoice ID']
            if invoice_id is not None:  # check if Invoice ID is None
                output_data.append({'Company ID & Name': T5_row['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5_row['fin_orig_supplier_nm'], 'fin_source_amt': T5_row['fin_source_amt'], 'inv_match_source_amt': Invoice_row['inv_match_source_amt'], 'inv_erp_vend_no': Invoice_row['inv_erp_vend_no'], 'inv_po_no': Invoice_row['inv_po_no'], 'Invoice ID': invoice_id})
                used_invoice_ids.append(invoice_id)
            break













from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

# Create a SparkSession
spark = SparkSession.builder.appName("MatchingData").getOrCreate()

# Read the rules from a text file
rule_strings = spark.read.text("Z:/Desktop/Rules_1.txt").rdd.map(lambda row: row[0]).collect()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as dataframes
T5_table = spark.read.format("excel").option("header", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_table = spark.read.format("excel").option("header", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty list for the output data
output_data = []

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Loop through each row in table1 and find a match in table2
for T5 in T5_table.collect():
    matched = False
    for Invoice in Invoice_table.collect():
        matched, rule_index = match_row(T5, Invoice)
        if matched:
            description = rule_descriptions[rule_index]
            output_data.append({'Company ID & Name': T5['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5['fin_orig_supplier_nm'], 'fin_source_amt': T5['fin_source_amt'], 'inv_match_source_amt': Invoice['inv_match_source_amt'], 'inv_erp_vend_no': Invoice['inv_erp_vend_no'], 'inv_po_no': Invoice['inv_po_no']})
            break

# Convert the output data to a Spark dataframe
output_df = spark.createDataFrame(output_data)

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no']
output_df = output_df.select([col(c) for c in output_columns])
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').agg({'Match Rule': 'count'}).withColumnRenamed('count(Match Rule)', 'TRF')
total = aggregated.agg({"TRF": "sum"}).collect()[0][0]
aggregated = aggregated.withColumn('Company ID & Name', lit('-')).withColumn('description', lit('-')).withColumn('TRF', lit(total)).union(aggregated)

output_df.write.format("excel").option("header", "true").mode("overwrite").save("Z:/Desktop/output2.xlsx", sheet_name="Output")
aggregated.write





from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Initialize a SparkSession object
spark = SparkSession.builder.appName("Matching").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as dataframes
T5_df = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_df = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Define a UDF to apply the match_row function to each row pair
match_row_udf = udf(lambda T5, Invoice: match_row(T5, Invoice), StructType([StructField("matched", BooleanType()), StructField("rule_index", StringType())]))

# Join the two dataframes and apply the match_row UDF
joined_df = T5_df.crossJoin(Invoice_df).withColumn("matched_rule", match_row_udf(struct(col("T5_df.*")), struct(col("Invoice_df.*"))))

# Extract the matched rows and add rule descriptions
output_df = joined_df.filter(col("matched_rule.matched") == True).select(col("T5_df.Company ID & Name"), col("matched_rule.rule_index"), when(col("matched_rule.rule_index") == 1, "Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same").when(col("matched_rule.rule_index") == 2, "Ticket_no_right_of_12").alias("description"), col("T5_df.fin_orig_supplier_nm"), col("T5_df.fin_source_amt"), col("Invoice_df.inv_match_source_amt"), col("Invoice_df.inv_erp_vend_no"), col("Invoice_df.inv_po_no"))

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no']
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').agg(count('*').alias('TRF')).withColumn('Match Rule', when(col('Match Rule') == 1, 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same').when(col('Match Rule') == 2, 'Ticket_no_right_of_12')).orderBy('Company ID & Name', 'Match Rule')
total = aggregated.select(sum('TRF')).collect()[0][0]
aggregated = aggregated.union(spark.createDataFrame([('Total',









def match_row(T5_cols, Invoice_cols):
    T5 = tuple(T5_cols)
    Invoice = tuple(Invoice_cols)
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Define a UDF to apply the match_row function to each row pair
match_row_udf = udf(lambda T5_cols, Invoice_cols: match_row(T5_cols, Invoice_cols), StructType([StructField("matched", BooleanType()), StructField("rule_index", StringType())]))

# Join the two dataframes and apply the match_row UDF
joined_df = T5_df.crossJoin(Invoice_df).withColumn("matched_rule", match_row_udf(array([col("T5_df."+c) for c in T5_df.columns]), array([col("Invoice_df."+c) for c in Invoice_df.columns])))

# Extract the matched rows and add rule descriptions
output_df = joined_df.filter(col("matched_rule.matched") == True).select(col("T5_df.Company ID & Name"), col("matched_rule.rule_index"), when(col("matched_rule.rule_index") == 1, "Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same").when(col("matched_rule.rule_index") == 2, "Ticket_no_right_of_12").alias("description"), col("T5_df.fin_orig_supplier_nm"), col("T5_df.fin_source_amt"), col("Invoice_df.inv_match_source_amt"), col("Invoice_df.inv_erp_vend_no"), col("Invoice_df.inv_po_no"))

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no']
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').agg












# Define a UDF to apply the match_row function to each row pair
match_row_udf = udf(lambda T5_cols, Invoice_cols: match_row(T5_cols, Invoice_cols), StructType([StructField("matched", BooleanType()), StructField("rule_index", StringType())]))

# Join the two dataframes and apply the match_row UDF
joined_df = T5_df.crossJoin(Invoice_df).withColumn("matched_rule", match_row_udf(array([col("T5_df."+c) for c in T5_df.columns]), array([col("Invoice_df."+c) for c in Invoice_df.columns])))

# Extract the matched rows and add rule descriptions
output_df = joined_df.filter(col("matched_rule.matched") == True).select(col("T5_df.Company ID & Name"), col("matched_rule.rule_index"), when(col("matched_rule.rule_index") == 1, "Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same").when(col("matched_rule.rule_index") == 2, "Ticket_no_right_of_12").alias("description"), col("T5_df.fin_orig_supplier_nm"), col("T5_df.fin_source_amt"), col("Invoice_df.inv_match_source_amt"), col("Invoice_df.inv_erp_vend_no"), col("Invoice_df.inv_po_no"))

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no']
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').agg
output_df.write.format("com.databricks.spark.csv").

