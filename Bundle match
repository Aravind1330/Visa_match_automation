# Define the bundle match rule
def bundle_match(row, rows):
    # Get the transaction amount
    transaction_amt = row['fin_source_amt']
    
    # Filter the rows to include only the ones with the same auth/ticket number
    matching_rows = rows[(rows['inv_auth_no'] == row['inv_auth_no']) | (rows['inv_ticket_num'] == row['inv_ticket_num'])]
    
    # Calculate the sum of the invoice amounts
    invoice_amt_sum = matching_rows['inv_match_source_amt'].sum()
    
    # Check if the sum of invoice amounts matches the transaction amount
    return invoice_amt_sum == transaction_amt

# Define the output columns
output_columns = ['Company ID & Name', 'Match Rule', 'Description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no', 'inv_auth_no', 'inv_ticket_num']

# Define a list to store the output rows
output_rows = []

# Keep track of matched invoices
matched_invoices = []

# Filter T5 dataframe to remove rows with null 'fin_source_amt' column
T5_table = T5_table.dropna(subset=['fin_source_amt'])

# Loop through each row in table1 and find a match in table2
for T5 in T5_table:
    try:
        # Filter the invoice table to include only the ones that have not been matched
        unmatched_invoices = Invoice_table[~Invoice_table.index.isin(matched_invoices)]
        
        # Group the invoices by auth/ticket number and apply the bundle match rule
        groups = unmatched_invoices.groupby(['inv_auth_no', 'inv_ticket_num'])
        for _, group in groups:
            if bundle_match(T5, group):
                output_rows.append({
                    column: T5[column] if column in T5 else group.iloc[0][column] for column in output_columns
                })
                output_rows[-1]['Match Rule'] = 'Bundle Match'
                output_rows[-1]['Description'] = 'Bundle of invoices matched transaction'
                # Add the matched invoice indexes to the list
                matched_invoices.extend(group.index)
    except:
        # print (f"Error processing T5: {T5}")
        error_messages.append(f"Error processing T5: {T5}")
        continue

# Create a DataFrame from the output rows and aggregate the data
output_df = pd.DataFrame(output_rows, columns=output_columns)
aggregated = output_df.groupby(['Company ID & Name', 'Match Rule']).size().reset_index(name='TRF')
total = aggregated['TRF'].sum()
aggregated = aggregated.append(pd.Series(['Total', '-', total], index=aggregated.columns), ignore_index=True)

# Write the output data to a CSV file
output_df.to_csv('Z:/Desktop/output2.csv', index=False)
with pd.ExcelWriter(config['output_file']) as writer:
    output_df.to_excel(writer, sheet_name='Output', index=False)










import pandas as pd

rules_df = pd.read_csv('Z:/Desktop/Rules.csv')
rules = []
rule_descriptions = {}
for i, row in rules_df.iterrows():
    rule_string = row['Rule']
    rule_description = row['Description']
    rule = eval(f"lambda row, row2: {rule_string}")
    rules.append(rule)
    rule_descriptions[i+1] = rule_description

# Define the output columns
output_columns = ['Company ID & Name', 'Match Rule', 'Description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no']

# Read the previously unmatched transactions if available
try:
    unmatched_output_df = pd.read_csv('Z:/Desktop/unmatched_output.csv')
except FileNotFoundError:
    unmatched_output_df = pd.DataFrame(columns=output_columns)

# Keep track of matched invoices
matched_invoices = []

# Filter T5 dataframe to remove rows with null 'fin_source_amt' column
T5_table = T5_table.dropna(subset=['fin_source_amt'])

# Loop through each row in T5_table and find a match in Invoice_table
for T5_idx, T5_row in T5_table.iterrows():
    try:
        matched = False
        for Invoice_idx, Invoice_row in Invoice_table.iterrows():
            matched, rule_index = match_row(T5_row, Invoice_row)
            if matched:
                description = rule_descriptions[rule_index]
                matched_output = {
                    column: T5_row[column] if column in T5_row else Invoice_row[column] for column in output_columns
                }
                matched_output['Match Rule'] = rule_index
                matched_output['Description'] = description
                matched_invoices.append(Invoice_idx)  # Keep track of matched invoices
                break
        if not matched:
            unmatched_output = {
                column: T5_row[column] for column in output_columns
            }
            unmatched_output['Match Rule'] = 'N/A'
            unmatched_output['Description'] = 'No match found'
            unmatched_output_df = unmatched_output_df.append(unmatched_output, ignore_index=True)
    except Exception as e:
        error_messages.append(f"Error processing T5 at index {T5_idx}: {str(e)}")
        continue

# Remove previously matched transactions from unmatched_output_df
unmatched_output_df = unmatched_output_df.drop(matched_invoices)

# Create DataFrames from the output rows for matched transactions
matched_output_df = pd.DataFrame(columns=output_columns)  # Create an empty DataFrame for matched transactions

# Aggregate the data
matched_aggregated = matched_output_df.groupby(['Company ID & Name', 'Match Rule']).size().reset_index(name='TRF')
matched_total = matched_aggregated['TRF'].sum()
matched_aggregated = matched_aggregated.append(pd.Series(['Total', '-', matched_total], index=matched_aggregated.columns), ignore_index=True)

# Write the output data to separate CSV files
matched_output_df.to_csv('Z:/Desktop/matched_output.csv', index=False)
unmatched_output_df.to_csv('Z:/Desktop/unmatched_output.csv', index=False)
matched_aggregated.to_csv('Z:/Desktop/matched_aggregated.csv', index=False)




import pandas as pd

def load_rules():
    rules_df = pd.read_csv('Z:/Desktop/Rules.csv')
    rules = []
    rule_descriptions = {}
    for i, row in rules_df.iterrows():
        rule_string = row['Rule']
        rule_description = row['Description']
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)
        rule_descriptions[i+1] = rule_description
    return rules, rule_descriptions



import pandas as pd
from rules_functions import load_rules

# Load rules_df, rules, and rule_descriptions
rules, rule_descriptions = load_rules()

# Define the output columns
output_columns = ['Company ID & Name', 'Match Rule', 'Description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no']

# Define lists to store the output rows for matched and unmatched transactions
matched_output_rows = []
unmatched_output_rows = []

# Rest of your code...
import pandas as pd

import pandas as pd

def load_rules(file_path):
    rules_df = pd.read_csv(file_path)
    rules = []
    rule_descriptions = {}
    for i, row in rules_df.iterrows():
        rule_string = row['Rule']
        rule_description = row['Description']
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)
        rule_descriptions[i+1] = rule_description
    return rules, rule_descriptions

from rules_loader import load_rules
# Define the path to the rules CSV file
rules_file_path = 'Z:/Desktop/Rules.csv'

# Load the rules using the load_rules() function
rules, rule_descriptions = load_rules(rules_file_path)




# Read the rules from the CSV file into a PySpark DataFrame
rules_df = spark.read.csv('Z:/Desktop/Rules.csv', header=True, inferSchema=True)

# Create empty lists to store rules and rule descriptions
rules = []
rule_descriptions = {}

# Iterate over the rows of the rules DataFrame
for row in rules_df.rdd.collect():
    rule_string = row['Rule']
    rule_description = row['Description']
    rule = eval(f"lambda row, row2: {rule_string}")
    rules.append(rule)
    rule_descriptions[row['id']] = rule_description



from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Read the T5 table from Excel using the config file
T5_table = spark.read.format('com.crealytics.spark.excel') \
    .option('header', 'true') \
    .option('dataAddress', 'T5') \
    .option('treatEmptyValuesAsNulls', 'true') \
    .option('inferSchema', 'true') \
    .load(config['input_files']['T5-file'])

# Drop duplicates based on 'fin_record_key'
T5_table = T5_table.dropDuplicates(['fin_record_key'])

# Filter rows based on 'fin_region_id'
T5_table = T5_table.filter(col('fin_region_id').isin(['1', '2', '3']))

# Filter rows based on 'fin_merch_catg_cd'
T5_table = T5_table.filter(col('fin_merch_catg_cd').isin(['3000', '3001', '3002', '3003']))

# Filter rows based on date using a custom function
def filter_date(date_column, days):
    return col(date_column) >= current_date() - expr(f'interval {days} days')

T5_table = T5_table.filter(filter_date('fin_posting_dt', 7))

# Drop rows with null values in specified columns
T5_table = T5_table.dropna(subset=['fin_source_amt', 'fin_orig_supplier_nm'])

# Convert DataFrame to a list of dictionaries (orient='records')
T5_data = T5_table.collect()
T5_records = [row.asDict() for row in T5_data]

# Rest of your code...
