# Define the bundle match rule
def bundle_match(row, rows):
    # Get the transaction amount
    transaction_amt = row['fin_source_amt']
    
    # Filter the rows to include only the ones with the same auth/ticket number
    matching_rows = rows[(rows['inv_auth_no'] == row['inv_auth_no']) | (rows['inv_ticket_num'] == row['inv_ticket_num'])]
    
    # Calculate the sum of the invoice amounts
    invoice_amt_sum = matching_rows['inv_match_source_amt'].sum()
    
    # Check if the sum of invoice amounts matches the transaction amount
    return invoice_amt_sum == transaction_amt

# Define the output columns
output_columns = ['Company ID & Name', 'Match Rule', 'Description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no', 'inv_auth_no', 'inv_ticket_num']

# Define a list to store the output rows
output_rows = []

# Keep track of matched invoices
matched_invoices = []

# Filter T5 dataframe to remove rows with null 'fin_source_amt' column
T5_table = T5_table.dropna(subset=['fin_source_amt'])

# Loop through each row in table1 and find a match in table2
for T5 in T5_table:
    try:
        # Filter the invoice table to include only the ones that have not been matched
        unmatched_invoices = Invoice_table[~Invoice_table.index.isin(matched_invoices)]
        
        # Group the invoices by auth/ticket number and apply the bundle match rule
        groups = unmatched_invoices.groupby(['inv_auth_no', 'inv_ticket_num'])
        for _, group in groups:
            if bundle_match(T5, group):
                output_rows.append({
                    column: T5[column] if column in T5 else group.iloc[0][column] for column in output_columns
                })
                output_rows[-1]['Match Rule'] = 'Bundle Match'
                output_rows[-1]['Description'] = 'Bundle of invoices matched transaction'
                # Add the matched invoice indexes to the list
                matched_invoices.extend(group.index)
    except:
        # print (f"Error processing T5: {T5}")
        error_messages.append(f"Error processing T5: {T5}")
        continue

# Create a DataFrame from the output rows and aggregate the data
output_df = pd.DataFrame(output_rows, columns=output_columns)
aggregated = output_df.groupby(['Company ID & Name', 'Match Rule']).size().reset_index(name='TRF')
total = aggregated['TRF'].sum()
aggregated = aggregated.append(pd.Series(['Total', '-', total], index=aggregated.columns), ignore_index=True)

# Write the output data to a CSV file
output_df.to_csv('Z:/Desktop/output2.csv', index=False)
with pd.ExcelWriter(config['output_file']) as writer:
    output_df.to_excel(writer, sheet_name='Output', index=False)










import pandas as pd

rules_df = pd.read_csv('Z:/Desktop/Rules.csv')
rules = []
rule_descriptions = {}
for i, row in rules_df.iterrows():
    rule_string = row['Rule']
    rule_description = row['Description']
    rule = eval(f"lambda row, row2: {rule_string}")
    rules.append(rule)
    rule_descriptions[i+1] = rule_description

# Define the output columns
output_columns = ['Company ID & Name', 'Match Rule', 'Description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no']

# Read the previously unmatched transactions if available
try:
    unmatched_output_df = pd.read_csv('Z:/Desktop/unmatched_output.csv')
except FileNotFoundError:
    unmatched_output_df = pd.DataFrame(columns=output_columns)

# Keep track of matched invoices
matched_invoices = []

# Filter T5 dataframe to remove rows with null 'fin_source_amt' column
T5_table = T5_table.dropna(subset=['fin_source_amt'])

# Loop through each row in T5_table and find a match in Invoice_table
for T5_idx, T5_row in T5_table.iterrows():
    try:
        matched = False
        for Invoice_idx, Invoice_row in Invoice_table.iterrows():
            matched, rule_index = match_row(T5_row, Invoice_row)
            if matched:
                description = rule_descriptions[rule_index]
                matched_output = {
                    column: T5_row[column] if column in T5_row else Invoice_row[column] for column in output_columns
                }
                matched_output['Match Rule'] = rule_index
                matched_output['Description'] = description
                matched_invoices.append(Invoice_idx)  # Keep track of matched invoices
                break
        if not matched:
            unmatched_output = {
                column: T5_row[column] for column in output_columns
            }
            unmatched_output['Match Rule'] = 'N/A'
            unmatched_output['Description'] = 'No match found'
            unmatched_output_df = unmatched_output_df.append(unmatched_output, ignore_index=True)
    except Exception as e:
        error_messages.append(f"Error processing T5 at index {T5_idx}: {str(e)}")
        continue

# Remove previously matched transactions from unmatched_output_df
unmatched_output_df = unmatched_output_df.drop(matched_invoices)

# Create DataFrames from the output rows for matched transactions
matched_output_df = pd.DataFrame(columns=output_columns)  # Create an empty DataFrame for matched transactions

# Aggregate the data
matched_aggregated = matched_output_df.groupby(['Company ID & Name', 'Match Rule']).size().reset_index(name='TRF')
matched_total = matched_aggregated['TRF'].sum()
matched_aggregated = matched_aggregated.append(pd.Series(['Total', '-', matched_total], index=matched_aggregated.columns), ignore_index=True)

# Write the output data to separate CSV files
matched_output_df.to_csv('Z:/Desktop/matched_output.csv', index=False)
unmatched_output_df.to_csv('Z:/Desktop/unmatched_output.csv', index=False)
matched_aggregated.to_csv('Z:/Desktop/matched_aggregated.csv', index=False)




import pandas as pd

def load_rules():
    rules_df = pd.read_csv('Z:/Desktop/Rules.csv')
    rules = []
    rule_descriptions = {}
    for i, row in rules_df.iterrows():
        rule_string = row['Rule']
        rule_description = row['Description']
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)
        rule_descriptions[i+1] = rule_description
    return rules, rule_descriptions



import pandas as pd
from rules_functions import load_rules

# Load rules_df, rules, and rule_descriptions
rules, rule_descriptions = load_rules()

# Define the output columns
output_columns = ['Company ID & Name', 'Match Rule', 'Description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no']

# Define lists to store the output rows for matched and unmatched transactions
matched_output_rows = []
unmatched_output_rows = []

# Rest of your code...
import pandas as pd

import pandas as pd

def load_rules(file_path):
    rules_df = pd.read_csv(file_path)
    rules = []
    rule_descriptions = {}
    for i, row in rules_df.iterrows():
        rule_string = row['Rule']
        rule_description = row['Description']
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)
        rule_descriptions[i+1] = rule_description
    return rules, rule_descriptions

from rules_loader import load_rules
# Define the path to the rules CSV file
rules_file_path = 'Z:/Desktop/Rules.csv'

# Load the rules using the load_rules() function
rules, rule_descriptions = load_rules(rules_file_path)




# Read the rules from the CSV file into a PySpark DataFrame
rules_df = spark.read.csv('Z:/Desktop/Rules.csv', header=True, inferSchema=True)

# Create empty lists to store rules and rule descriptions
rules = []
rule_descriptions = {}

# Iterate over the rows of the rules DataFrame
for row in rules_df.rdd.collect():
    rule_string = row['Rule']
    rule_description = row['Description']
    rule = eval(f"lambda row, row2: {rule_string}")
    rules.append(rule)
    rule_descriptions[row['id']] = rule_description



from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Read the T5 table from Excel using the config file
T5_table = spark.read.format('com.crealytics.spark.excel') \
    .option('header', 'true') \
    .option('dataAddress', 'T5') \
    .option('treatEmptyValuesAsNulls', 'true') \
    .option('inferSchema', 'true') \
    .load(config['input_files']['T5-file'])

# Drop duplicates based on 'fin_record_key'
T5_table = T5_table.dropDuplicates(['fin_record_key'])

# Filter rows based on 'fin_region_id'
T5_table = T5_table.filter(col('fin_region_id').isin(['1', '2', '3']))

# Filter rows based on 'fin_merch_catg_cd'
T5_table = T5_table.filter(col('fin_merch_catg_cd').isin(['3000', '3001', '3002', '3003']))

# Filter rows based on date using a custom function
def filter_date(date_column, days):
    return col(date_column) >= current_date() - expr(f'interval {days} days')

T5_table = T5_table.filter(filter_date('fin_posting_dt', 7))

# Drop rows with null values in specified columns
T5_table = T5_table.dropna(subset=['fin_source_amt', 'fin_orig_supplier_nm'])

# Convert DataFrame to a list of dictionaries (orient='records')
T5_data = T5_table.collect()
T5_records = [row.asDict() for row in T5_data]

# Rest of your code...


from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.window import Window

# Process each T5 transaction
for T5 in T5_table.collect():
    try:
        matched = False
        for Invoice in Invoice_table.collect():
            matched, rule_index = match_row(T5, Invoice)  # Modify the match_row function for Spark DataFrame comparison
            if matched:
                description = rule_descriptions.get(rule_index, '')  # Retrieve the description using the rule_index
                output_row = {}
                for column in output_columns:
                    if column in T5.columns:
                        output_row[column] = T5[column]
                    elif column in Invoice.columns:
                        output_row[column] = Invoice[column]
                output_row['Match Rule'] = rule_index
                output_row['Description'] = description
                output_rows.append(output_row)
                break

        if not matched:
            unmatched_output_row = {column: T5[column] for column in unmatched_output_columns}
            unmatched_output_rows.append(unmatched_output_row)

    except Exception as e:
        error_messages.append(f"Error processing T5: {T5}\n{str(e)}")
        continue





import pandas as pd

def read_rules_df(file_path):
    rules_df = pd.read_csv(file_path)
    rules = []
    rule_descriptions = {}
    for i, row in rules_df.iterrows():
        rule_string = row['Rule']
        rule_description = row['Description']
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)
        rule_descriptions[i+1] = rule_description
    return rules_df, rules, rule_descriptions


import pandas as pd
import yaml
from rules_reader import read_rules_df

# Read the rules DataFrame using the function from the separate file
rules_df, rules, rule_descriptions = read_rules_df('Z:/Desktop/Rules.csv')

# Rest of your code...


import pandas as pd
import yaml
import os

# Read the rules from the CSV file
rules_df = pd.read_csv('Z:/Desktop/Rules.csv')
rules = []
rule_descriptions = {}
for i, row in rules_df.iterrows():
    rule_string = row['Rule']
    rule_description = row['Description']
    rule = eval(f"lambda row, row2: {rule_string}")
    rules.append(rule)
    rule_descriptions[i+1] = rule_description  # Fixed index to i+1

# Read the config file
with open('config.yml', 'r') as f:
    config = yaml.safe_load(f)

# Define the output columns
output_columns = config['output_columns']

# Define a list to store the output rows
output_rows = []

# Define a list to store the unmatched transactions
unmatched_output_columns = config['unmatched_output_columns']
unmatched_output_rows = []

# Filter T5 dataframe to remove rows with null 'fin_source_amt' column
T5_table = T5_table.dropna(subset=['fin_source_amt'])

# Process each T5 transaction
for T5 in T5_table:
    try:
        matched = False
        for Invoice in Invoice_table:
            matched, rule_index = match_row(T5, Invoice)
            if matched:
                description = rule_descriptions.get(rule_index, '')  # Retrieve the description using the rule_index
                output_row = {
                    column: T5[column] if column in T5 else Invoice[column] for column in output_columns
                }
                output_row['Match Rule'] = rule_index
                output_row['Description'] = description
                output_rows.append(output_row)
                break

        if not matched:
            unmatched_output_row = {
                column: T5[column] for column in unmatched_output_columns
            }
            unmatched_output_rows.append(unmatched_output_row)

    except Exception as e:
        error_messages.append(f"Error processing T5: {T5}\n{str(e)}")
        continue

# Create a DataFrame from the output rows
output_df = pd.DataFrame(output_rows, columns=output_columns)

# Aggregate the data
aggregated = output_df.groupby(['Company ID & Name', 'Match Rule']).size().reset_index(name='TRF')
total = aggregated['TRF'].sum()
aggregated = aggregated.append(pd.Series(['Total', '-', total], index=aggregated.columns), ignore_index=True)

# Write the output data to a CSV file
output_df.to_csv('Z:/Desktop/output.csv', index=False)

# Create a DataFrame from the unmatched output rows
unmatched_output_df = pd.DataFrame(unmatched_output_rows, columns=unmatched_output_columns)

# Append unmatched transactions to the existing unmatched output file
unmatched_file_path = 'Z:/Desktop/unmatched_output.csv'
if os.path.isfile(unmatched_file_path):
    previous_unmatched_df = pd.read_csv(unmatched_file_path)
    unmatched_output_df = pd.concat([previous_unmatched_df, unmatched_output_df])

# Remove matched transactions from the unmatched output file
matched_transactions = output_df[output_df['Match Rule'] != '-']
if not matched_transactions.empty:
    matched_indices = matched_transactions.index
    unmatched_output_df = unmatched_output_df.drop(matched_indices, errors='ignore')

# Write the updated unmatched transactions to the unmatched output file
unmatched_output_df.to_csv(unmatched_file_path, index=False)

# Write the aggregated data to Excel
with pd.ExcelWriter(config['output_file']) as writer:
    output_df.to_excel(writer, sheet_name='Output', index=False)
    aggregated.to_excel













from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize SparkSession
spark = SparkSession.builder.getOrCreate()

# Read the rules from the CSV file
rules_df = spark.read.csv('Z:/Desktop/Rules.csv', header=True)
rules = []
rule_descriptions = {}
for row in rules_df.collect():
    rule_string = row['Rule']
    rule_description = row['Description']
    rule = eval(f"lambda row, row2: {rule_string}")
    rules.append(rule)
    rule_descriptions[row['Rule Index']] = rule_description

# Read the config file
with open('config.yml', 'r') as f:
    config = yaml.safe_load(f)

# Define the output columns
output_columns = config['output_columns']

# Filter T5 dataframe to remove rows with null 'fin_source_amt' column
T5_table = T5_table.dropna(subset=['fin_source_amt'])

# Broadcast the rules and rule descriptions to all worker nodes
broadcast_rules = spark.sparkContext.broadcast(rules)
broadcast_rule_descriptions = spark.sparkContext.broadcast(rule_descriptions)

# Process each T5 transaction
output_rows = []
unmatched_output_rows = []
for T5 in T5_table.rdd.toLocalIterator():
    try:
        matched = False
        for Invoice in Invoice_table.rdd.collect():
            matched, rule_index = match_row(T5, Invoice)
            if matched:
                description = broadcast_rule_descriptions.value.get(rule_index, '')
                output_row = {
                    column: T5[column] if column in T5 else Invoice[column] for column in output_columns
                }
                output_row['Match Rule'] = rule_index
                output_row['Description'] = description
                output_rows.append(output_row)
                break

        if not matched:
            unmatched_output_row = {
                column: T5[column] for column in unmatched_output_columns
            }
            unmatched_output_rows.append(unmatched_output_row)

    except Exception as e:
        error_messages.append(f"Error processing T5: {T5}\n{str(e)}")
        continue

# Create DataFrames from the output rows
output_df = spark.createDataFrame(output_rows)
unmatched_output_df = spark.createDataFrame(unmatched_output_rows)

# Aggregate the data
aggregated = output_df.groupby(['Company ID & Name', 'Match Rule']).count().withColumnRenamed('count', 'TRF')
total = aggregated.selectExpr('sum(TRF)').collect()[0][0]
aggregated = aggregated.union(spark.createDataFrame([['Total', '-', total]], schema=aggregated.schema))

# Write the output data to a CSV file
output_df.write.csv('Z:/Desktop/output.csv', header=True, mode='overwrite')

# Write the unmatched transactions to a separate CSV file
unmatched_output_df.write.csv('Z:/Desktop/unmatched_output.csv', header=True, mode='overwrite')

# Write the aggregated data to Excel
output_df.write.format('excel').option('sheet_name', 'Output').option('header', 'true').mode('overwrite').save(config['output_file'], compression='uncompressed')
aggregated.write.format('excel').option('sheet_name', 'Aggregated').option('header', 'true').mode('append').save(config['output_file'], compression='uncompressed')








# T5 Data Filtering
T5_table = pd.read_excel(config['input_files'][0])
original_rows_count = len(T5_table)

T5_table = T5_table.drop_duplicates(subset=['fin_record_key'])
duplicated_rows = pd.concat([T5_table, T5_table]).drop_duplicates(keep=False)

T5_table = T5_table[T5_table['fin_region_id'].isin(['1', '2', '3'])]
T5_table = T5_table[T5_table['fin_merch_catg_cd'].isin(['3000', '3001', '3002', '3003'])]
T5_table = T5_table.dropna(subset=['fin_source_amt', 'fin_orig_supplier_nm'])

filtered_rows_count = len(T5_table)
removed_rows_count = original_rows_count - filtered_rows_count

# Store removed rows in a separate Excel file
removed_rows_df = pd.concat([duplicated_rows, T5_table]).drop_duplicates(keep=False)
removed_rows_df.to_excel('Z:/Desktop/removed_rows.xlsx', index=False)
