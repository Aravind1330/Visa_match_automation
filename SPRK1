from pyspark.sql.functions import col, lit, when
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("Matching Data").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as Spark DataFrames
T5_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty list for the output data
output_data = []

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Loop through each row in table1 and find a match in table2
for T5 in T5_table.collect():
    matched = False
    for Invoice in Invoice_table.collect():
        matched, rule_index = match_row(T5, Invoice)
        if matched:
            description = rule_descriptions[rule_index]
            output_data.append({'Company ID & Name': T5['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5['fin_orig_supplier_nm'], 'fin_source_amt': T5['fin_source_amt'], 'inv_match_source_amt': Invoice['inv_match_source_amt'], 'inv_erp_vend_no': Invoice['inv_erp_vend_no'], 'inv_po_no': Invoice['inv_po_no']})
            break

# Create a Spark DataFrame from the output data list
output_df = spark.createDataFrame(output_data)

# Add a column for Total Rows Found (TRF) and calculate the total rows found
aggregated = output_df.groupBy(['Company ID & Name', 'Match Rule']).count().withColumnRenamed("count", "TRF")
total = output_df.count()
total_row = spark.createDataFrame([("Total", "-", total,)], output_df.columns)

# Union the aggregated data with the total row and write to an Excel file
aggregated.union(total_row).write.format("com.databricks.spark.csv").option("header", "true").option("delimiter", "\t").mode("overwrite").save("Z:/Desktop/output2.csv")





from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType, BooleanType

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Define UDFs for applying the match_row function and getting the rule description
match_row_udf = udf(lambda T5, Invoice: match_row(T5.asDict(), Invoice.asDict()), BooleanType())
rule_description_udf = udf(lambda rule_index: rule_descriptions.get(rule_index, '-'), StringType())

# Read in the T5 and Invoice data as Spark dataframes
T5_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', delimiter=',', quote='"').load('Z:/Downloads/Unmatched_trans_Data_1.csv')
Invoice_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', delimiter=',', quote='"').load('Z:/Downloads/Unmatched_Inv_Data_1.csv')

# Create an empty column for the match rule index in the T5 dataframe
T5_df = T5_df.withColumn('Match Rule', lit("Unmatched"))

# Loop through each row in T5 and find a match in Invoice
for i, T5_row in T5_df.iterrows():
    matched = False
    for j, Invoice_row in Invoice_df.iterrows():
        matched, rule_index = match_row_udf(T5_row, Invoice_row)
        if matched:
            T5_df = T5_df.withColumn('Match Rule', lit(rule_index))
            break

# Define rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Create the "description" column based on the "Match Rule" column
T5_df = T5_df.withColumn('description', rule_description_udf(col('Match Rule')))

# Print the updated dataframe
# T5_df.show()

## Convert to csv
T5_df.write.format('com.databricks.spark.csv').options(header='true', delimiter=',', quote='"').mode('overwrite').save('Z:/Desktop/output2.csv')

# Aggregate the data by Company ID & Name and Match Rule
aggregated = T5_df.groupBy('Company ID & Name', 'Match Rule').count().withColumnRenamed('count', 'TRF')

# Add a row for the total count
total_count = aggregated.select('TRF').agg({'TRF': 'sum'}).collect()[0][0]
total_row = ('Total', '-', total_count)
total_df = spark.createDataFrame([total_row], ['Company ID & Name', 'Match Rule', 'TRF'])
aggregated = aggregated.union(total_df)

# Write the aggregated data to a separate csv file
aggregated.write.format('com.databricks.spark.csv').options(header='true






# Find a match for each row in T5 and update the "Match Rule" column
joined_df = T5_df.crossJoin(Invoice_df)
matched_df = joined_df.filter(match_row_udf(col('T5_df.*'), col('Invoice_df.*')))
rule_index_udf = udf(lambda T5, Invoice: match_row(T5, Invoice)[1])
T5_df = T5_df.join(matched_df, ['T5_no']).withColumn('Match Rule', rule_index_udf(col('T5_df.*'), col('Invoice_df.*')))










from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType, IntegerType, StringType, StructType, StructField

# Define the schema for the input data
schema = StructType([
    StructField("fin_orig_supplier_nm", StringType()),
    StructField("fin_source_amt", StringType()),
    StructField("Company ID & Name", StringType()),
    StructField("Match Rule", StringType()),
    StructField("description", StringType())
])

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = udf(eval(f"lambda T5, Invoice: {rule_string}"), BooleanType())
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the first Excel sheet
T5_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
# Read in the second Excel sheet
Invoice_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Define the UDF to match each row in table1 with table2
match_row_udf = udf(lambda T5, Invoice: match_row(T5, Invoice), StructType([
    StructField("matched", BooleanType()),
    StructField("rule_index", IntegerType())
]))

# Join the two dataframes and apply the match_row_udf to each row pair
matched_df = T5_df.crossJoin(Invoice_df).withColumn("match_result", match_row_udf(T5_df, Invoice_df))

# Extract the columns of interest and filter the matched rows
matched_df = matched_df.select("fin_orig_supplier_nm", "fin_source_amt", "Company ID & Name", "match_result.rule_index.alias('Match Rule')", "description").filter("match_result.matched")

# Write the matched data to an Excel file
matched_df.write.format("excel").option("header", "true").save("Z:/Desktop/output2.xlsx")

# Aggregate the matched data and compute the total count
aggregated_df = matched_df.groupBy("Company ID & Name", "Match Rule").count().withColumnRenamed("count", "TRF")
total_count = aggregated_df.agg({"TRF": "sum"}).collect()[0][0]

# Append the total row to the aggregated data and write to the same Excel file
aggregated_df = aggregated_df.unionAll(spark.createDataFrame([["Total", "-", total_count]], schema))
aggregated_df.write.format("excel").option("header", "true").mode("append").save("Z:/Desktop/output2.xlsx")







from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, BooleanType, IntegerType
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("matching").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = udf(lambda row, row2: eval(rule_string), BooleanType())
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as Spark DataFrames
T5_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Z:/Downloads/Unmatched_trans_Data_1.xlsx')
Invoice_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Z:/Downloads/Unmatched_Inv_Data_1.xlsx')

# Define a UDF for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}
get_description = udf(lambda rule_index: rule_descriptions.get(rule_index, ''), StringType())

# Define UDFs to convert column types
to_str = udf(lambda x: str(x), StringType())
to_float = udf(lambda x: float(x) if x is not None else None, FloatType())

# Apply UDFs to convert column types
T5_df = T5_df.withColumn('fin_source_amt', to_float('fin_source_amt'))
Invoice_df = Invoice_df.withColumn('inv_match_source_amt', to_float('inv_match_source_amt'))

# Create an empty list for the output data
output_data = []

# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.collect():
    matched = False
    for Invoice_row in Invoice_df.collect():
        matched, rule_index = match_row(T5_row, Invoice_row)
        if matched:
            description = get_description(rule_index)
            output_data.append({'Company ID & Name': T5_row['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5_row['fin_orig_supplier_nm'], 'fin_source_amt': T5_row['fin_source_amt'], 'inv_match_source_amt': Invoice_row['inv_match_source_amt']})
            break

# Convert output data to a Spark DataFrame
output_df = spark.createDataFrame(output_data)

# Aggregate output data
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').count()
total = aggregated.selectExpr('sum(count)').collect()[0][0]
aggregated = aggregated.union(spark.createDataFrame([('Total', '-', total)], ['Company ID & Name', 'Match Rule', 'count']))

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source

output_df.select(output_columns).coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('Z:/Desktop/matching_output.csv')







from pyspark.sql.functions import col, lit, when

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as DataFrames
T5_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty DataFrame for the output data
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt']
output_df = spark.createDataFrame([], output_columns)

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.rdd.toLocalIterator():
    matched = False
    for Invoice_row in Invoice_df.rdd.toLocalIterator():
        matched, rule_index = match_row(T5_row, Invoice_row)
        if matched:
            description = rule_descriptions[rule_index]
            output_row = [T5_row['Company ID & Name'], rule_index, description, T5_row['fin_orig_supplier_nm'], T5_row['fin_source_amt'], Invoice_row['inv_match_source_amt']]
            output_df = output_df.union(spark.createDataFrame([output_row], output_columns))
            break

# Add a total row to the aggregated DataFrame
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').count().withColumnRenamed('count', 'TRF')
total = output_df.count()
aggregated = aggregated.union(spark.createDataFrame([['Total', '-', total]], aggregated.columns))

# Write the output data to an Excel file
output_df.write.format("excel").option("header", "true").mode("overwrite").save("Z:/Desktop/output2.xlsx", sheet_name='Output', index=False)
aggregated.write.format("excel").option("header", "true").mode("overwrite").save("Z:/Desktop/output2.xlsx", sheet_name='Aggregated', index=False)








from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("T5_Invoice_Match").getOrCreate()

# Read the T5 and Invoice Excel files as Spark dataframes
t5_df = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
invoice_df = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Read the rules from the text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Define a UDF to match T5 and Invoice tables
from pyspark.sql.functions import udf
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

match_udf = udf(lambda T5, Invoice: match_row(T5, Invoice), StructType([StructField("matched", StringType()), StructField("rule_index", IntegerType())]))

# Apply the match UDF to T5 and Invoice tables
matched_df = t5_df.crossJoin(invoice_df).withColumn("match_result", match_udf(t5_df, invoice_df)).select("Company ID & Name", "match_result.*")

# Filter out unmatched rows
matched_df = matched_df.filter(matched_df.matched == "True")

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Define a UDF to get the rule description
rule_description_udf = udf(lambda rule_index: rule_descriptions.get(rule_index, "Unknown"), StringType())

# Add the rule description column to the matched dataframe
matched_df = matched_df.withColumn("description", rule_description_udf(matched_df.rule_index))

# Select the required columns and write the output data to an Excel file
output_columns = ['Company ID & Name', 'rule_index', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt']
matched_df.select(output_columns).write.format("com.databricks.spark.csv").option("header", "true").mode("overwrite").save("Z:/Desktop/output2.csv")

# Aggregate the data by company ID and rule index
aggregated_df = matched_df.groupBy("Company ID & Name", "rule_index").count().withColumnRenamed("count", "TRF")

# Add the total row to the aggregated dataframe
total_row = spark.createDataFrame([["Total", "-", aggregated_df.select("TRF").rdd.flatMap(lambda x: x).sum()]], ["Company ID & Name", "rule_index", "TRF"])
aggregated_df = aggregated_df.union(total_row














from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

# Define the path to the rule file
RULES_FILE_PATH = "Z:/Desktop/Rules_1.txt"

# Create a SparkSession
spark = SparkSession.builder.appName("MatchRows").getOrCreate()

# Define a UDF to parse the rule string and create a lambda function
def parse_rule(rule_string):
    rule_string = rule_string.strip()
    if not rule_string:
        return None
    rule = eval(f"lambda row, row2: {rule_string}")
    return rule

parse_rule_udf = spark.udf.register("parse_rule", parse_rule)

# Load the rules from the file and create a DataFrame
rules_df = spark.read.text(RULES_FILE_PATH)
rules_df = rules_df.select(parse_rule_udf(col("value")).alias("rule")).filter(col("rule").isNotNull())

# Load the T5 and Invoice tables as DataFrames
T5_table = spark.read.format("excel").option("header", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_table = spark.read.format("excel").option("header", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Define a UDF to apply the rules to each row pair and return the rule index
def match_row_udf(T5_dict, Invoice_dict, rules_list):
    for i, rule in enumerate(rules_list):
        if rule(T5_dict, Invoice_dict):
            return (True, i+1)
    return (False, "Unmatched")

match_row_udf = spark.udf.register("match_row_udf", match_row_udf)

# Join the T5 and Invoice tables and apply the rules to each row pair
joined_df = T5_table.crossJoin(Invoice_table)
matched_df = joined_df.withColumn("matched", match_row_udf(col("T5_table"), col("Invoice_table"), lit(rules_df.collect())))
matched_df = matched_df.filter(col("matched")[0] == True).select(col("T5_table.*"), col("matched")[1].alias("rule_index"), col("Invoice_table.inv_match_source_amt"))

# Define a DataFrame for rule descriptions
rule_desc_df = spark.createDataFrame([(1, "Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same"),
                                      (2, "Ticket_no_right_of_12")],
                                     ["rule_index", "description"])

# Join the matched DataFrame with the rule descriptions DataFrame and select the output columns
output_df = matched_df.join(rule_desc_df, on="rule_index").select(col("Company ID & Name"), col("rule_index").alias("Match Rule"), col("description"), col("fin_orig_supplier_nm"), col("fin_source_amt"), col("inv_match_source_amt"))

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt']
output_df.write.format("excel").option("header", "true").mode("overwrite").save("Z:/Desktop/output2.xlsx")

# Aggregate the output by Company ID & Name and Match Rule and add a Total row
aggregated_df = output_df.groupBy("Company ID & Name", "Match Rule").count().withColumnRenamed("count", "TRF")
total_df = aggregated_df.groupBy().agg({"TRF": "sum"}).withColumn("Company ID & Name", lit("Total")).withColumn("Match Rule













from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lit, count, sum

# Create a Spark session
spark = SparkSession.builder.appName("DataMatching").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as DataFrames
T5_df = spark.read.format("com.crealytics.spark.excel").option("header", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_df = spark.read.format("com.crealytics.spark.excel").option("header", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty list for the output data
output_data = []

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Loop through each row in T5_df and find a match in Invoice_df
for T5_row in T5_df.rdd.collect():
    matched = False
    for Invoice_row in Invoice_df.rdd.collect():
        matched, rule_index = match_row(T5_row, Invoice_row)
        if matched:
            description = rule_descriptions[rule_index]
            output_data.append({'Company ID & Name': T5_row['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5_row['fin_orig_supplier_nm'], 'fin_source_amt': T5_row['fin_source_amt'], 'inv_match_source_amt': Invoice_row['inv_match_source_amt']})
            break

# Create a DataFrame from the output data
output_df = spark.createDataFrame(output_data)

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt']
output_df.write.format("com.crealytics.spark.excel").option("header", "true").option("dataAddress", "A2").mode("overwrite").save("Z:/Desktop/output2.xlsx")

# Aggregate the output data and write to the same Excel file
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').agg(count("*").alias('TRF'))
total = aggregated.agg(sum("TRF")).collect()[0][0]
total_row = spark.createDataFrame([("Total", "-", total)], output_columns + ['TRF'])
aggregated = aggregated.union(total_row).orderBy(col("Company ID & Name"), col("Match Rule"))

aggregated.write.format("








import pyspark.sql.functions as F
from pyspark.sql.types import StringType
from pyspark.sql.functions import udf
from pyspark.sql import SparkSession
Initialize a SparkSession

spark = SparkSession.builder.appName("Matching").getOrCreate()
Read the rules from a text file

with open("Z:/Desktop/Rules_1.txt", 'r') as f:
rule_strings = f.readlines()
Remove new line characters and empty lines, and convert to list of functions

rules = []
for rule_string in rule_strings:
rule_string = rule_string.strip()
if rule_string:
rule = eval(f"lambda row, row2: {rule_string}")
rules.append(rule)
Define a UDF to apply the rules to each row pair and return the rule index

@udf(returnType=StringType())
def match_row(T5, Invoice):
for i, rule in enumerate(rules):
if rule(T5, Invoice):
return str(i+1)
return "Unmatched"
Read in the two Excel sheets as Spark dataframes

T5_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")
Create an empty list for the output data

output_data = []
Define a dictionary for rule descriptions

rule_descriptions = {
'1': 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
'2': 'Ticket_no_right_of_12'
}
Define UDFs for getting the company ID and name and calculating the total number of matches

@udf(returnType=StringType())
def get_company_id_and_name(row):
return row['Company ID & Name']

@udf(returnType=StringType())
def get_description(rule_index):
return rule_descriptions[rule_index]

@udf(returnType=StringType())
def get_total_matches(matches):
total = sum(matches)
return str(total)
Loop through each row in table1 and find a match in table2

matched_df = T5_table.crossJoin(Invoice_table)
matched_df = matched_df.withColumn("match_rule", match_row(F.struct([matched_df[x] for x in T5_table.columns]), F.struct([matched_df[x] for x in Invoice_table.columns])))
matched_df = matched_df.filter(matched_df["match_rule"] != "Unmatched")

output_df = matched_df.select(get_company_id_and_name(T5_table).alias("Company ID & Name"),
"match_rule",
get_description("match_rule").alias("description"),
"fin_orig_supplier_nm",
"fin_source_amt",
"inv_match_source_amt")
Write the output data to an Excel file

aggregated = output_df.groupBy(['Company ID & Name', 'match_rule']).count().withColumnRenamed("count", "TRF")
total = aggregated.select(F.sum("TRF")).collect()[0][0]
aggregated = aggregated.union(spark.createDataFrame([("Total", "-", total)], schema=['Company ID & Name', 'match_rule', 'TRF']))

output_columns = ['Company ID & Name', 'match_rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt']
output_df = output_df.select(output_columns)

output_df.write.format("com.databricks.spark.csv












from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, BooleanType, IntegerType

# Define a SparkSession
spark = SparkSession.builder.appName("matching").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = udf(eval(f"lambda t5, invoice: {rule_string}"), BooleanType())
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
match_row_udf = udf(lambda t5, invoice: [(i+1) for i, rule in enumerate(rules) if rule(t5, invoice)], ArrayType(IntegerType()))

# Read in the two Excel sheets as Spark DataFrames
T5_df = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_df = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Define a UDF for the rule description
rule_desc_udf = udf(lambda rule_index: rule_descriptions.get(rule_index, "Unmatched"), StringType())

# Join the two DataFrames and apply the matching function to each row pair
matched_df = T5_df.crossJoin(Invoice_df).withColumn("matched_rules", match_row_udf(T5_df["Company ID & Name"], Invoice_df["Company ID & Name"]))

# Filter out unmatched rows and explode the matched_rules array
matched_df = matched_df.filter(matched_df["matched_rules"].isNotNull()).select("Company ID & Name", "matched_rules", "fin_orig_supplier_nm", "fin_source_amt", "inv_match_source_amt")
matched_df = matched_df.select("Company ID & Name", "matched_rules", rule_desc_udf("matched_rules").alias("description"), "fin_orig_supplier_nm", "fin_source_amt", "inv_match_source_amt").withColumn("matched_rule", explode("matched_rules")).drop("matched_rules")

# Aggregate the matched data by Company ID & Name and matched rule
aggregated_df = matched_df.groupBy("Company ID & Name", "matched_rule", "description").count().withColumnRenamed("count", "TRF")
total = aggregated_df.select("TRF").rdd.flatMap(lambda x: x).sum()
total_row = ("Total", "-", total)
aggregated_df = aggregated_df.union(spark.createDataFrame([total_row], schema=aggregated_df.schema))

# Write the output data to an Excel file
matched_df.write.format("com.databricks.spark.csv").option("header", "true").option("delimiter", "\t").mode("overwrite").save("Z:/Desktop/output2_matched")
aggregated_df.write.format("com.databricks.spark.csv").option("header", "












from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, IntegerType, StructType, StructField
import pyspark.sql.functions as F

# Read the rules from a text file
rules_df = spark.read.text("Z:/Desktop/Rules_1.txt")
rules = []
for row in rules_df.collect():
    rule_string = row.value.strip()
    if rule_string:
        exec(f"rule = lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a UDF to apply the rules to each row pair and return the rule index
match_row_udf = udf(lambda T5, Invoice: next((i+1 for i, rule in enumerate(rules) if rule(T5, Invoice)), "Unmatched"), StringType())

# Read in the two Excel sheets as dataframes
T5_table = spark.read.format('csv').options(header=True, inferSchema=True).load('Z:/Downloads/Unmatched_trans_Data_1.xlsx')
Invoice_table = spark.read.format('csv').options(header=True, inferSchema=True).load('Z:/Downloads/Unmatched_Inv_Data_1.xlsx')

# Define a schema for the output data
output_schema = StructType([
    StructField("Company ID & Name", StringType(), True),
    StructField("Match Rule", StringType(), True),
    StructField("description", StringType(), True),
    StructField("fin_orig_supplier_nm", StringType(), True),
    StructField("fin_source_amt", StringType(), True),
    StructField("inv_match_source_amt", StringType(), True)
])

# Loop through each row in T5_table and find a match in Invoice_table
output_data = T5_table.crossJoin(Invoice_table) \
    .withColumn("Match Rule", match_row_udf(F.struct(*T5_table.columns), F.struct(*Invoice_table.columns))) \
    .filter(F.col("Match Rule") != "Unmatched") \
    .select("Company ID & Name", "Match Rule", "fin_orig_supplier_nm", "fin_source_amt", "inv_match_source_amt") \
    .withColumn("description", F.when(F.col("Match Rule") == 1, "Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same").when(F.col("Match Rule") == 2, "Ticket_no_right_of_12")) \
    .orderBy("Company ID & Name", "Match Rule") \
    .collect()

# Convert the output data to a dataframe and write to an Excel file
output_df = spark.createDataFrame(output_data, output_schema)
aggregated = output_df.groupBy("Company ID & Name", "Match Rule").agg(F.count("*").alias("TRF")).orderBy("Company ID & Name", "Match Rule")
total = aggregated.agg(F.sum("TRF")).collect()[0][0]
aggregated = aggregated.union(spark.createDataFrame([("Total", "-", total,)], ["Company ID & Name", "Match Rule", "TRF"]))
output_df.write.format("com.databricks.spark.excel") \
    .option("header", "true")















import pyspark.sql.functions as F
Read the rules from a text file

with open("Z:/Desktop/Rules_1.txt", 'r') as f:
rule_strings = f.readlines()
Remove new line characters and empty lines, and convert to list of functions

rules = []
for rule_string in rule_strings:
rule_string = rule_string.strip()
if rule_string:
rule = eval(f"lambda t5, invoice: {rule_string}")
rules.append(rule)
Define a function to apply the rules to each row pair and return the rule index

def match_row(t5, invoice):
for i, rule in enumerate(rules):
if rule(t5, invoice):
return True, i+1
return False, "Unmatched"
Read in the two Excel sheets as dataframes

t5_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true').load('Z:/Downloads/Unmatched_trans_Data_1.xlsx')
invoice_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true').load('Z:/Downloads/Unmatched_Inv_Data_1.xlsx')
Create an empty list for the output data

output_data = []
Define a dictionary for rule descriptions

rule_descriptions = {
1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
2: 'Ticket_no_right_of_12'
}
Loop through each row in t5_df and find a match in invoice_df

for row_t5 in t5_df.rdd.collect():
matched = False
for row_invoice in invoice_df.rdd.collect():
matched, rule_index = match_row(row_t5, row_invoice)
if matched:
description = rule_descriptions[rule_index]
output_data.append({'Company ID & Name': row_t5['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': row_t5['fin_orig_supplier_nm'], 'fin_source_amt': row_t5['fin_source_amt'], 'inv_match_source_amt': row_invoice['inv_match_source_amt']})
break
Convert the output data to a dataframe

output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt']
output_df = spark.createDataFrame(output_data, schema=output_columns)
Write the output data to an Excel file

aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').agg(F.count('*').alias('TRF'))
total = aggregated.agg(F.sum('TRF').alias('Total')).select(F.lit('Total').alias('Company ID & Name'), F.lit('-').alias('Match Rule'), 'Total',).union(aggregated)

output_df.write.format('com.databricks.spark.csv').option('header', 'true').option('delimiter', ',').mode('overwrite').save('Z:/Desktop/output2.csv')
aggregated.write.format('com.databricks.spark.csv').option('header', 'true').option('delimiter', ',').mode('overwrite').save('Z:/Desktop/aggregated.csv')







from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when

# Create a SparkSession
spark = SparkSession.builder.appName("Match Rows").getOrCreate()

# Read in the T5 and Invoice tables
T5_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.csv")
Invoice_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.csv")

# Define UDFs for the match rules
rule1 = lambda T5, Invoice: T5.fin_orig_supplier_nm[-13:] == Invoice.inv_ticket_num[-13:] and T5.fin_source_amt == Invoice.inv_match_source_amt
rule2 = lambda T5, Invoice: T5.fin_orig_supplier_nm[-12:] == Invoice.inv_ticket_num[-12:] and T5.fin_source_amt == Invoice.inv_match_source_amt
rule3 = lambda T5, Invoice: T5.fin_orig_supplier_nm[-11:] == Invoice.inv_ticket_num[-11:]

# Define a UDF to apply the rules to each row pair and return the rule index
match_row_udf = udf(lambda T5, Invoice: \
                    when(rule1(T5, Invoice), lit(1)) \
                    .when(rule2(T5, Invoice), lit(2)) \
                    .when(rule3(T5, Invoice), lit(3)) \
                    .otherwise(lit(0)), IntegerType())

# Join the T5 and Invoice tables on the Company ID & Name column
joined_table = T5_table.join(Invoice_table, on=["Company ID & Name"], how="inner")

# Add a column for the match rule index
joined_table = joined_table.withColumn("Match Rule", match_row_udf(col("T5"), col("Invoice")))

# Define rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12',
    3: 'Ticket_no_right_of_11'
}

# Add a column for the description of the match rule
joined_table = joined_table.withColumn("description", when(col("Match Rule") > 0, rule_descriptions[col("Match Rule")]).otherwise("Unmatched"))

# Write the output to an Excel file
joined_table.write.format("com.crealytics.spark.excel") \
    .option("header", "true") \
    .option("dataAddress", "'Sheet1'!A1") \
    .save("Z:/Desktop/output2.xlsx")

# Aggregate the data by Company ID & Name and Match Rule
aggregated = joined_table.groupBy("Company ID & Name", "Match Rule").count().withColumnRenamed("count", "TRF")

# Compute the total count and append it to the aggregated data
total_count = aggregated.select("TRF").agg({"TRF": "sum"}).collect()[0]["sum(TRF)"]
total_row = spark.createDataFrame([("Total", "-", total_count)], schema=["Company ID & Name", "Match Rule", "TRF"])
aggregated = aggregated.union(total_row)

# Write the aggregated data to a separate sheet in the Excel file
aggregated.write.format("com.crealytics.spark.excel") \
    .option("header",







from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType, IntegerType, StringType, StructType, StructField

# Define the schema for the input data
T5_schema = StructType([
    StructField("Company ID & Name", StringType(), True),
    StructField("fin_orig_supplier_nm", StringType(), True),
    StructField("fin_source_amt", StringType(), True)
])

Invoice_schema = StructType([
    StructField("Company ID & Name", StringType(), True),
    StructField("inv_ticket_num", StringType(), True),
    StructField("inv_match_source_amt", StringType(), True)
])

# Define the matching rules as UDFs
def rule1(T5, Invoice):
    if T5["fin_orig_supplier_nm"][-13:] == Invoice["inv_ticket_num"][-13:] \
        and T5["fin_source_amt"] == Invoice["inv_match_source_amt"]:
        return True
    return False

def rule2(T5, Invoice):
    if T5["fin_orig_supplier_nm"][-12:] == Invoice["inv_ticket_num"][-12:] \
        and T5["fin_source_amt"] == Invoice["inv_match_source_amt"]:
        return True
    return False

def rule3(T5, Invoice):
    if T5["fin_orig_supplier_nm"][-11:] == Invoice["inv_ticket_num"][-11:] \
        and T5["fin_source_amt"] == Invoice["inv_match_source_amt"]:
        return True
    return False

# Define the match_row function as a UDF
match_row_udf = udf(lambda T5, Invoice: int(rule1(T5, Invoice))*1 + int(rule2(T5, Invoice))*2 + int(rule3(T5, Invoice))*3,
                    IntegerType())

# Create a SparkSession
spark = SparkSession.builder.appName("Matching").getOrCreate()

# Read the data from Excel files as DataFrames
T5_df = spark.read.format("com.crealytics.spark.excel").option("header", "true").schema(T5_schema).load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_df = spark.read.format("com.crealytics.spark.excel").option("header", "true").schema(Invoice_schema).load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Join the two DataFrames on the "Company ID & Name" column
joined_df = T5_df.join(Invoice_df, "Company ID & Name")

# Add a new column with the matching rule index
joined_df = joined_df.withColumn("Match Rule", match_row_udf(joined_df.columns))

# Define rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Add a new column with the rule description
joined_df = joined_df.withColumn("description", udf(lambda i: rule_descriptions.get(i, "Unmatched"), StringType())("Match Rule"))

# Write the output to an Excel file
joined_df.write.format("com.crealytics.spark.excel").option("header", "true").option("dataAddress", "'Sheet1'!A1").mode("overwrite").save("Z:/Desktop/output2.xlsx")

# Aggregate the data and write to a separate sheet in the same Excel file
aggregated_df = joined_df.groupBy("Company ID & Name", "Match Rule").count().with
