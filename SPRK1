from pyspark.sql.functions import col, lit, when
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("Matching Data").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as Spark DataFrames
T5_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty list for the output data
output_data = []

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Loop through each row in table1 and find a match in table2
for T5 in T5_table.collect():
    matched = False
    for Invoice in Invoice_table.collect():
        matched, rule_index = match_row(T5, Invoice)
        if matched:
            description = rule_descriptions[rule_index]
            output_data.append({'Company ID & Name': T5['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5['fin_orig_supplier_nm'], 'fin_source_amt': T5['fin_source_amt'], 'inv_match_source_amt': Invoice['inv_match_source_amt'], 'inv_erp_vend_no': Invoice['inv_erp_vend_no'], 'inv_po_no': Invoice['inv_po_no']})
            break

# Create a Spark DataFrame from the output data list
output_df = spark.createDataFrame(output_data)

# Add a column for Total Rows Found (TRF) and calculate the total rows found
aggregated = output_df.groupBy(['Company ID & Name', 'Match Rule']).count().withColumnRenamed("count", "TRF")
total = output_df.count()
total_row = spark.createDataFrame([("Total", "-", total,)], output_df.columns)

# Union the aggregated data with the total row and write to an Excel file
aggregated.union(total_row).write.format("com.databricks.spark.csv").option("header", "true").option("delimiter", "\t").mode("overwrite").save("Z:/Desktop/output2.csv")





from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType, BooleanType

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Define UDFs for applying the match_row function and getting the rule description
match_row_udf = udf(lambda T5, Invoice: match_row(T5.asDict(), Invoice.asDict()), BooleanType())
rule_description_udf = udf(lambda rule_index: rule_descriptions.get(rule_index, '-'), StringType())

# Read in the T5 and Invoice data as Spark dataframes
T5_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', delimiter=',', quote='"').load('Z:/Downloads/Unmatched_trans_Data_1.csv')
Invoice_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', delimiter=',', quote='"').load('Z:/Downloads/Unmatched_Inv_Data_1.csv')

# Create an empty column for the match rule index in the T5 dataframe
T5_df = T5_df.withColumn('Match Rule', lit("Unmatched"))

# Loop through each row in T5 and find a match in Invoice
for i, T5_row in T5_df.iterrows():
    matched = False
    for j, Invoice_row in Invoice_df.iterrows():
        matched, rule_index = match_row_udf(T5_row, Invoice_row)
        if matched:
            T5_df = T5_df.withColumn('Match Rule', lit(rule_index))
            break

# Define rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Create the "description" column based on the "Match Rule" column
T5_df = T5_df.withColumn('description', rule_description_udf(col('Match Rule')))

# Print the updated dataframe
# T5_df.show()

## Convert to csv
T5_df.write.format('com.databricks.spark.csv').options(header='true', delimiter=',', quote='"').mode('overwrite').save('Z:/Desktop/output2.csv')

# Aggregate the data by Company ID & Name and Match Rule
aggregated = T5_df.groupBy('Company ID & Name', 'Match Rule').count().withColumnRenamed('count', 'TRF')

# Add a row for the total count
total_count = aggregated.select('TRF').agg({'TRF': 'sum'}).collect()[0][0]
total_row = ('Total', '-', total_count)
total_df = spark.createDataFrame([total_row], ['Company ID & Name', 'Match Rule', 'TRF'])
aggregated = aggregated.union(total_df)

# Write the aggregated data to a separate csv file
aggregated.write.format('com.databricks.spark.csv').options(header='true






# Find a match for each row in T5 and update the "Match Rule" column
joined_df = T5_df.crossJoin(Invoice_df)
matched_df = joined_df.filter(match_row_udf(col('T5_df.*'), col('Invoice_df.*')))
rule_index_udf = udf(lambda T5, Invoice: match_row(T5, Invoice)[1])
T5_df = T5_df.join(matched_df, ['T5_no']).withColumn('Match Rule', rule_index_udf(col('T5_df.*'), col('Invoice_df.*')))










from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType, IntegerType, StringType, StructType, StructField

# Define the schema for the input data
schema = StructType([
    StructField("fin_orig_supplier_nm", StringType()),
    StructField("fin_source_amt", StringType()),
    StructField("Company ID & Name", StringType()),
    StructField("Match Rule", StringType()),
    StructField("description", StringType())
])

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = udf(eval(f"lambda T5, Invoice: {rule_string}"), BooleanType())
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the first Excel sheet
T5_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
# Read in the second Excel sheet
Invoice_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Define the UDF to match each row in table1 with table2
match_row_udf = udf(lambda T5, Invoice: match_row(T5, Invoice), StructType([
    StructField("matched", BooleanType()),
    StructField("rule_index", IntegerType())
]))

# Join the two dataframes and apply the match_row_udf to each row pair
matched_df = T5_df.crossJoin(Invoice_df).withColumn("match_result", match_row_udf(T5_df, Invoice_df))

# Extract the columns of interest and filter the matched rows
matched_df = matched_df.select("fin_orig_supplier_nm", "fin_source_amt", "Company ID & Name", "match_result.rule_index.alias('Match Rule')", "description").filter("match_result.matched")

# Write the matched data to an Excel file
matched_df.write.format("excel").option("header", "true").save("Z:/Desktop/output2.xlsx")

# Aggregate the matched data and compute the total count
aggregated_df = matched_df.groupBy("Company ID & Name", "Match Rule").count().withColumnRenamed("count", "TRF")
total_count = aggregated_df.agg({"TRF": "sum"}).collect()[0][0]

# Append the total row to the aggregated data and write to the same Excel file
aggregated_df = aggregated_df.unionAll(spark.createDataFrame([["Total", "-", total_count]], schema))
aggregated_df.write.format("excel").option("header", "true").mode("append").save("Z:/Desktop/output2.xlsx")







from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, BooleanType, IntegerType
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("matching").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = udf(lambda row, row2: eval(rule_string), BooleanType())
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as Spark DataFrames
T5_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Z:/Downloads/Unmatched_trans_Data_1.xlsx')
Invoice_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Z:/Downloads/Unmatched_Inv_Data_1.xlsx')

# Define a UDF for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}
get_description = udf(lambda rule_index: rule_descriptions.get(rule_index, ''), StringType())

# Define UDFs to convert column types
to_str = udf(lambda x: str(x), StringType())
to_float = udf(lambda x: float(x) if x is not None else None, FloatType())

# Apply UDFs to convert column types
T5_df = T5_df.withColumn('fin_source_amt', to_float('fin_source_amt'))
Invoice_df = Invoice_df.withColumn('inv_match_source_amt', to_float('inv_match_source_amt'))

# Create an empty list for the output data
output_data = []

# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.collect():
    matched = False
    for Invoice_row in Invoice_df.collect():
        matched, rule_index = match_row(T5_row, Invoice_row)
        if matched:
            description = get_description(rule_index)
            output_data.append({'Company ID & Name': T5_row['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5_row['fin_orig_supplier_nm'], 'fin_source_amt': T5_row['fin_source_amt'], 'inv_match_source_amt': Invoice_row['inv_match_source_amt']})
            break

# Convert output data to a Spark DataFrame
output_df = spark.createDataFrame(output_data)

# Aggregate output data
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').count()
total = aggregated.selectExpr('sum(count)').collect()[0][0]
aggregated = aggregated.union(spark.createDataFrame([('Total', '-', total)], ['Company ID & Name', 'Match Rule', 'count']))

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source

output_df.select(output_columns).coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('Z:/Desktop/matching_output.csv')







from pyspark.sql.functions import col, lit, when

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as DataFrames
T5_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty DataFrame for the output data
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt']
output_df = spark.createDataFrame([], output_columns)

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.rdd.toLocalIterator():
    matched = False
    for Invoice_row in Invoice_df.rdd.toLocalIterator():
        matched, rule_index = match_row(T5_row, Invoice_row)
        if matched:
            description = rule_descriptions[rule_index]
            output_row = [T5_row['Company ID & Name'], rule_index, description, T5_row['fin_orig_supplier_nm'], T5_row['fin_source_amt'], Invoice_row['inv_match_source_amt']]
            output_df = output_df.union(spark.createDataFrame([output_row], output_columns))
            break

# Add a total row to the aggregated DataFrame
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').count().withColumnRenamed('count', 'TRF')
total = output_df.count()
aggregated = aggregated.union(spark.createDataFrame([['Total', '-', total]], aggregated.columns))

# Write the output data to an Excel file
output_df.write.format("excel").option("header", "true").mode("overwrite").save("Z:/Desktop/output2.xlsx", sheet_name='Output', index=False)
aggregated.write.format("excel").option("header", "true").mode("overwrite").save("Z:/Desktop/output2.xlsx", sheet_name='Aggregated', index=False)








from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("T5_Invoice_Match").getOrCreate()

# Read the T5 and Invoice Excel files as Spark dataframes
t5_df = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
invoice_df = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Read the rules from the text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Define a UDF to match T5 and Invoice tables
from pyspark.sql.functions import udf
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

match_udf = udf(lambda T5, Invoice: match_row(T5, Invoice), StructType([StructField("matched", StringType()), StructField("rule_index", IntegerType())]))

# Apply the match UDF to T5 and Invoice tables
matched_df = t5_df.crossJoin(invoice_df).withColumn("match_result", match_udf(t5_df, invoice_df)).select("Company ID & Name", "match_result.*")

# Filter out unmatched rows
matched_df = matched_df.filter(matched_df.matched == "True")

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Define a UDF to get the rule description
rule_description_udf = udf(lambda rule_index: rule_descriptions.get(rule_index, "Unknown"), StringType())

# Add the rule description column to the matched dataframe
matched_df = matched_df.withColumn("description", rule_description_udf(matched_df.rule_index))

# Select the required columns and write the output data to an Excel file
output_columns = ['Company ID & Name', 'rule_index', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt']
matched_df.select(output_columns).write.format("com.databricks.spark.csv").option("header", "true").mode("overwrite").save("Z:/Desktop/output2.csv")

# Aggregate the data by company ID and rule index
aggregated_df = matched_df.groupBy("Company ID & Name", "rule_index").count().withColumnRenamed("count", "TRF")

# Add the total row to the aggregated dataframe
total_row = spark.createDataFrame([["Total", "-", aggregated_df.select("TRF").rdd.flatMap(lambda x: x).sum()]], ["Company ID & Name", "rule_index", "TRF"])
aggregated_df = aggregated_df.union(total_row














from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

# Define the path to the rule file
RULES_FILE_PATH = "Z:/Desktop/Rules_1.txt"

# Create a SparkSession
spark = SparkSession.builder.appName("MatchRows").getOrCreate()

# Define a UDF to parse the rule string and create a lambda function
def parse_rule(rule_string):
    rule_string = rule_string.strip()
    if not rule_string:
        return None
    rule = eval(f"lambda row, row2: {rule_string}")
    return rule

parse_rule_udf = spark.udf.register("parse_rule", parse_rule)

# Load the rules from the file and create a DataFrame
rules_df = spark.read.text(RULES_FILE_PATH)
rules_df = rules_df.select(parse_rule_udf(col("value")).alias("rule")).filter(col("rule").isNotNull())

# Load the T5 and Invoice tables as DataFrames
T5_table = spark.read.format("excel").option("header", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_table = spark.read.format("excel").option("header", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Define a UDF to apply the rules to each row pair and return the rule index
def match_row_udf(T5_dict, Invoice_dict, rules_list):
    for i, rule in enumerate(rules_list):
        if rule(T5_dict, Invoice_dict):
            return (True, i+1)
    return (False, "Unmatched")

match_row_udf = spark.udf.register("match_row_udf", match_row_udf)

# Join the T5 and Invoice tables and apply the rules to each row pair
joined_df = T5_table.crossJoin(Invoice_table)
matched_df = joined_df.withColumn("matched", match_row_udf(col("T5_table"), col("Invoice_table"), lit(rules_df.collect())))
matched_df = matched_df.filter(col("matched")[0] == True).select(col("T5_table.*"), col("matched")[1].alias("rule_index"), col("Invoice_table.inv_match_source_amt"))

# Define a DataFrame for rule descriptions
rule_desc_df = spark.createDataFrame([(1, "Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same"),
                                      (2, "Ticket_no_right_of_12")],
                                     ["rule_index", "description"])

# Join the matched DataFrame with the rule descriptions DataFrame and select the output columns
output_df = matched_df.join(rule_desc_df, on="rule_index").select(col("Company ID & Name"), col("rule_index").alias("Match Rule"), col("description"), col("fin_orig_supplier_nm"), col("fin_source_amt"), col("inv_match_source_amt"))

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt']
output_df.write.format("excel").option("header", "true").mode("overwrite").save("Z:/Desktop/output2.xlsx")

# Aggregate the output by Company ID & Name and Match Rule and add a Total row
aggregated_df = output_df.groupBy("Company ID & Name", "Match Rule").count().withColumnRenamed("count", "TRF")
total_df = aggregated_df.groupBy().agg({"TRF": "sum"}).withColumn("Company ID & Name", lit("Total")).withColumn("Match Rule
