from pyspark.sql.functions import col, lit, when
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("Matching Data").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as Spark DataFrames
T5_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty list for the output data
output_data = []

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Loop through each row in table1 and find a match in table2
for T5 in T5_table.collect():
    matched = False
    for Invoice in Invoice_table.collect():
        matched, rule_index = match_row(T5, Invoice)
        if matched:
            description = rule_descriptions[rule_index]
            output_data.append({'Company ID & Name': T5['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5['fin_orig_supplier_nm'], 'fin_source_amt': T5['fin_source_amt'], 'inv_match_source_amt': Invoice['inv_match_source_amt'], 'inv_erp_vend_no': Invoice['inv_erp_vend_no'], 'inv_po_no': Invoice['inv_po_no']})
            break

# Create a Spark DataFrame from the output data list
output_df = spark.createDataFrame(output_data)

# Add a column for Total Rows Found (TRF) and calculate the total rows found
aggregated = output_df.groupBy(['Company ID & Name', 'Match Rule']).count().withColumnRenamed("count", "TRF")
total = output_df.count()
total_row = spark.createDataFrame([("Total", "-", total,)], output_df.columns)

# Union the aggregated data with the total row and write to an Excel file
aggregated.union(total_row).write.format("com.databricks.spark.csv").option("header", "true").option("delimiter", "\t").mode("overwrite").save("Z:/Desktop/output2.csv")





from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType, BooleanType

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Define UDFs for applying the match_row function and getting the rule description
match_row_udf = udf(lambda T5, Invoice: match_row(T5.asDict(), Invoice.asDict()), BooleanType())
rule_description_udf = udf(lambda rule_index: rule_descriptions.get(rule_index, '-'), StringType())

# Read in the T5 and Invoice data as Spark dataframes
T5_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', delimiter=',', quote='"').load('Z:/Downloads/Unmatched_trans_Data_1.csv')
Invoice_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', delimiter=',', quote='"').load('Z:/Downloads/Unmatched_Inv_Data_1.csv')

# Create an empty column for the match rule index in the T5 dataframe
T5_df = T5_df.withColumn('Match Rule', lit("Unmatched"))

# Loop through each row in T5 and find a match in Invoice
for i, T5_row in T5_df.iterrows():
    matched = False
    for j, Invoice_row in Invoice_df.iterrows():
        matched, rule_index = match_row_udf(T5_row, Invoice_row)
        if matched:
            T5_df = T5_df.withColumn('Match Rule', lit(rule_index))
            break

# Define rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Create the "description" column based on the "Match Rule" column
T5_df = T5_df.withColumn('description', rule_description_udf(col('Match Rule')))

# Print the updated dataframe
# T5_df.show()

## Convert to csv
T5_df.write.format('com.databricks.spark.csv').options(header='true', delimiter=',', quote='"').mode('overwrite').save('Z:/Desktop/output2.csv')

# Aggregate the data by Company ID & Name and Match Rule
aggregated = T5_df.groupBy('Company ID & Name', 'Match Rule').count().withColumnRenamed('count', 'TRF')

# Add a row for the total count
total_count = aggregated.select('TRF').agg({'TRF': 'sum'}).collect()[0][0]
total_row = ('Total', '-', total_count)
total_df = spark.createDataFrame([total_row], ['Company ID & Name', 'Match Rule', 'TRF'])
aggregated = aggregated.union(total_df)

# Write the aggregated data to a separate csv file
aggregated.write.format('com.databricks.spark.csv').options(header='true






# Find a match for each row in T5 and update the "Match Rule" column
joined_df = T5_df.crossJoin(Invoice_df)
matched_df = joined_df.filter(match_row_udf(col('T5_df.*'), col('Invoice_df.*')))
rule_index_udf = udf(lambda T5, Invoice: match_row(T5, Invoice)[1])
T5_df = T5_df.join(matched_df, ['T5_no']).withColumn('Match Rule', rule_index_udf(col('T5_df.*'), col('Invoice_df.*')))










from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType, IntegerType, StringType, StructType, StructField

# Define the schema for the input data
schema = StructType([
    StructField("fin_orig_supplier_nm", StringType()),
    StructField("fin_source_amt", StringType()),
    StructField("Company ID & Name", StringType()),
    StructField("Match Rule", StringType()),
    StructField("description", StringType())
])

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = udf(eval(f"lambda T5, Invoice: {rule_string}"), BooleanType())
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the first Excel sheet
T5_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
# Read in the second Excel sheet
Invoice_df = spark.read.format("excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Define the UDF to match each row in table1 with table2
match_row_udf = udf(lambda T5, Invoice: match_row(T5, Invoice), StructType([
    StructField("matched", BooleanType()),
    StructField("rule_index", IntegerType())
]))

# Join the two dataframes and apply the match_row_udf to each row pair
matched_df = T5_df.crossJoin(Invoice_df).withColumn("match_result", match_row_udf(T5_df, Invoice_df))

# Extract the columns of interest and filter the matched rows
matched_df = matched_df.select("fin_orig_supplier_nm", "fin_source_amt", "Company ID & Name", "match_result.rule_index.alias('Match Rule')", "description").filter("match_result.matched")

# Write the matched data to an Excel file
matched_df.write.format("excel").option("header", "true").save("Z:/Desktop/output2.xlsx")

# Aggregate the matched data and compute the total count
aggregated_df = matched_df.groupBy("Company ID & Name", "Match Rule").count().withColumnRenamed("count", "TRF")
total_count = aggregated_df.agg({"TRF": "sum"}).collect()[0][0]

# Append the total row to the aggregated data and write to the same Excel file
aggregated_df = aggregated_df.unionAll(spark.createDataFrame([["Total", "-", total_count]], schema))
aggregated_df.write.format("excel").option("header", "true").mode("append").save("Z:/Desktop/output2.xlsx")







from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, BooleanType, IntegerType
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("matching").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = udf(lambda row, row2: eval(rule_string), BooleanType())
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as Spark DataFrames
T5_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Z:/Downloads/Unmatched_trans_Data_1.xlsx')
Invoice_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Z:/Downloads/Unmatched_Inv_Data_1.xlsx')

# Define a UDF for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}
get_description = udf(lambda rule_index: rule_descriptions.get(rule_index, ''), StringType())

# Define UDFs to convert column types
to_str = udf(lambda x: str(x), StringType())
to_float = udf(lambda x: float(x) if x is not None else None, FloatType())

# Apply UDFs to convert column types
T5_df = T5_df.withColumn('fin_source_amt', to_float('fin_source_amt'))
Invoice_df = Invoice_df.withColumn('inv_match_source_amt', to_float('inv_match_source_amt'))

# Create an empty list for the output data
output_data = []

# Loop through each row in table1 and find a match in table2
for T5_row in T5_df.collect():
    matched = False
    for Invoice_row in Invoice_df.collect():
        matched, rule_index = match_row(T5_row, Invoice_row)
        if matched:
            description = get_description(rule_index)
            output_data.append({'Company ID & Name': T5_row['Company ID & Name'], 'Match Rule': rule_index, 'description': description, 'fin_orig_supplier_nm': T5_row['fin_orig_supplier_nm'], 'fin_source_amt': T5_row['fin_source_amt'], 'inv_match_source_amt': Invoice_row['inv_match_source_amt']})
            break

# Convert output data to a Spark DataFrame
output_df = spark.createDataFrame(output_data)

# Aggregate output data
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').count()
total = aggregated.selectExpr('sum(count)').collect()[0][0]
aggregated = aggregated.union(spark.createDataFrame([('Total', '-', total)], ['Company ID & Name', 'Match Rule', 'count']))

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source

