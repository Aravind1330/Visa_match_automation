from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType, IntegerType, StringType, StructField, StructType

# Create SparkSession
spark = SparkSession.builder.appName("Matching").getOrCreate()

# Define schema for the input data
input_schema = StructType([
    StructField("Company ID & Name", StringType(), True),
    StructField("fin_orig_supplier_nm", StringType(), True),
    StructField("fin_source_amt", StringType(), True),
    StructField("Currency", StringType(), True),
    StructField("Date", StringType(), True),
    StructField("C&D indicator", StringType(), True),
    StructField("RPIC", StringType(), True)
])

# Read the input data from Excel sheets
t5_table = spark.read.format("com.crealytics.spark.excel").option("header", "true").schema(input_schema).load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
invoice_table = spark.read.format("com.crealytics.spark.excel").option("header", "true").schema(input_schema).load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)

# Define a UDF to apply the rules to each row pair and return the rule index
match_row_udf = udf(lambda t5, invoice: match_row(t5, invoice), BooleanType())
rule_index_udf = udf(lambda t5, invoice: get_rule_index(t5, invoice), IntegerType())

def match_row(t5, invoice):
    for i, rule in enumerate(rules):
        if rule(t5, invoice):
            return True
    return False

def get_rule_index(t5, invoice):
    for i, rule in enumerate(rules):
        if rule(t5, invoice):
            return i+1
    return -1

# Join the tables using a cross join, apply the matching rules, and select the relevant columns
matched_data = t5_table.crossJoin(invoice_table).filter(match_row_udf(t5_table, invoice_table))
output_data = matched_data.select(
    t5_table["Company ID & Name"],
    rule_index_udf(t5_table, invoice_table).alias("Match Rule"),
    "fin_orig_supplier_nm",
    "fin_source_amt",
    "inv_match_source_amt",
    "inv_erp_vend_no",
    "inv_po_no"
)

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Add a column for the rule description
output_data = output_data.withColumn("description", rule_descriptions[output_data["Match Rule"]])

# Write the output data to an Excel file
output_data.write.format("com.crealytics.spark.excel").option("header", "true").option("dataAddress", "'Output'!A1").save("Z:/Desktop/output2.xlsx")

# Aggregate the data by Company ID & Name and Match Rule, and add a row for the total count
aggregated











from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

# Create a SparkSession
spark = SparkSession.builder.appName("MatchingData").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda T5, Invoice: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Read in the two Excel sheets as Spark DataFrames
T5_df = spark.read.format("com.crealytics.spark.excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")
Invoice_df = spark.read.format("com.crealytics.spark.excel").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Create an empty DataFrame for the output data
output_df = spark.createDataFrame([], T5_df.schema)

# Define a dictionary for rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Loop through each row in T5_df and find a match in Invoice_df
for T5_row in T5_df.collect():
    matched = False
    for Invoice_row in Invoice_df.collect():
        matched, rule_index = match_row(T5_row, Invoice_row)
        if matched:
            description = rule_descriptions[rule_index]
            output_row = T5_row.asDict()
            output_row.update({
                'Match Rule': rule_index,
                'description': description,
                'inv_match_source_amt': Invoice_row['inv_match_source_amt'],
                'inv_erp_vend_no': Invoice_row['inv_erp_vend_no'],
                'inv_po_no': Invoice_row['inv_po_no']
            })
            output_df = output_df.union(spark.createDataFrame([output_row]))
            break

# Add Company ID & Name and fin_orig_supplier_nm columns to the output DataFrame
output_df = output_df.withColumn('Company ID & Name', col('company_id_name')).withColumn('fin_orig_supplier_nm', col('fin_orig_supplier_name'))

# Drop unnecessary columns from the output DataFrame
output_df = output_df.drop('company_id_name', 'fin_orig_supplier_name')

# Add a Total row to the aggregated DataFrame
aggregated_df = output_df.groupBy(['Company ID & Name', 'Match Rule']).count().withColumnRenamed('count', 'TRF')
total_row = {'Company ID & Name': 'Total', 'Match Rule': '-', 'TRF': aggregated_df.select('TRF').groupBy().sum().collect()[0][0]}
aggregated_df = aggregated_df.union(spark.createDataFrame([total_row]))

# Write the output data to an Excel file
output_columns = ['Company ID & Name', 'Match Rule',
