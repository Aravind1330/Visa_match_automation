import pandas as pd
import multiprocessing

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return True, i+1
    return False, "Unmatched"

# Define a function to process a chunk of rows
def process_chunk(T5_chunk, Invoice_chunk, output_queue):
    output_data = []
    for i, T5 in T5_chunk.iterrows():
        matched = False
        for j, Invoice in Invoice_chunk.iterrows():
            matched, rule_index = match_row(T5.to_dict(), Invoice.to_dict())
            if matched:
                description = rule_descriptions[rule_index]
                output_data.append([T5['Company ID & Name'], rule_index, description, T5['fin_orig_supplier_nm'], T5['fin_source_amt'], Invoice['inv_match_source_amt'], Invoice['inv_erp_vend_no'], Invoice['inv_po_no']])
                break
    output_queue.put(output_data)

T5_table = pd.read_excel('Z:/Downloads/Unmatched_trans_Data_1.xlsx', dtype=str)
# Read in the second Excel sheet
Invoice_table = pd.read_excel('Z:/Downloads/Unmatched_Inv_Data_1.xlsx', dtype=str)

# Split the data into four chunks
num_chunks = 4
T5_chunks = np.array_split(T5_table, num_chunks)
Invoice_chunks = np.array_split(Invoice_table, num_chunks)

# Create an empty column for the match rule index and description in the output file
output_columns = ['Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no']
output_data = []
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Run the processing function in parallel for each chunk
output_queue = multiprocessing.Queue()
processes = []
for i in range(num_chunks):
    process = multiprocessing.Process(target=process_chunk, args=(T5_chunks[i], Invoice_chunks[i], output_queue))
    process.start()
    processes.append(process)

# Collect the output data from the processes
for i in range(num_chunks):
    output_data += output_queue.get()

# Join the processes
for process in processes:
    process.join()

# Write the output data to an Excel file
output_df = pd.DataFrame(output_data, columns=output_columns)
aggregated = output_df.groupby(['Company ID & Name', 'Match Rule']).size().reset_index(name='TRF')
total = aggregated['TRF'].sum()
aggregated = aggregated.append(pd.Series(['Total', '-', total], index=aggregated.columns), ignore_index=True




def process_chunk(T5, Invoice_chunk, output_queue):
    output_data = []
    for i, row in T5.iterrows():
        matched = False
        for j, Invoice in Invoice_chunk.iterrows():
            matched, rule_index = match_row(row.to_dict(), Invoice.to_dict())
            if matched:
                description = rule_descriptions[rule_index]
                output_data.append([row['Company ID & Name'], rule_index, description, row['fin_orig_supplier_nm'], row['fin_source_amt'], Invoice['inv_match_source_amt'], Invoice['inv_erp_vend_no'], Invoice['inv_po_no']])
                break
    output_queue.put(output_data)















import pyspark.sql.functions as F
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName('Matching').getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = eval(f"lambda row, row2: {rule_string}")
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return i+1
    return 0

# Read the data into Spark DataFrames
T5_df = spark.read.format('csv').option('header', True).option('inferSchema', True).load('Z:/Downloads/Unmatched_trans_Data_1.xlsx')
Invoice_df = spark.read.format('csv').option('header', True).option('inferSchema', True).load('Z:/Downloads/Unmatched_Inv_Data_1.xlsx')

# Add a column to T5_df for each rule, indicating whether the row matches the rule
for i, rule in enumerate(rules):
    udf_match_row = F.udf(lambda row: match_row(row, Invoice_df.collect()), returnType=IntegerType())
    T5_df = T5_df.withColumn(f'rule_{i+1}', udf_match_row(F.struct([T5_df[col] for col in T5_df.columns])))

# Join T5_df and Invoice_df on the rule columns
join_cols = [f'rule_{i+1}' for i in range(len(rules))]
joined_df = T5_df.join(Invoice_df, join_cols)

# Filter out rows where no rule matches
matched_df = joined_df.filter(F.col('rule_1') + F.col('rule_2') > 0)

# Add columns for the rule index and description to the matched DataFrame
udf_rule_description = F.udf(lambda rule_index: rule_descriptions.get(rule_index, 'Unmatched'), returnType=StringType())
matched_df = matched_df.withColumn('Match Rule', F.greatest(*join_cols)).withColumn('description', udf_rule_description(F.greatest(*join_cols)))

# Select the columns we want in the output DataFrame
output_df = matched_df.select('Company ID & Name', 'Match Rule', 'description', 'fin_orig_supplier_nm', 'fin_source_amt', 'inv_match_source_amt', 'inv_erp_vend_no', 'inv_po_no')

# Write the output data to an Excel file
output_df.toPandas().to_excel('Z:/Desktop/output2.xlsx', index=False)

# Aggregate the output data by Company ID & Name and Match Rule
aggregated = output_df.groupBy('Company ID & Name', 'Match Rule').count().withColumnRenamed('count', 'TRF')
total = aggregated.agg(F.sum('TRF')).collect()[0][0]
aggregated = aggregated.union(spark.createDataFrame([('Total', '-', total)], ['Company ID & Name', 'Match Rule', 'TRF']))

# Write the aggregated data to a separate sheet in the output file
aggregated.toPandas().to_excel('Z:/Desktop/output2.xlsx', sheet_name='Aggregated', index=False)
