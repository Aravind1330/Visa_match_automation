from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, BooleanType

# Create a SparkSession
spark = SparkSession.builder.appName("DataMatching").getOrCreate()

# Read the rules from a text file
with open("Z:/Desktop/Rules_1.txt", 'r') as f:
    rule_strings = f.readlines()

# Remove new line characters and empty lines, and convert to list of functions
rules = []
for rule_string in rule_strings:
    rule_string = rule_string.strip()
    if rule_string:
        rule = udf(eval(f"lambda T5, Invoice: {rule_string}"), BooleanType())
        rules.append(rule)

# Define a function to apply the rules to each row pair and return the rule index
def match_row(T5, Invoice):
    for i, rule in enumerate(rules):
        if rule(T5, Invoice):
            return i+1
    return "Unmatched"

# Read in the T5 table and create a DataFrame
T5_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")

# Read in the Invoice table and create a DataFrame
Invoice_table = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Define a UDF to convert dictionary to string
dict_to_str = udf(lambda x: str(x), StringType())

# Define a UDF to add Match Rule and Description columns
add_match_rule = udf(lambda x: match_row(x.asDict(), Invoice_table.where(Invoice_table['inv_match_source_amt'] == x['fin_source_amt']).first().asDict()))

# Add Match Rule and Description columns to T5 table
T5_table = T5_table.withColumn('Match Rule', add_match_rule(T5_table)).withColumn('Description', dict_to_str(T5_table['Match Rule']))

# Define rule descriptions
rule_descriptions = {
    1: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same',
    2: 'Ticket_no_right_of_12'
}

# Define a UDF to add Description column
add_description = udf(lambda x: rule_descriptions[x] if x in rule_descriptions else '')

# Add Description column to T5 table
T5_table = T5_table.withColumn('Description', add_description(T5_table['Match Rule']))

# Write the T5 table to an Excel file
T5_table.write.format("com.databricks.spark.csv").option("header", "true").option("delimiter", "\t").mode("overwrite").save("Z:/Desktop/output2.xlsx")

# Aggregate the T5 table by Company ID & Name and Match Rule, and calculate the TRF
aggregated = T5_table.groupBy(['Company ID & Name', 'Match Rule']).count().withColumnRenamed('count', 'TRF')

# Calculate the total TRF
total = aggregated.groupBy().sum('TRF').collect()[0][0]

# Add a row for the total TRF
aggregated = aggregated.union(spark.createDataFrame([('Total', '-', total)], ['Company ID & Name', 'Match Rule', 'TRF']))

# Write the aggregated data to the second sheet of the Excel file
aggregated.write.format("com.databricks.spark.csv").option("header", "true").option







from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType, IntegerType, StringType
from pyspark.sql import SparkSession

# Define UDFs for each rule
def rule1(T5_dict, invoice_dict):
    return (
        T5_dict['fin_orig_supplier_nm'][-13:] == invoice_dict['inv_ticket_num'][-13:]
        and T5_dict['fin_account'] == invoice_dict['inv_match_account']
        and T5_dict['fin_source_amt'] == invoice_dict['inv_match_source_amt']
        and T5_dict['fin_source_currency'] == invoice_dict['inv_match_currency']
        and T5_dict['fin_date'] == invoice_dict['inv_match_date']
        and T5_dict['fin_cd_indicator'] == invoice_dict['inv_cd_indicator']
        and T5_dict['fin_rpic'] == invoice_dict['inv_match_rpic']
    )

def rule2(T5_dict, invoice_dict):
    return T5_dict['fin_orig_supplier_nm'][-12:] == invoice_dict['inv_ticket_num'][-12:] and T5_dict['fin_source_amt'] == invoice_dict['inv_match_source_amt']

def rule3(T5_dict, invoice_dict):
    return T5_dict['fin_orig_supplier_nm'][-11:] == invoice_dict['inv_ticket_num'][-11:]

# Create a SparkSession
spark = SparkSession.builder.appName("InvoiceMatching").getOrCreate()

# Read the data from Excel files into DataFrames
T5_df = spark.read.format("com.crealytics.spark.excel") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .option("treatEmptyValuesAsNulls", "true") \
    .load("Z:/Downloads/Unmatched_trans_Data_1.xlsx")

Invoice_df = spark.read.format("com.crealytics.spark.excel") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .option("treatEmptyValuesAsNulls", "true") \
    .load("Z:/Downloads/Unmatched_Inv_Data_1.xlsx")

# Define UDFs for matching rules
match_rule1_udf = udf(rule1, BooleanType())
match_rule2_udf = udf(rule2, BooleanType())
match_rule3_udf = udf(rule3, BooleanType())

# Define UDFs for rule descriptions
rule_desc1_udf = udf(lambda x: 'Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same' if x == 1 else None, StringType())
rule_desc2_udf = udf(lambda x: 'Ticket_no_right_of_12' if x == 2 else None, StringType())

# Apply rules to each row pair and return the rule index
def match_row(T5_struct, invoice_struct):
    T5_dict = T5_struct.asDict()
    invoice_dict = invoice_struct.asDict()
    
    if match_rule1_udf(T5_dict, invoice_dict):
        return 1
    elif match_rule2_udf(T5_dict, invoice_dict):
        return 2
    elif match_rule3_udf(T5_dict, invoice_dict):
        return 3
    else:
        return None

match_row_udf = udf(match_row, IntegerType())

# Join the two DataFrames and add a column for the match rule index
joined_df = T5_df.join(Invoice_df, on='Company ID & Name', how='inner')
matched_df = joined_df.withColumn('Match Rule', match_row_udf(joined_df.T5.getItem(0), joined_df.Invoice.getItem(0)))






input_files:
  rules_file: Z:/Desktop/Rules_1.txt
  t5_file: Z:/Downloads/Unmatched_trans_Data_1.xlsx
  invoice_file: Z:/Downloads/Unmatched_Inv_Data_1.xlsx

output_file: Z:/Desktop/output2.xlsx

output_columns:
  - Company ID & Name
  - Match Rule
  - description
  - fin_orig_supplier_nm
  - fin_source_amt
  - inv_match_source_amt

rule_descriptions:
  1: Ticket_no_right_of_13, Acct_no, Amount, Currency, Date, C&D indicator, RPIC are same
  2: Ticket_no_right_of_12
